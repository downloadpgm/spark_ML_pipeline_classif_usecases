---- Exploratory Data Analysis --------------

val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("diabetes/diabetes.csv")

import org.apache.spark.sql.types._
val df1 = df.withColumn("label", col("Outcome").cast(DoubleType)).
             withColumn("sqrt_insulin", sqrt('Insulin))

// ----- logistic regression model hyperparameter tunning

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

import org.apache.spark.ml.feature.VectorAssembler
val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("Pregnancies","Glucose","BloodPressure","SkinThickness","sqrt_insulin","BMI","DiabetesPedigreeFunction","Age"))

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression
lr.setLabelCol("Outcome").setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(1, 0.1, 0.01, 0.001)).
addGrid(lr.fitIntercept).
addGrid(lr.maxIter, Array(10, 20, 40, 100)).build()

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bce = new BinaryClassificationEvaluator

val cv = new CrossValidator().
setEstimator(lr).
setEvaluator(bce.setMetricName("areaUnderROC")).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(va,stdScaler,cv))
val pipelinemodel = pipeline.fit(trainingData)

import org.apache.spark.ml.tuning.CrossValidatorModel
val cvmodel = pipelinemodel.stages.last.asInstanceOf[CrossValidatorModel]

-- CV hyperparameter evaluation

cvmodel.getEstimatorParamMaps.zip(cvmodel.avgMetrics)
res24: Array[(org.apache.spark.ml.param.ParamMap, Double)] =
Array(({
        logreg_aa1b755cf155-fitIntercept: true,
        logreg_aa1b755cf155-maxIter: 10,
        logreg_aa1b755cf155-regParam: 1.0
},0.818003449558395), ({
        logreg_aa1b755cf155-fitIntercept: true,
        logreg_aa1b755cf155-maxIter: 10,
        logreg_aa1b755cf155-regParam: 0.1
},0.8306490585539873), ({
        logreg_aa1b755cf155-fitIntercept: true,
        logreg_aa1b755cf155-maxIter: 10,
        logreg_aa1b755cf155-regParam: 0.01
},0.8315140726706973), ({
        logreg_aa1b755cf155-fitIntercept: true,
        logreg_aa1b755cf155-maxIter: 10,
        logreg_aa1b755cf155-regParam: 0.001
},0.8309308602026858), ({
        logreg_aa1b755cf155-fitIntercept: true,
        logreg_aa1b755cf155-maxIter: 20,
        logreg_aa1b755cf155-regParam: 1.0
},0.818003449558395), ({
        logreg_aa1b755cf155-fitIntercept: true,
        logr...

-- extract best LR model 

import org.apache.spark.ml.classification.LogisticRegressionModel
val lrmodel = cvmodel.bestModel.asInstanceOf[LogisticRegressionModel]

lrmodel.getRegParam
res27: Double = 0.01

lrmodel.getMaxIter
res28: Int = 10

lrmodel.getThreshold
res29: Double = 0.5

lrmodel.getFitIntercept
res31: Boolean = true

-- collecting feature importance

println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
0.5926039270031349
0.927389576631519
-0.36289707962184947
0.1407360979898216
-0.15349156267909334
0.676535266198667
0.25363462296435163
0.20228774169123512

import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary

val trainingSummary = lrmodel.summary

trainingSummary.objectiveHistory
res32: Array[Double] = Array(0.6537596512496414, 0.5066075447484147, 0.48780908169825277, 0.47993118399913803, 0.4796373558153267, 0.47935363617424437, 0.4793275572974692, 0.4793171180132598, 0.47931362808572553, 0.479313355394056, 0.4793133464045364)

val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]

binarySummary.areaUnderROC
res33: Double = 0.8436076480460323

binarySummary.accuracy
es34: Double = 0.7695167286245354

-- collecting metric performance

val pred = pipelinemodel.transform(testData)

val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res39: Double = 0.809598059598061

bceval.setMetricName("areaUnderPR").evaluate(pred)
res40: Double = 0.6742059473078765

val predRDD = pred.select("prediction","label").rdd.map( row => (row.getDouble(0),row.getDouble(1).toDouble)).cache

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

metrics.accuracy
res43: Double = 0.7869565217391304

metrics.confusionMatrix
res44: org.apache.spark.mllib.linalg.Matrix =
135.0  21.0
28.0   46.0
