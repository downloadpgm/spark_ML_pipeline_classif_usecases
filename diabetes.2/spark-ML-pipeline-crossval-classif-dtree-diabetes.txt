---- Exploratory Data Analysis --------------

val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("diabetes/diabetes.csv")

df.printSchema
root
 |-- Pregnancies: integer (nullable = true)
 |-- Glucose: integer (nullable = true)
 |-- BloodPressure: integer (nullable = true)
 |-- SkinThickness: integer (nullable = true)
 |-- Insulin: integer (nullable = true)
 |-- BMI: double (nullable = true)
 |-- DiabetesPedigreeFunction: double (nullable = true)
 |-- Age: integer (nullable = true)
 |-- Outcome: integer (nullable = true)
 
df.show(10)
+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+
|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|
+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+
|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|
|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|
|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|
|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|
|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|
|          5|    116|           74|            0|      0|25.6|                   0.201| 30|      0|
|          3|     78|           50|           32|     88|31.0|                   0.248| 26|      1|
|         10|    115|            0|            0|      0|35.3|                   0.134| 29|      0|
|          2|    197|           70|           45|    543|30.5|                   0.158| 53|      1|
|          8|    125|           96|            0|      0| 0.0|                   0.232| 54|      1|
+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+
 
df.describe().show
+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+
|summary|       Pregnancies|          Glucose|     BloodPressure|     SkinThickness|           Insulin|               BMI|DiabetesPedigreeFunction|               Age|           Outcome|
+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+
|  count|               768|              768|               768|               768|               768|               768|                     768|               768|               768|
|   mean|3.8450520833333335|     120.89453125|       69.10546875|20.536458333333332| 79.79947916666667|31.992578124999977|      0.4718763020833327|33.240885416666664|0.3489583333333333|
| stddev|  3.36957806269887|31.97261819513622|19.355807170644777|15.952217567727642|115.24400235133803| 7.884160320375441|       0.331328595012775|11.760231540678689| 0.476951377242799|
|    min|                 0|                0|                 0|                 0|                 0|               0.0|                   0.078|                21|                 0|
|    max|                17|              199|               122|                99|               846|              67.1|                    2.42|                81|                 1|
+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+

df.groupBy("Outcome").count.show
+-------+-----+
|Outcome|count|
+-------+-----+
|      1|  268|
|      0|  500|
+-------+-----+

---- Feature extraction & Data Munging --------------

import org.apache.spark.sql.types._
val df1 = df.withColumn("label", col("Outcome").cast(DoubleType))

import org.apache.spark.ml.feature.{VectorAssembler}

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("Pregnancies","Glucose","BloodPressure","SkinThickness","Insulin","BMI","DiabetesPedigreeFunction","Age"))

// ----- building the decision tree model

import org.apache.spark.ml.classification.DecisionTreeClassifier
val dt = new DecisionTreeClassifier

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(va, dt))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

val model = pipeline.fit(trainingData)

-- collecting feature importance

import org.apache.spark.ml.classification.DecisionTreeClassificationModel

val dtmodel = model.stages.last.asInstanceOf[DecisionTreeClassificationModel]

val featureImp = va.getInputCols.zip(dtmodel.featureImportances.toArray)

val columns = Array("feature", "Importance")
val featureImpDF = spark.createDataFrame(featureImp).toDF(columns: _*)

featureImpDF.orderBy($"Importance".desc).show()
+--------------------+--------------------+
|             feature|          Importance|
+--------------------+--------------------+
|             Glucose|  0.3753053146756258|
|                 BMI| 0.23805429071267606|
|                 Age| 0.14328284318272586|
|DiabetesPedigreeF...| 0.08564940340072369|
|       BloodPressure|  0.0784438836865335|
|         Pregnancies| 0.05295862523551117|
|             Insulin| 0.02289690432492951|
|       SkinThickness|0.003408734781274413|
+--------------------+--------------------+


-- collecting metric performance

val pred = model.transform(testData)

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res2: Double = 0.7674982674982676

bceval.setMetricName("areaUnderPR").evaluate(pred)
res3: Double = 0.6555242135673434


val validPredicts = pred.select("label","prediction").rdd.map( row => (row.getDouble(0),row.getDouble(1)) )

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)

metrics.confusionMatrix
res11: org.apache.spark.mllib.linalg.Matrix =
141.0  35.0
15.0   39.0


// ----- DT model hyperparameter tunning

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(dt.maxBins, Array(32, 48, 64)).
addGrid(dt.impurity, Array("gini", "entropy")).
addGrid(dt.maxDepth, Array(10,20,30)).build()

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new BinaryClassificationEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val cvmodel = cv.fit(trainingData)

-- CV hyperparameter evaluation

(new BinaryClassificationEvaluator).getMetricName
res10: String = areaUnderROC

cvmodel.getEstimatorParamMaps.zip(cvmodel.avgMetrics)
res3: Array[(org.apache.spark.ml.param.ParamMap, Double)] =
Array(({
        dtc_eb11f5a58bc1-impurity: gini,
        dtc_eb11f5a58bc1-maxBins: 32,
        dtc_eb11f5a58bc1-maxDepth: 10
},0.6829594541750472), ({
        dtc_eb11f5a58bc1-impurity: gini,
        dtc_eb11f5a58bc1-maxBins: 48,
        dtc_eb11f5a58bc1-maxDepth: 10
},0.6953787474597024), ({
        dtc_eb11f5a58bc1-impurity: gini,
        dtc_eb11f5a58bc1-maxBins: 64,
        dtc_eb11f5a58bc1-maxDepth: 10
},0.7085795925408872), ({
        dtc_eb11f5a58bc1-impurity: entropy,
        dtc_eb11f5a58bc1-maxBins: 32,
        dtc_eb11f5a58bc1-maxDepth: 10
},0.6835426208011638), ({
        dtc_eb11f5a58bc1-impurity: entropy,
        dtc_eb11f5a58bc1-maxBins: 48,
        dtc_eb11f5a58bc1-maxDepth: 10
},0.7139440173713245), ({
        dtc_eb11f5a58bc1-impurity: entropy,
        dtc_eb11f5a58bc1-maxBins: 64,
        dtc_eb11f5a58bc1-maxDepth: 10
},0.703307650...


-- extract best DT model 

import org.apache.spark.ml.PipelineModel
val bestmodel = cvmodel.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.classification.DecisionTreeClassificationModel
val dtmodel = bestmodel.stages.last.asInstanceOf[DecisionTreeClassificationModel]

dtmodel.getMaxBins
res0: Int = 48

dtmodel.getImpurity
res2: String = entropy

dtmodel.getMaxDepth
res3: Int = 10

-- collecting feature importance

val featureImp = va.getInputCols.zip(dtmodel.featureImportances.toArray)

val columns = Array("feature", "Importance")
val featureImpDF = spark.createDataFrame(featureImp).toDF(columns: _*)

featureImpDF.orderBy($"Importance".desc).show()
+--------------------+-------------------+
|             feature|         Importance|
+--------------------+-------------------+
|             Glucose| 0.2645057502863585|
|                 BMI|0.15206637757605346|
|         Pregnancies|0.14930522894248274|
|                 Age|0.12338350674191023|
|       BloodPressure|0.10696824699334317|
|DiabetesPedigreeF...|0.08815785695323829|
|       SkinThickness|0.06640840058555657|
|             Insulin|0.04920463192105709|
+--------------------+-------------------+


-- collecting metric performance

val pred = bestmodel.transform(testData)

val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res16: Double = 0.7436763686763688

bceval.setMetricName("areaUnderPR").evaluate(pred)
res17: Double = 0.6269005913941122


val validPredicts = pred.select("label","prediction").rdd.map( row => (row.getDouble(0),row.getDouble(1)) )

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)

metrics.confusionMatrix
res18: org.apache.spark.mllib.linalg.Matrix =
124.0  28.0
32.0   46.0
