---- Feature extraction & Data Munging --------------

val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("obesity/ObesityDataSet_raw_and_data_sinthetic.csv")

df.printSchema
root
 |-- Gender: string (nullable = true)
 |-- Age: double (nullable = true)
 |-- Height: double (nullable = true)
 |-- Weight: double (nullable = true)
 |-- family_history_with_overweight: string (nullable = true)
 |-- FAVC: string (nullable = true)
 |-- FCVC: double (nullable = true)
 |-- NCP: double (nullable = true)
 |-- CAEC: string (nullable = true)
 |-- SMOKE: string (nullable = true)
 |-- CH2O: double (nullable = true)
 |-- SCC: string (nullable = true)
 |-- FAF: double (nullable = true)
 |-- TUE: double (nullable = true)
 |-- CALC: string (nullable = true)
 |-- MTRANS: string (nullable = true)
 |-- NObeyesdad: string (nullable = true)
 
df.show(10)
+------+----+------+------+------------------------------+----+----+---+---------+-----+----+---+---+---+----------+--------------------+-------------------+
|Gender| Age|Height|Weight|family_history_with_overweight|FAVC|FCVC|NCP|     CAEC|SMOKE|CH2O|SCC|FAF|TUE|      CALC|              MTRANS|         NObeyesdad|
+------+----+------+------+------------------------------+----+----+---+---------+-----+----+---+---+---+----------+--------------------+-------------------+
|Female|21.0|  1.62|  64.0|                           yes|  no| 2.0|3.0|Sometimes|   no| 2.0| no|0.0|1.0|        no|Public_Transporta...|      Normal_Weight|
|Female|21.0|  1.52|  56.0|                           yes|  no| 3.0|3.0|Sometimes|  yes| 3.0|yes|3.0|0.0| Sometimes|Public_Transporta...|      Normal_Weight|
|  Male|23.0|   1.8|  77.0|                           yes|  no| 2.0|3.0|Sometimes|   no| 2.0| no|2.0|1.0|Frequently|Public_Transporta...|      Normal_Weight|
|  Male|27.0|   1.8|  87.0|                            no|  no| 3.0|3.0|Sometimes|   no| 2.0| no|2.0|0.0|Frequently|             Walking| Overweight_Level_I|
|  Male|22.0|  1.78|  89.8|                            no|  no| 2.0|1.0|Sometimes|   no| 2.0| no|0.0|0.0| Sometimes|Public_Transporta...|Overweight_Level_II|
|  Male|29.0|  1.62|  53.0|                            no| yes| 2.0|3.0|Sometimes|   no| 2.0| no|0.0|0.0| Sometimes|          Automobile|      Normal_Weight|
|Female|23.0|   1.5|  55.0|                           yes| yes| 3.0|3.0|Sometimes|   no| 2.0| no|1.0|0.0| Sometimes|           Motorbike|      Normal_Weight|
|  Male|22.0|  1.64|  53.0|                            no|  no| 2.0|3.0|Sometimes|   no| 2.0| no|3.0|0.0| Sometimes|Public_Transporta...|      Normal_Weight|
|  Male|24.0|  1.78|  64.0|                           yes| yes| 3.0|3.0|Sometimes|   no| 2.0| no|1.0|1.0|Frequently|Public_Transporta...|      Normal_Weight|
|  Male|22.0|  1.72|  68.0|                           yes| yes| 2.0|3.0|Sometimes|   no| 2.0| no|1.0|1.0|        no|Public_Transporta...|      Normal_Weight|
+------+----+------+------+------------------------------+----+----+---+---------+-----+----+---+---+---+----------+--------------------+-------------------+

scala> df.select("NObeyesdad").distinct.show
+-------------------+
|         NObeyesdad|
+-------------------+
|   Obesity_Type_III|
| Overweight_Level_I|
|    Obesity_Type_II|
|Insufficient_Weight|
|Overweight_Level_II|
|      Normal_Weight|
|     Obesity_Type_I|
+-------------------+

scala> df.select("MTRANS").distinct.show
+--------------------+
|              MTRANS|
+--------------------+
|                Bike|
|             Walking|
|          Automobile|
|           Motorbike|
|Public_Transporta...|
+--------------------+

scala> df.select("CALC").distinct.show
+----------+
|      CALC|
+----------+
| Sometimes|
|Frequently|
|        no|
|    Always|
+----------+

scala> df.select("CAEC").distinct.show
+----------+
|      CAEC|
+----------+
| Sometimes|
|Frequently|
|        no|
|    Always|
+----------+

scala> df.select("Gender").distinct.show
+------+
|Gender|
+------+
|Female|
|  Male|
+------+

val df1 = df.select("Gender","family_history_with_overweight","FAVC","CAEC","SMOKE","SCC","CALC","MTRANS","Age","Height","Weight","FCVC","NCP","CH2O","FAF","TUE","NObeyesdad")

val rdd = df1.rdd.map( x => x.toSeq ).map( x => x.map( y => { y.toString } ))

rdd.take(2)
res0: Array[Seq[String]] = Array(ArrayBuffer(Female, yes, no, Sometimes, no, no, no, Public_Transportation, 21.0, 1.62, 64.0, 2.0, 3.0, 2.0, 0.0, 1.0, Normal_Weight), ArrayBuffer(Female, yes, no, Sometimes, yes, yes, Sometimes, Public_Transportation, 21.0, 1.52, 56.0, 3.0, 3.0, 3.0, 3.0, 0.0, Normal_Weight))




---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
	val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
	if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
    categoryFeatures
	})
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val rdd1 = rdd.map( x => x.toArray )

rdd1.take(2)
res2: Array[Array[String]] = Array(Array(Female, yes, no, Sometimes, no, no, no, Public_Transportation, 21.0, 1.62, 64.0, 2.0, 3.0, 2.0, 0.0, 1.0, Normal_Weight), Array(Female, yes, no, Sometimes, yes, yes, Sometimes, Public_Transportation, 21.0, 1.52, 56.0, 3.0, 3.0, 3.0, 3.0, 0.0, Normal_Weight))

val concat = mergeArray(rdd1,0,1,2,3,4,5,6,7)

concat.take(2)
res1: Array[Array[Double]] = Array(Array(1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0), Array(1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0))

val categories = rdd1.map( x => x(x.size-1)).distinct.zipWithIndex.collectAsMap
categories: scala.collection.Map[String,Long] = Map(Overweight_Level_II -> 4, Obesity_Type_III -> 1, Normal_Weight -> 0, Overweight_Level_I -> 6, Obesity_Type_I -> 5, Obesity_Type_II -> 3, Insufficient_Weight -> 2)

val rdd2 = rdd1.map( x => x.slice(8,x.size)).map( y => y.map( x => {
                     try { x.toDouble } catch { case _ : Throwable => categories(x).toString.toDouble } }))
					 
rdd2.take(5)
res3: Array[Array[Double]] = Array(Array(21.0, 1.62, 64.0, 2.0, 3.0, 2.0, 0.0, 1.0, 0.0), Array(21.0, 1.52, 56.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(2)
res5: Array[Array[Double]] = Array(Array(1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 21.0, 1.62, 64.0, 2.0, 3.0, 2.0, 0.0, 1.0, 0.0), Array(1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 21.0, 1.52, 56.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0))


val categ_gender = rdd1.map( x => x(0)).distinct.zipWithIndex.collectAsMap
categ_gender: scala.collection.Map[String,Long] = Map(Male -> 0, Female -> 1)

val categ_yesno = rdd1.map( x => x(1)).distinct.zipWithIndex.collectAsMap
categ_yesno: scala.collection.Map[String,Long] = Map(no -> 1, yes -> 0)

val categ_frequency = rdd1.map( x => x(3)).distinct.zipWithIndex.collectAsMap
categ_frequency: scala.collection.Map[String,Long] = Map(Frequently -> 0, Always -> 3, no -> 2, Sometimes -> 1)

val categ_transp = rdd1.map( x => x(7)).distinct.zipWithIndex.collectAsMap
categ_transp: scala.collection.Map[String,Long] = Map(Public_Transportation -> 4, Motorbike -> 0, Walking -> 1, Automobile -> 3, Bike -> 2)

val rdd2_dt = rdd1.map( x => {
  val y = Array(categ_gender(x(0)),categ_yesno(x(1)),categ_yesno(x(2)),categ_frequency(x(3)),categ_yesno(x(4)),categ_yesno(x(5)),categ_frequency(x(6)),categ_transp(x(7)),x(8),x(9),x(10),x(11),x(12),x(13),x(14),x(15),categories(x(16)))
  y.map( z => z.toString.toDouble)
  })
  
rdd2_dt.take(2)
res20: Array[Array[Double]] = Array(Array(1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 4.0, 21.0, 1.62, 64.0, 2.0, 3.0, 2.0, 0.0, 1.0, 0.0), Array(1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 4.0, 21.0, 1.52, 56.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0))

val data = vect.zip(rdd2_dt)

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val train_rdd = sets(0)
val test_rdd = sets(1)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint  

val trainSet = train_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

---- MLlib Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(7).run(trainSet)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res23: Array[(Double, Double)] = Array((4.0,0.0), (0.0,6.0), (4.0,0.0), (6.0,5.0), (0.0,4.0), (2.0,0.0), (5.0,5.0), (6.0,0.0), (5.0,5.0), (4.0,4.0), (5.0,4.0), (0.0,0.0), (0.0,0.0), (6.0,0.0), (6.0,0.0), (2.0,0.0), (2.0,2.0), (6.0,0.0), (0.0,4.0), (4.0,3.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 503
validPredicts.count                            // 610
val accuracy = metrics.accuracy   // 0.8245901639344262

metrics.confusionMatrix
res26: org.apache.spark.mllib.linalg.Matrix =
41.0  1.0   14.0  0.0   9.0   0.0   20.0
0.0   87.0  0.0   0.0   0.0   1.0   0.0
0.0   0.0   78.0  0.0   0.0   0.0   0.0
1.0   0.0   0.0   78.0  1.0   4.0   0.0
6.0   0.0   0.0   0.0   67.0  5.0   10.0
2.0   1.0   0.0   2.0   5.0   95.0  1.0
12.0  0.0   0.0   0.0   12.0  0.0   57.0


---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res30: org.apache.spark.mllib.linalg.Vector = [1.0,55.137881,1.947406,173.0,1.0,1.0,3.0,4.0,3.0,1.0,3.0,1.0,3.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0]

matrixSummary.min
res31: org.apache.spark.mllib.linalg.Vector = [0.0,15.0,1.456346,39.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]

matrixSummary.mean
res32: org.apache.spark.mllib.linalg.Vector = [0.505338078291815,24.315622775207594,1.7027039940688016,86.51312923428229,0.8185053380782918,0.8754448398576512,2.416038842230127,2.6936422562277573,1.1381969157769878,0.020166073546856466,2.012419110913405,0.042704626334519574,1.0094075521945443,0.6573686595492284,0.7325029655990519,0.005338078291814947,0.028469750889679714,0.004151838671411625,0.2188612099644128,0.7431791221826809]

matrixSummary.variance
res33: org.apache.spark.mllib.linalg.Vector = [0.2501198559616461,39.712905886257026,0.008677325671109732,675.2214294349506,0.14864251243439602,0.10910588508611677,0.287883452208947,0.6024424266927134,0.22124636120116414,0.0197711296732385,0.38315927872779487,0.040905202910335066,0.7396651892454043,0.37834807191623315,0.2637144436113787,0.0053127342999250236,0.027675639143795474,0.004137054676142503,0.1710624412600188,0.1909771868873002]

---- Apply standardization to dataset -------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, false).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))

---- MLlib Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(7).run(trainScaled)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

validPredicts.take(20)
res28: Array[(Double, Double)] = Array((0.0,0.0), (4.0,6.0), (0.0,0.0), (5.0,5.0), (6.0,4.0), (2.0,0.0), (3.0,5.0), (2.0,0.0), (5.0,5.0), (4.0,4.0), (4.0,4.0), (0.0,0.0), (0.0,0.0), (6.0,0.0), (2.0,0.0), (0.0,0.0), (2.0,2.0), (6.0,0.0), (4.0,4.0), (3.0,3.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 539
validPredicts.count                            // 610
val accuracy = metrics.accuracy   // 0.8836065573770492

metrics.confusionMatrix
res31: org.apache.spark.mllib.linalg.Matrix =
43.0  0.0   29.0  0.0   0.0   0.0    13.0
0.0   88.0  0.0   0.0   0.0   0.0    0.0
5.0   0.0   73.0  0.0   0.0   0.0    0.0
0.0   2.0   0.0   82.0  0.0   0.0    0.0
0.0   0.0   0.0   0.0   82.0  4.0    2.0
0.0   1.0   0.0   2.0   1.0   102.0  0.0
7.0   0.0   2.0   0.0   2.0   1.0    69.0

---- MLlib Decision Tree regression --------------

val trainSet = train_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 0->2, 1->2, 2->2, 3->4, 4->2, 5->2, 6->4, 7->5)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, accuracy") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 7, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new MulticlassMetrics(validPredicts)
    println("%s, %d, %d ->  %d / %d, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.accuracy))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, accuracy
gini, 10, 32 ->  553 / 610, 0.9066
gini, 10, 48 ->  544 / 610, 0.8918
gini, 10, 64 ->  556 / 610, 0.9115
gini, 20, 32 ->  547 / 610, 0.8967
gini, 20, 48 ->  552 / 610, 0.9049
gini, 20, 64 ->  551 / 610, 0.9033
gini, 30, 32 ->  547 / 610, 0.8967
gini, 30, 48 ->  552 / 610, 0.9049
gini, 30, 64 ->  551 / 610, 0.9033
entropy, 10, 32 ->  572 / 610, 0.9377
entropy, 10, 48 ->  566 / 610, 0.9279
entropy, 10, 64 ->  582 / 610, 0.9541 *
entropy, 20, 32 ->  571 / 610, 0.9361
entropy, 20, 48 ->  566 / 610, 0.9279
entropy, 20, 64 ->  580 / 610, 0.9508
entropy, 30, 32 ->  571 / 610, 0.9361
entropy, 30, 48 ->  566 / 610, 0.9279
entropy, 30, 64 ->  580 / 610, 0.9508


val model = DecisionTree.trainClassifier(trainSet, 7, categoricalFeaturesInfo, "entropy", 10, 48)

scala> model.toDebugString

res37: String =
"DecisionTreeModel classifier of depth 10 with 169 nodes
  If (feature 10 <= 95.3907315)
   If (feature 10 <= 62.105785999999995)
    If (feature 9 <= 1.660162)
     If (feature 10 <= 45.1243135)
      If (feature 11 <= 2.0002329999999997)
       If (feature 10 <= 42.111116)
        Predict: 2.0
       Else (feature 10 > 42.111116)
        If (feature 3 in {1.0})
         Predict: 0.0
        Else (feature 3 not in {1.0})
         Predict: 2.0
      Else (feature 11 > 2.0002329999999997)
       If (feature 7 in {3.0})
        Predict: 0.0
       Else (feature 7 not in {3.0})
        Predict: 2.0
     Else (feature 10 > 45.1243135)
      If (feature 9 <= 1.5204985)
       If (feature 8 <= 18.0015765)
        Predict: 6.0
       Else (feature 8 > 18.0015765)
        If (fe...

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res38: Array[(Double, Double)] = Array((0.0,0.0), (0.0,6.0), (0.0,0.0), (5.0,5.0), (6.0,4.0), (0.0,0.0), (4.0,5.0), (0.0,0.0), (5.0,5.0), (4.0,4.0), (4.0,4.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (2.0,2.0), (0.0,0.0), (4.0,4.0), (5.0,3.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 566
validPredicts.count                            // 610
val accuracy = metrics.accuracy   // 0.9278688524590164

metrics.confusionMatrix
res41: org.apache.spark.mllib.linalg.Matrix =
75.0  0.0   3.0   0.0   1.0   0.0    6.0
0.0   88.0  0.0   0.0   0.0   0.0    0.0
3.0   0.0   75.0  0.0   0.0   0.0    0.0
0.0   1.0   0.0   80.0  0.0   3.0    0.0
0.0   0.0   0.0   0.0   80.0  3.0    5.0
0.0   0.0   0.0   0.0   3.0   102.0  1.0
10.0  0.0   0.0   0.0   5.0   0.0    66.0