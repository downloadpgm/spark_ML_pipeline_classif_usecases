
val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("ObesityDataSet_raw_and_data_sinthetic.csv")

df.printSchema
root
 |-- Gender: string (nullable = true)
 |-- Age: double (nullable = true)
 |-- Height: double (nullable = true)
 |-- Weight: double (nullable = true)
 |-- family_history_with_overweight: string (nullable = true)
 |-- FAVC: string (nullable = true)
 |-- FCVC: double (nullable = true)
 |-- NCP: double (nullable = true)
 |-- CAEC: string (nullable = true)
 |-- SMOKE: string (nullable = true)
 |-- CH2O: double (nullable = true)
 |-- SCC: string (nullable = true)
 |-- FAF: double (nullable = true)
 |-- TUE: double (nullable = true)
 |-- CALC: string (nullable = true)
 |-- MTRANS: string (nullable = true)
 |-- NObeyesdad: string (nullable = true)
 
import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}

val dfInd1 = new StringIndexer().setInputCol("Gender").setOutputCol("GenderIdx")
val dfInd2 = new StringIndexer().setInputCol("family_history_with_overweight").setOutputCol("FamHistIdx")
val dfInd3 = new StringIndexer().setInputCol("FAVC").setOutputCol("FAVCIdx")
val dfInd4 = new StringIndexer().setInputCol("CAEC").setOutputCol("CAECIdx")
val dfInd5 = new StringIndexer().setInputCol("SMOKE").setOutputCol("SMOKEIdx")
val dfInd6 = new StringIndexer().setInputCol("SCC").setOutputCol("SCCIdx")
val dfInd7 = new StringIndexer().setInputCol("CALC").setOutputCol("CALCIdx").setHandleInvalid("keep")
val dfInd8 = new StringIndexer().setInputCol("MTRANS").setOutputCol("MTRANSIdx")
val dfInd9 = new StringIndexer().setInputCol("NObeyesdad").setOutputCol("label")

val dfOne1 = new OneHotEncoder().setInputCol("GenderIdx").setOutputCol("GenderVect")
val dfOne2 = new OneHotEncoder().setInputCol("FamHistIdx").setOutputCol("FamHistVect")
val dfOne3 = new OneHotEncoder().setInputCol("FAVCIdx").setOutputCol("FAVCVect")
val dfOne4 = new OneHotEncoder().setInputCol("CAECIdx").setOutputCol("CAECVect")
val dfOne5 = new OneHotEncoder().setInputCol("SMOKEIdx").setOutputCol("SMOKEVect")
val dfOne6 = new OneHotEncoder().setInputCol("SCCIdx").setOutputCol("SCCVect")
val dfOne7 = new OneHotEncoder().setInputCol("CALCIdx").setOutputCol("CALCVect")
val dfOne8 = new OneHotEncoder().setInputCol("MTRANSIdx").setOutputCol("MTRANSVect")

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("GenderVect","Age","Height","Weight","FamHistVect","FAVCVect","FCVC","NCP","CAECVect","SMOKEVect","CH2O","SCCVect","FAF","TUE","CALCVect","MTRANSVect","label"))

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(500).setFitIntercept(true).setFamily("binomial").setFeaturesCol("scaledFeatures")

val ovr = new OneVsRest().setClassifier(lr)

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfInd4,dfInd5,dfInd6,dfInd7,dfInd8,dfInd9,dfOne1,dfOne2,dfOne3,dfOne4,dfOne5,dfOne6,dfOne7,dfOne8,va,stdScaler,ovr))

val Array(trainingData, testData) = df.randomSplit(Array(0.7,0.3),11L)

val model = pipeline.fit(trainingData)

val pred = model.transform(testData)

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)
res15: Double = 0.9

-------------------

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().addGrid(lr.regParam, Array(0.1, 0.01)).addGrid(lr.fitIntercept, Array(true)).addGrid(lr.maxIter, Array(100,300)).build()

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new MulticlassClassificationEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.classification.OneVsRestModel
val lrmodel = bestmodel.stages(4).asInstanceOf[OneVsRestModel]

val pred = bestmodel.transform(testData)

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)
res28: Double = 0.9032786885245901