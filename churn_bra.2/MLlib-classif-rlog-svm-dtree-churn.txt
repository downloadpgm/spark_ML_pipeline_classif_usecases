---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("churn_bra/dados_clientes.csv").map( x => x.split(","))

val hdr = rdd.take(1)(0)
hdr: Array[String] = Array(id, Churn, Mais65anos, Conjuge, Dependentes, MesesDeContrato, TelefoneFixo, MaisDeUmaLinhaTelefonica, Internet, SegurancaOnline, BackupOnline, SeguroDispositivo, SuporteTecnico, TVaCabo, StreamingFilmes, TipoContrato, ContaCorreio, MetodoPagamento, MesesCobrados)

val rdd1 = rdd.filter( x => ! x(2).contains("Mais65anos"))

rdd1.take(10).map( x => x.mkString(", ")).foreach(println)
0, Nao, 0, Sim, Nao, 1, Nao, SemServicoTelefonico, DSL, Nao, Sim, Nao, Nao, Nao, Nao, Mensalmente, Sim, BoletoEletronico, 29.85
1, Nao, 0, Nao, Nao, 34, Sim, Nao, DSL, Sim, Nao, Sim, Nao, Nao, Nao, UmAno, Nao, Boleto, 56.95
2, Sim, 0, Nao, Nao, 2, Sim, Nao, DSL, Sim, Sim, Nao, Nao, Nao, Nao, Mensalmente, Sim, Boleto, 53.85
3, Nao, 0, Nao, Nao, 45, Nao, SemServicoTelefonico, DSL, Sim, Nao, Sim, Sim, Nao, Nao, UmAno, Nao, DebitoEmConta, 42.3
4, Sim, 0, Nao, Nao, 2, Sim, Nao, FibraOptica, Nao, Nao, Nao, Nao, Nao, Nao, Mensalmente, Sim, BoletoEletronico, 70.7
5, Sim, 0, Nao, Nao, 8, Sim, Sim, FibraOptica, Nao, Nao, Sim, Nao, Sim, Sim, Mensalmente, Sim, BoletoEletronico, 99.65
6, Nao, 0, Nao, Sim, 22, Sim, Sim, FibraOptica, Nao, Sim, Nao, Nao, Sim, Nao, Mensalmente, Sim, CartaoCredito, 89.1
7, Nao, 0, Nao, Nao, 10, Nao, SemServicoTelefonico, DSL, Sim, Nao, Nao, Nao, Nao, Nao, Mensalmente, Nao, Boleto, 29.75
8, Sim, 0, Sim, Nao, 28, Sim, Sim, FibraOptica, Nao, Nao, Sim, Sim, Sim, Sim, Mensalmente, Sim, BoletoEletronico, 104.8
9, Nao, 0, Nao, Sim, 62, Sim, Nao, DSL, Sim, Sim, Nao, Nao, Nao, Nao, UmAno, Nao, DebitoEmConta, 56.15

// MaisDeUmaLinhaTelefonica
rdd1.map( x => (x(7),1)).reduceByKey(_+_).take(10)
res2: Array[(String, Int)] = Array((SemServicoTelefonico,985), (Nao,4848), (Sim,4515))

// Internet
rdd1.map( x => (x(8),1)).reduceByKey(_+_).take(10)
res1: Array[(String, Int)] = Array((Nao,1741), (FibraOptica,5401), (DSL,3206))

// TipoContrato
rdd1.map( x => (x(15),1)).reduceByKey(_+_).take(10)
res5: Array[(String, Int)] = Array((DoisAnos,1750), (UmAno,1672), (Mensalmente,6926))

// MetodoPagamento
rdd1.map( x => (x(17),1)).reduceByKey(_+_).take(10)
res4: Array[(String, Int)] = Array((CartaoCredito,1761), (DebitoEmConta,1822), (BoletoEletronico,4714), (Boleto,2051))

val categ_simnao = rdd1.map(x => x(1)).distinct.zipWithIndex.collectAsMap
categ_simnao: scala.collection.Map[String,Long] = Map(Sim -> 1, Nao -> 0)

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  print(f"$idx%.0f -> "); println(categories)
  
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
    val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories - 1) else Array.ofDim[Double](1)
    if (numCategories > 2) { 
      if (categoryIdx > 0) categoryFeatures(categoryIdx - 1) = 1.0
    }
    else categoryFeatures(0) = categoryIdx
    categoryFeatures
    })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

// 1- Churn,
// 2- Mais65anos,
// 3- Conjuge,
// 4- Dependentes,
// 5- MesesDeContrato,
// 6- TelefoneFixo,
// 7- MaisDeUmaLinhaTelefonica,
// 8- Internet,
// 9- SegurancaOnline,
// 10- BackupOnline,
// 11- SeguroDispositivo,
// 12- SuporteTecnico,
// 13- TVaCabo,
// 14- StreamingFilmes,
// 15- TipoContrato,
// 16- ContaCorreio,
// 17- MetodoPagamento,
// 18- MesesCobrados

val concat = mergeArray(rdd1,3,4,6,7,8,9,10,11,12,13,14,15,16,17)
3 -> Map(Sim -> 1, Nao -> 0)
4 -> Map(Sim -> 1, Nao -> 0)
6 -> Map(Sim -> 1, Nao -> 0)
7 -> Map(SemServicoTelefonico -> 0, Sim -> 2, Nao -> 1)
8 -> Map(Nao -> 0, DSL -> 2, FibraOptica -> 1)
9 -> Map(SemServicoInternet -> 1, Sim -> 2, Nao -> 0)
10 -> Map(SemServicoInternet -> 1, Sim -> 2, Nao -> 0)
11 -> Map(SemServicoInternet -> 1, Sim -> 2, Nao -> 0)
12 -> Map(SemServicoInternet -> 1, Sim -> 2, Nao -> 0)
13 -> Map(SemServicoInternet -> 1, Sim -> 2, Nao -> 0)
14 -> Map(SemServicoInternet -> 1, Sim -> 2, Nao -> 0)
15 -> Map(Mensalmente -> 2, UmAno -> 1, DoisAnos -> 0)
16 -> Map(Sim -> 1, Nao -> 0)
17 -> Map(BoletoEletronico -> 2, DebitoEmConta -> 1, Boleto -> 3, CartaoCredito -> 0)

concat.take(2)
res0: Array[Array[Double]] = Array(Array(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0), Array(0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0))

// Mais65anos, MesesDeContrato, MesesCobrados, Churn
val rdd2 = rdd1.map( x => {
  val y = Array(x(2),x(5),x(18),categ_simnao(x(1)).toString)
  y.map( z => z.toDouble)
})

rdd2.take(2)
res1: Array[Array[Double]] = Array(Array(0.0, 1.0, 29.85, 0.0), Array(0.0, 34.0, 56.95, 0.0))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(10).map( x => x.mkString(", ")).foreach(println)
1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 29.85, 0.0
0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 34.0, 56.95, 0.0
0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 53.85, 1.0
0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 45.0, 42.3, 0.0
0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 2.0, 70.7, 1.0
0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 8.0, 99.65, 1.0
0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 22.0, 89.1, 0.0
0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 10.0, 29.75, 0.0
1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 28.0, 104.8, 1.0
0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 62.0, 56.15, 0.0


---- calculate cosine column similarities to check multicolinearity

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = vect.map( x => Vectors.dense(x.slice(0, x.size - 1)))
val matrix = new RowMatrix(vectors)

val colsims = matrix.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0
var j = 0.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    i = x._1._1
    j = i + 1
    while (j < x._1._2) { 
      print("0.0000 ")
      j = j + 1
    }
    print(f"$sim%.4f ")
  } else {
      j = j + 1
      while (j < x._1._2) { 
        print("0.0000 ")
        j = j + 1
      }
      print(f"$sim%.4f ")
    }
})
// Conjuge,Dependentes,TelefoneFixo,MaisDeUmaLinhaTelefonica,Internet,SegurancaOnline,BackupOnline,SeguroDispositivo,SuporteTecnico,TVaCabo,StreamingFilmes,TipoContrato,ContaCorreio,MetodoPagamento,Mais65anos,MesesDeContrato,MesesCobrados
0.6246 0.6311 0.3748 0.5205 0.4807 0.3543 0.4453 0.4169 0.3853 0.4754 0.3756 0.4875 0.4488 0.4088 0.3503 0.4968 0.3507 0.4962 0.4100 0.3515 0.5253 0.2339 0.3407 0.3647 0.2777 0.6981 0.6383
0.4707 0.3659 0.2987 0.2463 0.3197 0.2595 0.3243 0.2573 0.3038 0.2511 0.3116 0.2610 0.3192 0.2564 0.2964 0.2727 0.2794 0.2658 0.2786 0.3267 0.2817 0.2767 0.2693 0.0500 0.4800 0.4101
0.7196 0.6944 0.7595 0.4054 0.7298 0.4340 0.6628 0.5306 0.6679 0.5242 0.7236 0.4442 0.6016 0.5993 0.6024 0.5985 0.7770 0.3837 0.7757 0.4235 0.3940 0.4038 0.3620 0.7185 0.9119
0.0000 0.3940 0.3645 0.4676 0.2598 0.4815 0.2529 0.4814 0.2531 0.4650 0.2645 0.4719 0.2808 0.4804 0.2723 0.5869 0.2715 0.4861 0.4586 0.2533 0.2571 0.1684 0.3655 0.5337
0.6855 0.2061 0.5664 0.3557 0.4556 0.5021 0.4630 0.4926 0.5601 0.3657 0.3773 0.5721 0.3697 0.5797 0.5107 0.2711 0.6133 0.1347 0.3050 0.3152 0.3468 0.6559 0.7603
0.0000 0.7544 0.2840 0.6229 0.4893 0.6227 0.4896 0.7464 0.2999 0.4905 0.6326 0.4961 0.6264 0.6964 0.2243 0.7308 0.1055 0.2429 0.2671 0.4015 0.5382 0.8828
0.4209 0.4826 0.4768 0.3810 0.4764 0.3815 0.4221 0.4768 0.5427 0.3164 0.5335 0.3263 0.4117 0.2712 0.4048 0.3124 0.2866 0.2540 0.1295 0.4247 0.4207
0.0000 0.7347 0.4484 0.7289 0.4559 0.8173 0.3040 0.6262 0.5815 0.6340 0.5735 0.8000 0.2073 0.7520 0.2159 0.2282 0.2450 0.3669 0.4607 0.8099
0.3016 0.4562 0.3103 0.4450 0.2898 0.5174 0.3555 0.3798 0.3405 0.3953 0.2296 0.3197 0.3531 0.1727 0.3318 0.3028 0.1608 0.5618 0.5084
0.0000 0.6966 0.3839 0.7314 0.3096 0.6315 0.4753 0.6292 0.4779 0.7311 0.1841 0.6496 0.2442 0.2132 0.2205 0.2986 0.3701 0.7063
0.3830 0.5152 0.4436 0.4608 0.3618 0.5182 0.3629 0.5168 0.3699 0.3205 0.5102 0.1332 0.3227 0.3123 0.2626 0.6372 0.6483
0.0000 0.7485 0.2812 0.6893 0.4147 0.6889 0.4155 0.7483 0.1748 0.6575 0.2495 0.2182 0.2267 0.3092 0.3786 0.6966
0.4221 0.4965 0.2889 0.5945 0.2876 0.5954 0.3483 0.3321 0.5003 0.1265 0.3162 0.3043 0.2491 0.6262 0.6605
0.0000 0.6675 0.5307 0.6639 0.5346 0.8097 0.2028 0.7334 0.2120 0.2275 0.2401 0.3760 0.4554 0.7945
0.2898 0.4610 0.2933 0.4570 0.2186 0.3236 0.3848 0.1783 0.3295 0.3081 0.1473 0.5656 0.5329
0.0000 0.7403 0.2700 0.6581 0.1817 0.5480 0.2847 0.2487 0.2354 0.2686 0.3567 0.5874
0.2676 0.7218 0.4743 0.3061 0.6217 0.0998 0.2712 0.2841 0.2935 0.6177 0.7683
0.0000 0.6618 0.1728 0.5507 0.2868 0.2352 0.2291 0.2513 0.3520 0.5874
0.4707 0.3150 0.6187 0.0979 0.2852 0.2905 0.3113 0.6220 0.7678
0.0000 0.7363 0.3494 0.2019 0.2252 0.3492 0.3640 0.7687
0.2820 0.1895 0.2681 0.2446 0.1293 0.4755 0.3720
0.2397 0.3042 0.3064 0.3728 0.5993 0.8338
0.0000 0.0000 0.0590 0.2203 0.2659
0.0000 0.1491 0.4718 0.3827
0.1521 0.4759 0.3901
0.3151 0.4123
0.7510

// TelefoneFixo x Mais65anos = 0.9119 can be considered multicolinear
// DebitoEmConta x MaisDeUmaLinhaTelefonica_Sim = 0.8828 can be considered multicolinear
// MaisDeUmaLinhaTelefonica_Sim x TipoContrato_UmAno = 0.8338 can be considered multicolinear


---- Splitting dataset as train/test sets  --------------

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)
   val f = x.slice(0, arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache
testSet.cache


----- Analyze the individual statistics and possibly standardize 

import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics

matrixSummary.max.toArray.foreach( x => print(f"$x%.2f\t"))
1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00     1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    72.00   118.75

matrixSummary.min.toArray.foreach( x => print(f"$x%.2f\t"))
0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00     0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    18.25

matrixSummary.mean.toArray.foreach( x => print(f"$x%.2f\t"))
0.43    0.24    0.91    0.47    0.44    0.52    0.31    0.17    0.22    0.17    0.32    0.17    0.32    0.17    0.23    0.17    0.40    0.17     0.40    0.16    0.67    0.66    0.17    0.45    0.20    0.14    27.49   67.75

matrixSummary.variance.toArray.foreach( x => print(f"$x%.2f\t"))
0.25    0.18    0.08    0.25    0.25    0.25    0.21    0.14    0.17    0.14    0.22    0.14    0.22    0.14    0.18    0.14    0.24    0.14     0.24    0.14    0.22    0.22    0.14    0.25    0.16    0.12    580.45  831.14


----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache
testScaled.cache


---- MLlib logistic regression STD --------------

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD

val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(100).setRegParam(0.01)
val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res24: Double = 0.7309077763499346

metrics.areaUnderROC
res25: Double = 0.783001255617235

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res26: Double = 0.7821357943309163

metrics1.confusionMatrix
res27: org.apache.spark.mllib.linalg.Matrix =
1157.0  395.0
266.0   1216.0


---- Hyperparameter tunning with logistic regression STD --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], regSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step, reg  -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes; reg <- regSizes) {
    val model = new LogisticRegressionWithSGD
    model.optimizer.setNumIterations(numIter).setStepSize(step).setRegParam(reg)
    val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, reg, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(10, 20, 40, 100),Array(1, 0.1, 0.01, 0.001),Array(0.1, 0.01), trainScaled, testScaled)
iter, step, reg  -> pred / total, AuPR, AuROC
10, 1.000, 0.100 -> 2357 / 3034, 0.7257, 0.7777
10, 1.000, 0.010 -> 2359 / 3034, 0.7265, 0.7784
10, 0.100, 0.100 -> 2320 / 3034, 0.7063, 0.7662
10, 0.100, 0.010 -> 2320 / 3034, 0.7063, 0.7662
10, 0.010, 0.100 -> 2297 / 3034, 0.6949, 0.7590
10, 0.010, 0.010 -> 2297 / 3034, 0.6949, 0.7590
10, 0.001, 0.100 -> 2301 / 3034, 0.6958, 0.7604
10, 0.001, 0.010 -> 2301 / 3034, 0.6958, 0.7604
20, 1.000, 0.100 -> 2356 / 3034, 0.7251, 0.7774
20, 1.000, 0.010 -> 2360 / 3034, 0.7270, 0.7787
20, 0.100, 0.100 -> 2333 / 3034, 0.7120, 0.7703
20, 0.100, 0.010 -> 2333 / 3034, 0.7120, 0.7703
20, 0.010, 0.100 -> 2305 / 3034, 0.6979, 0.7616
20, 0.010, 0.010 -> 2305 / 3034, 0.6979, 0.7616
20, 0.001, 0.100 -> 2301 / 3034, 0.6958, 0.7604
20, 0.001, 0.010 -> 2301 / 3034, 0.6958, 0.7604
40, 1.000, 0.100 -> 2359 / 3034, 0.7258, 0.7784
40, 1.000, 0.010 -> 2373 / 3034, 0.7313, 0.7830 *
40, 0.100, 0.100 -> 2337 / 3034, 0.7148, 0.7715
40, 0.100, 0.010 -> 2338 / 3034, 0.7152, 0.7718
40, 0.010, 0.100 -> 2308 / 3034, 0.6993, 0.7625
40, 0.010, 0.010 -> 2309 / 3034, 0.6997, 0.7629
40, 0.001, 0.100 -> 2301 / 3034, 0.6958, 0.7604
40, 0.001, 0.010 -> 2301 / 3034, 0.6958, 0.7604
100, 1.000, 0.100 -> 2359 / 3034, 0.7258, 0.7784
100, 1.000, 0.010 -> 2373 / 3034, 0.7309, 0.7830
100, 0.100, 0.100 -> 2349 / 3034, 0.7210, 0.7753
100, 0.100, 0.010 -> 2352 / 3034, 0.7223, 0.7762
100, 0.010, 0.100 -> 2308 / 3034, 0.6993, 0.7625
100, 0.010, 0.010 -> 2309 / 3034, 0.6997, 0.7629
100, 0.001, 0.100 -> 2301 / 3034, 0.6958, 0.7604
100, 0.001, 0.010 -> 2301 / 3034, 0.6958, 0.7604


val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(40).setStepSize(1.0).setRegParam(0.01)

val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))

validPredicts.take(20)
res30: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res31: Double = 0.7313049591280343

metrics.areaUnderROC
res32: Double = 0.7829708216814836

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res33: Double = 0.7821357943309163

metrics1.confusionMatrix
res34: org.apache.spark.mllib.linalg.Matrix =
1159.0  393.0
268.0   1214.0


----- with MLlib SVM regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AreaPR, AreaROC") 
  for(numIter <- iterNums) {
    val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(10, 20, 40, 100), trainScaled, testScaled)
iter -> pred / total, AreaPR, AreaROC
10 -> 2354 / 3034, 0.7233, 0.7768 *
20 -> 2350 / 3034, 0.7211, 0.7756
40 -> 2344 / 3034, 0.7184, 0.7737
100 -> 2344 / 3034, 0.7177, 0.7737


val model = SVMWithSGD.train(trainScaled, 10)

val validPredicts = testScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res36: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res37: Double = 0.7232756614687276

metrics.areaUnderROC
res38: Double = 0.776849687660865

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res39: Double = 0.7758734344100198

metrics1.confusionMatrix
res40: org.apache.spark.mllib.linalg.Matrix =
1140.0  412.0
268.0   1214.0

----- with MLlib Decision tree regression ----------------------

val categ_maisdeumalinha = rdd1.map(x => x(7)).distinct.zipWithIndex.collectAsMap
categ_maisdeumalinha: scala.collection.Map[String,Long] = Map(SemServicoTelefonico -> 0, Sim -> 2, Nao -> 1)

val categ_internet = rdd1.map(x => x(8)).distinct.zipWithIndex.collectAsMap
categ_internet: scala.collection.Map[String,Long] = Map(Nao -> 0, DSL -> 2, FibraOptica -> 1)

val categ_seguranca = rdd1.map(x => x(9)).distinct.zipWithIndex.collectAsMap
categ_seguranca: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_backup = rdd1.map(x => x(10)).distinct.zipWithIndex.collectAsMap
categ_backup: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_seguro = rdd1.map(x => x(11)).distinct.zipWithIndex.collectAsMap
categ_seguro: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_suporte = rdd1.map(x => x(12)).distinct.zipWithIndex.collectAsMap
categ_suporte: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_tvacabo = rdd1.map(x => x(13)).distinct.zipWithIndex.collectAsMap
categ_tvacabo: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_streaming = rdd1.map(x => x(14)).distinct.zipWithIndex.collectAsMap
categ_streaming: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_tipocontrato = rdd1.map(x => x(15)).distinct.zipWithIndex.collectAsMap
categ_tipocontrato: scala.collection.Map[String,Long] = Map(Mensalmente -> 1, UmAno -> 2, DoisAnos -> 0)

val categ_metodopagto = rdd1.map(x => x(17)).distinct.zipWithIndex.collectAsMap
categ_metodopagto: scala.collection.Map[String,Long] = Map(BoletoEletronico -> 0, DebitoEmConta -> 3, Boleto -> 1, CartaoCredito -> 2)

val rdd2_dt = rdd1.map( x => {
  val y = Array(x(2),categ_simnao(x(3)),categ_simnao(x(4)),x(5),categ_simnao(x(6)),categ_maisdeumalinha(x(7)),categ_internet(x(8)),categ_seguranca(x(9)),categ_backup(x(10)),categ_seguro(x(11)),categ_suporte(x(12)),categ_tvacabo(x(13)),categ_streaming(x(14)),categ_tipocontrato(x(15)),categ_simnao(x(16)),categ_metodopagto(x(17)),x(18),categ_simnao(x(1)))
  y.map( z => z.toString.toDouble)
})

rdd2_dt.take(2)
res3: Array[Array[Double]] = Array(Array(0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 29.85, 0.0), Array(0.0, 0.0, 0.0, 34.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 56.95, 0.0))

rdd2_dt.take(10).map( x => x.mkString(", ")).foreach(println)
0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 2.0, 29.85, 0.0
0.0, 0.0, 0.0, 34.0, 1.0, 1.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 56.95, 0.0
0.0, 0.0, 0.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 3.0, 53.85, 1.0
0.0, 0.0, 0.0, 45.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 1.0, 0.0, 1.0, 42.3, 0.0
0.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 2.0, 70.7, 1.0
0.0, 0.0, 0.0, 8.0, 1.0, 2.0, 1.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 2.0, 99.65, 1.0
0.0, 0.0, 1.0, 22.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 89.1, 0.0
0.0, 0.0, 0.0, 10.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 3.0, 29.75, 0.0
0.0, 1.0, 0.0, 28.0, 1.0, 2.0, 1.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 104.8, 1.0
0.0, 0.0, 1.0, 62.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 56.15, 0.0

val data = rdd2_dt.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)
   val f = x.slice(0, arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })
 
val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache
testSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 0->2 , 1->2, 2->2, 4->2, 5->3, 6->3, 7->3, 8->3, 9->3, 10->3, 11->3, 12->3, 13->3, 14->2, 15->4)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, AreaPR, AreaROC") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AreaPR, AreaROC
gini, 10, 32 -> 2360 / 3034, 0.7243, 0.7789
gini, 10, 48 -> 2385 / 3034, 0.7298, 0.7874
gini, 10, 64 -> 2391 / 3034, 0.7332, 0.7892
gini, 20, 32 -> 2393 / 3034, 0.7472, 0.7889
gini, 20, 48 -> 2376 / 3034, 0.7402, 0.7834
gini, 20, 64 -> 2376 / 3034, 0.7408, 0.7833
gini, 30, 32 -> 2390 / 3034, 0.7468, 0.7879
gini, 30, 48 -> 2375 / 3034, 0.7404, 0.7830
gini, 30, 64 -> 2376 / 3034, 0.7413, 0.7833
entropy, 10, 32 -> 2380 / 3034, 0.7362, 0.7851
entropy, 10, 48 -> 2379 / 3034, 0.7303, 0.7852
entropy, 10, 64 -> 2396 / 3034, 0.7373, 0.7907
entropy, 20, 32 -> 2403 / 3034, 0.7486, 0.7923 *
entropy, 20, 48 -> 2397 / 3034, 0.7463, 0.7904
entropy, 20, 64 -> 2382 / 3034, 0.7413, 0.7854
entropy, 30, 32 -> 2395 / 3034, 0.7470, 0.7896
entropy, 30, 48 -> 2395 / 3034, 0.7463, 0.7897
entropy, 30, 64 -> 2380 / 3034, 0.7417, 0.7847

val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "entropy", 20, 32)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res47: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (1.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res48: Double = 0.7485902219666455

metrics.areaUnderROC
res49: Double = 0.7923466477454539

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)
metrics1: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@5f986fcb

metrics1.accuracy
res50: Double = 0.7920237310481213

metrics1.confusionMatrix
res51: org.apache.spark.mllib.linalg.Matrix =
1208.0  344.0
287.0   1195.0