---- Feature extraction & Data Munging --------------

val colname = sc.textFile("churn_bra/dados_clientes.csv").filter( x => x.contains("Mais65anos")).take(1)
res11: Array[String] = Array(id,Churn,Mais65anos,Conjuge,Dependentes,MesesDeContrato,TelefoneFixo,MaisDeUmaLinhaTelefonica,Internet,SegurancaOnline,BackupOnline,SeguroDispositivo,SuporteTecnico,TVaCabo,StreamingFilmes,TipoContrato,ContaCorreio,MetodoPagamento,MesesCobrados)

val rdd = sc.textFile("churn_bra/dados_clientes.csv").filter( x => ! x.contains("Mais65anos"))

val rdd1 = rdd.map( x => x.split(",")).map( x => x.slice(1,x.size))

rdd1.take(5)
res9: Array[Array[String]] = Array(Array(Nao, 0, Sim, Nao, 1, Nao, SemServicoTelefonico, DSL, Nao, Sim, Nao, Nao, Nao, Nao, Mensalmente, Sim, BoletoEletronico, 29.85), Array(Nao, 0, Nao, Nao, 34, Sim, Nao, DSL, Sim, Nao, Sim, Nao, Nao, Nao, UmAno, Nao, Boleto, 56.95), Array(Sim, 0, Nao, Nao, 2, Sim, Nao, DSL, Sim, Sim, Nao, Nao, Nao, Nao, Mensalmente, Sim, Boleto, 53.85), Array(Nao, 0, Nao, Nao, 45, Nao, SemServicoTelefonico, DSL, Sim, Nao, Sim, Sim, Nao, Nao, UmAno, Nao, DebitoEmConta, 42.3), Array(Sim, 0, Nao, Nao, 2, Sim, Nao, FibraOptica, Nao, Nao, Nao, Nao, Nao, Nao, Mensalmente, Sim, BoletoEletronico, 70.7))

val categ_simnao = rdd1.map(x => x(0)).distinct.zipWithIndex.collectAsMap
categ_simnao: scala.collection.Map[String,Long] = Map(Sim -> 1, Nao -> 0)

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  print(f"$idx%.0f -> "); println(categories)
  
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
    val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories - 1) else Array.ofDim[Double](1)
    if (numCategories > 2) { 
      if (categoryIdx > 0) categoryFeatures(categoryIdx - 1) = 1.0
    }
    else categoryFeatures(0) = categoryIdx
    categoryFeatures
    })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

// 0- Churn,
// 1- Mais65anos,
// 2- Conjuge,
// 3- Dependentes,
// 4- MesesDeContrato,
// 5- TelefoneFixo,
// 6- MaisDeUmaLinhaTelefonica,
// 7- Internet,
// 8- SegurancaOnline,
// 9- BackupOnline,
// 10- SeguroDispositivo,
// 11- SuporteTecnico,
// 12- TVaCabo,
// 13- StreamingFilmes,
// 14- TipoContrato,
// 15- ContaCorreio,
// 16- MetodoPagamento,
// 17- MesesCobrados

val concat = mergeArray(rdd1,2,3,5,6,7,8,9,10,11,12,13,14,15,16)
2 -> Map(Sim -> 1, Nao -> 0)
3 -> Map(Sim -> 1, Nao -> 0)
5 -> Map(Sim -> 1, Nao -> 0)
6 -> Map(SemServicoTelefonico -> 0, Sim -> 2, Nao -> 1)
7 -> Map(Nao -> 0, DSL -> 2, FibraOptica -> 1)
8 -> Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)
9 -> Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)
10 -> Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)
11 -> Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)
12 -> Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)
13 -> Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)
14 -> Map(Mensalmente -> 1, UmAno -> 2, DoisAnos -> 0)
15 -> Map(Sim -> 1, Nao -> 0)
16 -> Map(BoletoEletronico -> 0, DebitoEmConta -> 3, Boleto -> 1, CartaoCredito -> 2)

concat.take(2)
res0: Array[Array[Double]] = Array(Array(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0), Array(0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0))

// Mais65anos, MesesDeContrato, MesesCobrados, Churn
val rdd2 = rdd1.map( x => {
  val y = Array(x(1),x(4),x(17),categ_simnao(x(0)).toString)
  y.map( z => z.toDouble)
})

rdd2.take(2)
res1: Array[Array[Double]] = Array(Array(0.0, 1.0, 29.85, 0.0), Array(0.0, 34.0, 56.95, 0.0))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(2)
res2: Array[Array[Double]] = Array(Array(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 29.85, 0.0), Array(0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 34.0, 56.95, 0.0))

val categ_maisdeumalinha = rdd1.map(x => x(6)).distinct.zipWithIndex.collectAsMap
categ_maisdeumalinha: scala.collection.Map[String,Long] = Map(SemServicoTelefonico -> 0, Sim -> 2, Nao -> 1)

val categ_internet = rdd1.map(x => x(7)).distinct.zipWithIndex.collectAsMap
categ_internet: scala.collection.Map[String,Long] = Map(Nao -> 0, DSL -> 2, FibraOptica -> 1)

val categ_seguranca = rdd1.map(x => x(8)).distinct.zipWithIndex.collectAsMap
categ_seguranca: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_backup = rdd1.map(x => x(9)).distinct.zipWithIndex.collectAsMap
categ_backup: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_seguro = rdd1.map(x => x(10)).distinct.zipWithIndex.collectAsMap
categ_seguro: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_suporte = rdd1.map(x => x(11)).distinct.zipWithIndex.collectAsMap
categ_suporte: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_tvacabo = rdd1.map(x => x(12)).distinct.zipWithIndex.collectAsMap
categ_tvacabo: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_streaming = rdd1.map(x => x(13)).distinct.zipWithIndex.collectAsMap
categ_streaming: scala.collection.Map[String,Long] = Map(SemServicoInternet -> 0, Sim -> 2, Nao -> 1)

val categ_tipocontrato = rdd1.map(x => x(14)).distinct.zipWithIndex.collectAsMap
categ_tipocontrato: scala.collection.Map[String,Long] = Map(Mensalmente -> 1, UmAno -> 2, DoisAnos -> 0)

val categ_metodopagto = rdd1.map(x => x(16)).distinct.zipWithIndex.collectAsMap
categ_metodopagto: scala.collection.Map[String,Long] = Map(BoletoEletronico -> 0, DebitoEmConta -> 3, Boleto -> 1, CartaoCredito -> 2)

val rdd2_dt = rdd1.map( x => {
  val y = Array(x(1),categ_simnao(x(2)),categ_simnao(x(3)),x(4),categ_simnao(x(5)),categ_maisdeumalinha(x(6)),categ_internet(x(7)),categ_seguranca(x(8)),categ_backup(x(9)),categ_seguro(x(10)),categ_suporte(x(11)),categ_tvacabo(x(12)),categ_streaming(x(13)),categ_tipocontrato(x(14)),categ_simnao(x(15)),categ_metodopagto(x(16)),x(17),categ_simnao(x(0)))
  y.map( z => z.toString.toDouble)
})

rdd2_dt.take(2)
res3: Array[Array[Double]] = Array(Array(0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 29.85, 0.0), Array(0.0, 0.0, 0.0, 34.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 56.95, 0.0))

val data = vect.zip(rdd2_dt)

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val train_rdd = sets(0)
val test_rdd = sets(1)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint  

val trainSet = train_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache
testSet.cache

---- calculate cosine column similarities to check multicolinearity

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = vect.map( x => Vectors.dense(x.slice(0, x.size - 1)))
val matrix = new RowMatrix(vectors)

val colsims = matrix.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0
var j = 0.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    i = x._1._1
    j = i + 1
    while (j < x._1._2) { 
      print("0.0000 ")
      j = j + 1
    }
    print(f"$sim%.4f ")
  } else {
      j = j + 1
      while (j < x._1._2) { 
        print("0.0000 ")
        j = j + 1
      }
      print(f"$sim%.4f ")
    }
})
// Conjuge,Dependentes,TelefoneFixo,MaisDeUmaLinhaTelefonica,Internet,SegurancaOnline,BackupOnline,SeguroDispositivo,SuporteTecnico,TVaCabo,StreamingFilmes,TipoContrato,ContaCorreio,MetodoPagamento,Mais65anos,MesesDeContrato,MesesCobrados
0.6246 0.6311 0.3748 0.5205 0.4807 0.3543 0.4453 0.4169 0.3853 0.4754 0.3756 0.4875 0.4488 0.4088 0.3503 0.4968 0.3507 0.4962 0.4100 0.3515 0.5253 0.2339 0.3407 0.3647 0.2777 0.6981 0.6383
0.4707 0.3659 0.2987 0.2463 0.3197 0.2595 0.3243 0.2573 0.3038 0.2511 0.3116 0.2610 0.3192 0.2564 0.2964 0.2727 0.2794 0.2658 0.2786 0.3267 0.2817 0.2767 0.2693 0.0500 0.4800 0.4101
0.7196 0.6944 0.7595 0.4054 0.7298 0.4340 0.6628 0.5306 0.6679 0.5242 0.7236 0.4442 0.6016 0.5993 0.6024 0.5985 0.7770 0.3837 0.7757 0.4235 0.3940 0.4038 0.3620 0.7185 0.9119
0.0000 0.3940 0.3645 0.4676 0.2598 0.4815 0.2529 0.4814 0.2531 0.4650 0.2645 0.4719 0.2808 0.4804 0.2723 0.5869 0.2715 0.4861 0.4586 0.2533 0.2571 0.1684 0.3655 0.5337
0.6855 0.2061 0.5664 0.3557 0.4556 0.5021 0.4630 0.4926 0.5601 0.3657 0.3773 0.5721 0.3697 0.5797 0.5107 0.2711 0.6133 0.1347 0.3050 0.3152 0.3468 0.6559 0.7603
0.0000 0.7544 0.2840 0.6229 0.4893 0.6227 0.4896 0.7464 0.2999 0.4905 0.6326 0.4961 0.6264 0.6964 0.2243 0.7308 0.1055 0.2429 0.2671 0.4015 0.5382 0.8828
0.4209 0.4826 0.4768 0.3810 0.4764 0.3815 0.4221 0.4768 0.5427 0.3164 0.5335 0.3263 0.4117 0.2712 0.4048 0.3124 0.2866 0.2540 0.1295 0.4247 0.4207
0.0000 0.7347 0.4484 0.7289 0.4559 0.8173 0.3040 0.6262 0.5815 0.6340 0.5735 0.8000 0.2073 0.7520 0.2159 0.2282 0.2450 0.3669 0.4607 0.8099
0.3016 0.4562 0.3103 0.4450 0.2898 0.5174 0.3555 0.3798 0.3405 0.3953 0.2296 0.3197 0.3531 0.1727 0.3318 0.3028 0.1608 0.5618 0.5084
0.0000 0.6966 0.3839 0.7314 0.3096 0.6315 0.4753 0.6292 0.4779 0.7311 0.1841 0.6496 0.2442 0.2132 0.2205 0.2986 0.3701 0.7063
0.3830 0.5152 0.4436 0.4608 0.3618 0.5182 0.3629 0.5168 0.3699 0.3205 0.5102 0.1332 0.3227 0.3123 0.2626 0.6372 0.6483
0.0000 0.7485 0.2812 0.6893 0.4147 0.6889 0.4155 0.7483 0.1748 0.6575 0.2495 0.2182 0.2267 0.3092 0.3786 0.6966
0.4221 0.4965 0.2889 0.5945 0.2876 0.5954 0.3483 0.3321 0.5003 0.1265 0.3162 0.3043 0.2491 0.6262 0.6605
0.0000 0.6675 0.5307 0.6639 0.5346 0.8097 0.2028 0.7334 0.2120 0.2275 0.2401 0.3760 0.4554 0.7945
0.2898 0.4610 0.2933 0.4570 0.2186 0.3236 0.3848 0.1783 0.3295 0.3081 0.1473 0.5656 0.5329
0.0000 0.7403 0.2700 0.6581 0.1817 0.5480 0.2847 0.2487 0.2354 0.2686 0.3567 0.5874
0.2676 0.7218 0.4743 0.3061 0.6217 0.0998 0.2712 0.2841 0.2935 0.6177 0.7683
0.0000 0.6618 0.1728 0.5507 0.2868 0.2352 0.2291 0.2513 0.3520 0.5874
0.4707 0.3150 0.6187 0.0979 0.2852 0.2905 0.3113 0.6220 0.7678
0.0000 0.7363 0.3494 0.2019 0.2252 0.3492 0.3640 0.7687
0.2820 0.1895 0.2681 0.2446 0.1293 0.4755 0.3720
0.2397 0.3042 0.3064 0.3728 0.5993 0.8338
0.0000 0.0000 0.0590 0.2203 0.2659
0.0000 0.1491 0.4718 0.3827
0.1521 0.4759 0.3901
0.3151 0.4123
0.7510

// TelefoneFixo x Mais65anos = 0.9119 can be considered multicolinear
// DebitoEmConta x MaisDeUmaLinhaTelefonica_Sim = 0.8828 can be considered multicolinear
// MaisDeUmaLinhaTelefonica_Sim x TipoContrato_UmAno = 0.8338 can be considered multicolinear

---- MLlib logistic regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AreaPR, AreaROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
    model.optimizer.setNumIterations(numIter).setStepSize(step)
    val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainSet, testSet)
iter, step -> pred / total, AreaPR, AreaROC
100, 0.100 -> 2001 / 3011, 0.7283, 0.6626
100, 0.010 -> 2185 / 3011, 0.6719, 0.7263 *
100, 0.001 -> 2181 / 3011, 0.6671, 0.7251
300, 0.100 -> 2083 / 3011, 0.7421, 0.6902
300, 0.010 -> 2185 / 3011, 0.6719, 0.7263
300, 0.001 -> 2181 / 3011, 0.6671, 0.7251
500, 0.100 -> 2078 / 3011, 0.7488, 0.6884
500, 0.010 -> 2185 / 3011, 0.6719, 0.7263
500, 0.001 -> 2181 / 3011, 0.6671, 0.7251

---- MLlib SVM regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AreaPR, AreaROC") 
  for(numIter <- iterNums) {
    val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(100,300,500), trainSet, testSet)
iter, step -> pred / total, AreaPR, AreaROC
100 -> 2012 / 3011, 0.7294, 0.6663 *
300 -> 1984 / 3011, 0.7361, 0.6568
500 -> 2001 / 3011, 0.7431, 0.6625

---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res11: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,72.0,118.75]

matrixSummary.min
res12: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,18.25]

matrixSummary.mean
res13: org.apache.spark.mllib.linalg.Vector = [0.4345100177184135,0.239880059970015,0.9054109308981872,0.4711735041570124,0.4342374267411749,0.5195584026168734,0.3108900095406842,0.6085593566852937,0.22188905547226387,0.5111080823224752,0.3193403298350825,0.5079732860842306,0.32247512607332696,0.6011994002998501,0.22924901185770752,0.4287856071964018,0.4016628049611558,0.42483303802644135,0.40561537413111626,0.6675753032574622,0.16137385852528283,0.6633501431102631,0.19694698105492708,0.1715960201717323,0.17404933896688018,0.13997546681204853,27.700013629548923,67.66625007885341]

matrixSummary.variance
res14: org.apache.spark.mllib.linalg.Vector = [0.245744556095106,0.18236247198102148,0.08565365131452317,0.24920299838184407,0.2457087729556457,0.24965149525960723,0.21426661508147482,0.2382473381134356,0.17267783774415954,0.24991067220429636,0.21739171308566,0.24997049655998807,0.21851470170535828,0.2397913638600547,0.17671798828453325,0.24496189745912214,0.24036255640396317,0.24438323616024843,0.24112440656840212,0.22194876835400085,0.13535078401118664,0.22334717195779033,0.15818042698686346,0.1421702031902657,0.14377576255988195,0.12039874530861862,583.5033710467045,831.5976507951368]

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache
testScaled.cache

----- with MLlib logistic regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AreaPR, AreaROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
    model.optimizer.setNumIterations(numIter).setStepSize(step)
    val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainScaled, testScaled)
iter, step -> pred / total, AreaPR, AreaROC
100, 0.100 -> 2342 / 3011, 0.7310, 0.7781
100, 0.010 -> 2342 / 3011, 0.7301, 0.7781
100, 0.001 -> 2340 / 3011, 0.7294, 0.7775
300, 0.100 -> 2344 / 3011, 0.7323, 0.7788 *
300, 0.010 -> 2342 / 3011, 0.7301, 0.7781
300, 0.001 -> 2340 / 3011, 0.7294, 0.7775
500, 0.100 -> 2344 / 3011, 0.7323, 0.7788
500, 0.010 -> 2342 / 3011, 0.7301, 0.7781
500, 0.001 -> 2340 / 3011, 0.7294, 0.7775

----- with MLlib SVM regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AreaPR, AreaROC") 
  for(numIter <- iterNums) {
    val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(100,300,500), trainScaled, testScaled)
iter -> pred / total, AreaPR, AreaROC
100 -> 2336 / 3011, 0.7256, 0.7762
300 -> 2336 / 3011, 0.7256, 0.7762
500 -> 2336 / 3011, 0.7256, 0.7762

----- with MLlib Decision tree regression ----------------------

val trainSet = train_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache
testSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 0->2 , 1->2, 2->2, 4->2, 5->3, 6->3, 7->3, 8->3, 9->3, 10->3, 11->3, 12->3, 13->3, 14->2, 15->4)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, AreaPR, AreaROC") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AreaPR, AreaROC
gini, 10, 32 -> 2375 / 3011, 0.7360, 0.7892
gini, 10, 48 -> 2376 / 3011, 0.7357, 0.7896
gini, 10, 64 -> 2359 / 3011, 0.7339, 0.7838
gini, 20, 32 -> 2337 / 3011, 0.7361, 0.7763
gini, 20, 48 -> 2352 / 3011, 0.7426, 0.7812
gini, 20, 64 -> 2347 / 3011, 0.7399, 0.7796
gini, 30, 32 -> 2338 / 3011, 0.7363, 0.7766
gini, 30, 48 -> 2350 / 3011, 0.7421, 0.7806
gini, 30, 64 -> 2348 / 3011, 0.7401, 0.7799
entropy, 10, 32 -> 2390 / 3011, 0.7423, 0.7942 *
entropy, 10, 48 -> 2358 / 3011, 0.7336, 0.7835
entropy, 10, 64 -> 2374 / 3011, 0.7406, 0.7888
entropy, 20, 32 -> 2362 / 3011, 0.7441, 0.7846
entropy, 20, 48 -> 2356 / 3011, 0.7410, 0.7826
entropy, 20, 64 -> 2376 / 3011, 0.7482, 0.7893
entropy, 30, 32 -> 2362 / 3011, 0.7446, 0.7846
entropy, 30, 48 -> 2345 / 3011, 0.7397, 0.7789
entropy, 30, 64 -> 2370 / 3011, 0.7473, 0.7873

val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "entropy", 10, 32)

model.toDebugString
res65: String =
"DecisionTreeModel classifier of depth 10 with 853 nodes
  If (feature 13 in {0.0,2.0})
   If (feature 12 in {0.0,1.0})
    If (feature 16 <= 45.364659265398345)
     If (feature 3 <= 60.5)
      If (feature 3 <= 52.5)
       If (feature 16 <= 20.825000000000003)
        If (feature 15 in {1.0,2.0})
         Predict: 0.0
        Else (feature 15 not in {1.0,2.0})
         If (feature 13 in {0.0})
          Predict: 0.0
         Else (feature 13 not in {0.0})
          If (feature 3 <= 18.5)
           If (feature 2 in {0.0})
            Predict: 0.0
           Else (feature 2 not in {0.0})
            Predict: 0.0
          Else (feature 3 > 18.5)
           If (feature 15 in {3.0})
            Predict: 0.0
           Else (feature 15 not in {3.0})
            Predict: 0...

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res66: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 2390
validPredicts.count                            // 3011
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.7423358599722082
metrics.areaUnderROC  // 0.7941729563827712