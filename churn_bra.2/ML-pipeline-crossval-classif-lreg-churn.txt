
spark.conf.set("spark.sql.shuffle.partitions", 10)

val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("staging/dados_clientes.csv").drop("id")

val Array(trainingData, testData) = df.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

// ----- find best logistic regression model

import org.apache.spark.ml.feature.RFormula
val rf = new RFormula().setFormula("Churn ~ .")

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression
lr.setFitIntercept(true).setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(0.1, 0.01, 0.001)).
addGrid(lr.maxIter, Array(10,20,40,100)).build()

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val cv = new CrossValidator().
setEstimator(lr).
setEvaluator(new BinaryClassificationEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(rf,stdScaler,cv))
val pipelinemodel = pipeline.fit(trainingData)

import org.apache.spark.ml.tuning.CrossValidatorModel
val cvmodel = pipelinemodel.stages.last.asInstanceOf[CrossValidatorModel]

// -- CV hyperparameter evaluation

cvmodel.getEstimatorParamMaps.zip(cvmodel.avgMetrics)
res8: Array[(org.apache.spark.ml.param.ParamMap, Double)] =
Array(({
        logreg_ceb9c3b4e55d-maxIter: 10,
        logreg_ceb9c3b4e55d-regParam: 0.1
},0.8623124455391706), ({
        logreg_ceb9c3b4e55d-maxIter: 10,
        logreg_ceb9c3b4e55d-regParam: 0.01
},0.8645341794060438), ({
        logreg_ceb9c3b4e55d-maxIter: 10,
        logreg_ceb9c3b4e55d-regParam: 0.001
},0.8645039233978996), ({
        logreg_ceb9c3b4e55d-maxIter: 20,
        logreg_ceb9c3b4e55d-regParam: 0.1
},0.862319720936492), ({
        logreg_ceb9c3b4e55d-maxIter: 20,
        logreg_ceb9c3b4e55d-regParam: 0.01
},0.8645532906769965), ({
        logreg_ceb9c3b4e55d-maxIter: 20,
        logreg_ceb9c3b4e55d-regParam: 0.001
},0.8645537152560872), ({
        logreg_ceb9c3b4e55d-maxIter: 40,
        logreg_ceb9c3b4e55d-regParam: 0.1
},0.862319720936492), ({
        logreg_ceb9c3b4e55d-maxIter: 40,
        logreg_ceb9c3b4e55d-regP...

-- extract best LR model 

import org.apache.spark.ml.classification.LogisticRegressionModel
val lrmodel = cvmodel.bestModel.asInstanceOf[LogisticRegressionModel]

lrmodel.getRegParam
res1: Double = 0.001

lrmodel.getMaxIter
res2: Int = 40

lrmodel.getThreshold
res3: Double = 0.5

lrmodel.getFitIntercept
res4: Boolean = true

// -----  metrics extracted from model

println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
-0.198251765593552     // Mais65anos
-0.04379731401112643   // Conjuge
0.2718609968058266     // Dependentes
-0.6423069189643607    // MesesDeContrato
-0.12397582996556902   // TelefoneFixo
-0.08031852477442004   // 
0.00871417698805903    // MaisDeUmaLinhaTelefonica
0.12487086096573265    // 
-0.09490754475213016   // Internet
0.1557155292973795     
-0.1379079138706212    // SegurancaOnline
0.05862353106297416    
-0.02318576758886744   // BackupOnline
0.028913211200497523   
0.00868807001403029    // SeguroDispositivo
0.1256984375623669
-0.1020620386501361    // SuporteTecnico
-0.01691281779103503   
0.05471061717770097    // TVaCabo
-0.012518279840483053
0.05031217816674895    // StreamingFilmes
0.3981392163364734     
-0.3088112330272261    // TipoContrato
0.1968897641298962     // ContaCorreio
0.3843389246192365
0.11120878481427551
0.06946403687867442    // MetodoPagamento
0.39527102192954094    // MesesCobrados

import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary

val trainingSummary = lrmodel.summary

trainingSummary.objectiveHistory
res8: Array[Double] = Array(0.6931438275050771, 0.49915312456664246, 0.4776576097005334, 0.46551098017660203, 0.4624573771131976, 0.46022117181681316, 0.45886871847130006, 0.45857437471161006, 0.45841303780593334, 0.45823281541259636, 0.4581654535269004, 0.45814649225466614, 0.4581438872425141, 0.4581417541263477, 0.4581378633458809, 0.4581287680662911, 0.4581205141058715, 0.45810773159334295, 0.45810123056604785, 0.45809909555103334, 0.458096176657147, 0.4580940503734066, 0.4580924777720326, 0.45809217639957384, 0.4580921453498339, 0.45809212695905244, 0.4580921248611346, 0.45809212418461437, 0.4580921241580879, 0.4580921241512925, 0.45809212414241224, 0.45809212413830475, 0.4580921241352634, 0.4580921241344077)

val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]

binarySummary.areaUnderROC
res9: Double = 0.866529170216652

binarySummary.accuracy
res10: Double = 0.7816546272318387

-- collecting metric performance

val pred = pipelinemodel.transform(testData)

val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res11: Double = 0.8662194895960146 

bceval.setMetricName("areaUnderPR").evaluate(pred)
res12: Double = 0.8625581037199371

val predRDD = pred.select("prediction","label").rdd.map( row => (row.getDouble(0),row.getDouble(1))).cache

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

metrics.accuracy
res13: Double = 0.7771504483560279

metrics.confusionMatrix
res14: org.apache.spark.mllib.linalg.Matrix =
1102.0  394.0
277.0   1238.0