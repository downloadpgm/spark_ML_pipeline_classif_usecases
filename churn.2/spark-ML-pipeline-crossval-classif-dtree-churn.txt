---- Exploratory Data Analysis --------------

val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("churn/Churn_Modelling.csv")

df.printSchema
root
 |-- RowNumber: integer (nullable = true)
 |-- CustomerId: integer (nullable = true)
 |-- Surname: string (nullable = true)
 |-- CreditScore: integer (nullable = true)
 |-- Geography: string (nullable = true)
 |-- Gender: string (nullable = true)
 |-- Age: integer (nullable = true)
 |-- Tenure: integer (nullable = true)
 |-- Balance: double (nullable = true)
 |-- NumOfProducts: integer (nullable = true)
 |-- HasCrCard: integer (nullable = true)
 |-- IsActiveMember: integer (nullable = true)
 |-- EstimatedSalary: double (nullable = true)
 |-- Exited: integer (nullable = true)
 
+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+
|RowNumber|CustomerId| Surname|CreditScore|Geography|Gender|Age|Tenure|  Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|
+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+
|        1|  15634602|Hargrave|        619|   France|Female| 42|     2|      0.0|            1|        1|             1|      101348.88|     1|
|        2|  15647311|    Hill|        608|    Spain|Female| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|
|        3|  15619304|    Onio|        502|   France|Female| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|
|        4|  15701354|    Boni|        699|   France|Female| 39|     1|      0.0|            2|        0|             0|       93826.63|     0|
|        5|  15737888|Mitchell|        850|    Spain|Female| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|
|        6|  15574012|     Chu|        645|    Spain|  Male| 44|     8|113755.78|            2|        1|             0|      149756.71|     1|
|        7|  15592531|Bartlett|        822|   France|  Male| 50|     7|      0.0|            2|        1|             1|        10062.8|     0|
|        8|  15656148|  Obinna|        376|  Germany|Female| 29|     4|115046.74|            4|        1|             0|      119346.88|     1|
|        9|  15792365|      He|        501|   France|  Male| 44|     4|142051.07|            2|        0|             1|        74940.5|     0|
|       10|  15592389|      H?|        684|   France|  Male| 27|     2|134603.88|            1|        1|             1|       71725.73|     0|
+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+
only showing top 10 rows

import org.apache.spark.sql.types._
val df1 = df.drop("RowNumber","CustomerId","Surname").withColumn("label", col("Exited").cast(DoubleType))

df1.describe().show
+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+-------------------+
|summary|      CreditScore|Geography|Gender|               Age|            Tenure|          Balance|     NumOfProducts|          HasCrCard|     IsActiveMember|  EstimatedSalary|             Exited|              label|
+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+-------------------+
|  count|            10000|    10000| 10000|             10000|             10000|            10000|             10000|              10000|              10000|            10000|              10000|              10000|
|   mean|         650.5288|     null|  null|           38.9218|            5.0128|76485.88928799961|            1.5302|             0.7055|             0.5151|100090.2398809998|             0.2037|             0.2037|
| stddev|96.65329873613035|     null|  null|10.487806451704587|2.8921743770496837|62397.40520238599|0.5816543579989917|0.45584046447513327|0.49979692845891815|57510.49281769821|0.40276858399486065|0.40276858399486065|
|    min|              350|   France|Female|                18|                 0|              0.0|                 1|                  0|                  0|            11.58|                  0|                  0|
|    max|              850|    Spain|  Male|                92|                10|        250898.09|                 4|                  1|                  1|        199992.48|                  1|                  1|
+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+-------------------+

df1.groupBy('Geography).count.show
+---------+-----+
|Geography|count|
+---------+-----+
|  Germany| 2509|
|   France| 5014|
|    Spain| 2477|
+---------+-----+

df1.groupBy('Gender).count.show
+------+-----+
|Gender|count|
+------+-----+
|Female| 4543|
|  Male| 5457|
+------+-----+

df1.groupBy('Exited).count.show
+------+-----+
|Exited|count|
+------+-----+
|     1| 2037|
|     0| 7963|
+------+-----+

---- Feature extraction & Data Munging --------------

import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler}
val dfrawIndexer1 = new StringIndexer().setInputCol("Geography").setOutputCol("GeographyCat")
val dfrawIndexer2 = new StringIndexer().setInputCol("Gender").setOutputCol("GenderCat")

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("CreditScore","GeographyCat","GenderCat","Age","Tenure","Balance","NumOfProducts","HasCrCard","IsActiveMember","EstimatedSalary"))

// ----- building the decision tree model

import org.apache.spark.ml.classification.DecisionTreeClassifier
val dt = new DecisionTreeClassifier

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfrawIndexer1,dfrawIndexer2,va,dt))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

val model = pipeline.fit(trainingData)

-- collecting feature importance

import org.apache.spark.ml.classification.DecisionTreeClassificationModel

val dtmodel = model.stages.last.asInstanceOf[DecisionTreeClassificationModel]

val featureImp = va.getInputCols.zip(dtmodel.featureImportances.toArray)

val columns = Array("feature", "Importance")
val featureImpDF = spark.createDataFrame(featureImp).toDF(columns: _*)

featureImpDF.orderBy($"Importance".desc).show()
+---------------+--------------------+
|        feature|          Importance|
+---------------+--------------------+
|            Age|  0.4166341099985418|
|  NumOfProducts| 0.35159831564942395|
| IsActiveMember| 0.12393388004518885|
|   GeographyCat|0.054923720639724796|
|        Balance| 0.04097670199268527|
|EstimatedSalary| 0.00639332491591141|
|         Tenure|0.003076521755602206|
|    CreditScore|0.001936781808189...|
|      GenderCat|5.266431947325806E-4|
|      HasCrCard|                 0.0|
+---------------+--------------------+


-- collecting metric performance

val pred = model.transform(testData)

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res2: Double = 0.6369097342014668

bceval.setMetricName("areaUnderPR").evaluate(pred)
res3: Double = 0.34312839045215865


val validPredicts = pred.select("label","prediction").rdd.map( row => (row.getDouble(0),row.getDouble(1)) )

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)

metrics.confusionMatrix
res7: org.apache.spark.mllib.linalg.Matrix =
2182.0  309.0
127.0   281.0


// ----- DT model hyperparameter tunning

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(dt.maxBins, Array(32, 48, 64)).
addGrid(dt.impurity, Array("gini", "entropy")).
addGrid(dt.maxDepth, Array(10,20,30)).build()

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new BinaryClassificationEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val cvmodel = cv.fit(trainingData)

-- CV hyperparameter evaluation

(new BinaryClassificationEvaluator).getMetricName
res10: String = areaUnderROC

cvmodel.getEstimatorParamMaps.zip(cvmodel.avgMetrics)
res10: Array[(org.apache.spark.ml.param.ParamMap, Double)] =
Array(({
        dtc_57b5d709b5bc-impurity: gini,
        dtc_57b5d709b5bc-maxBins: 32,
        dtc_57b5d709b5bc-maxDepth: 10
},0.699701836620244), ({
        dtc_57b5d709b5bc-impurity: gini,
        dtc_57b5d709b5bc-maxBins: 32,
        dtc_57b5d709b5bc-maxDepth: 20
},0.7021679505396922), ({
        dtc_57b5d709b5bc-impurity: gini,
        dtc_57b5d709b5bc-maxBins: 32,
        dtc_57b5d709b5bc-maxDepth: 30
},0.7021679505396922), ({
        dtc_57b5d709b5bc-impurity: entropy,
        dtc_57b5d709b5bc-maxBins: 32,
        dtc_57b5d709b5bc-maxDepth: 10
},0.72907559557034), ({
        dtc_57b5d709b5bc-impurity: entropy,
        dtc_57b5d709b5bc-maxBins: 32,
        dtc_57b5d709b5bc-maxDepth: 20
},0.7077353830428793), ({
        dtc_57b5d709b5bc-impurity: entropy,
        dtc_57b5d709b5bc-maxBins: 32,
        dtc_57b5d709b5bc-maxDepth: 30
},0.70627613974...


-- extract best DT model 

import org.apache.spark.ml.PipelineModel
val bestmodel = cvmodel.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.classification.DecisionTreeClassificationModel
val dtmodel = bestmodel.stages.last.asInstanceOf[DecisionTreeClassificationModel]

dtmodel.getMaxBins
res0: Int = 48

dtmodel.getImpurity
res2: String = entropy

dtmodel.getMaxDepth
res3: Int = 10

-- collecting feature importance

val featureImp = va.getInputCols.zip(dtmodel.featureImportances.toArray)

val columns = Array("feature", "Importance")
val featureImpDF = spark.createDataFrame(featureImp).toDF(columns: _*)

featureImpDF.orderBy($"Importance".desc).show()
+---------------+--------------------+
|        feature|          Importance|
+---------------+--------------------+
|            Age| 0.31604938304058083|
|  NumOfProducts| 0.23189402144774074|
|        Balance| 0.10929864095163884|
|EstimatedSalary| 0.08497677584964165|
|    CreditScore| 0.07988257463387599|
| IsActiveMember| 0.06135428208674516|
|         Tenure|0.048667879773498395|
|   GeographyCat| 0.04094196349522547|
|      GenderCat|0.021431701611069706|
|      HasCrCard| 0.00550277710998303|
+---------------+--------------------+


-- collecting metric performance

val pred = bestmodel.transform(testData)

val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res11: Double = 0.7252358127004868

bceval.setMetricName("areaUnderPR").evaluate(pred)
res12: Double = 0.502910323822986


val validPredicts = pred.select("label","prediction").rdd.map( row => (row.getDouble(0),row.getDouble(1)) )

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)

metrics.confusionMatrix
res16: org.apache.spark.mllib.linalg.Matrix =
2139.0  299.0
170.0   291.0
