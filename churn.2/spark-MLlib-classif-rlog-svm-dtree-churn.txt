---- Feature extraction & Data Munging --------------

val colname = sc.textFile("churn/Churn_Modelling.csv").filter( x => x.contains("RowNumber")).take(1)
colname: Array[String] = Array(RowNumber,CustomerId,Surname,CreditScore,Geography,Gender,Age,Tenure,Balance,NumOfProducts,HasCrCard,IsActiveMember,EstimatedSalary,Exited)

val rdd = sc.textFile("churn/Churn_Modelling.csv").filter( x => ! x.contains("RowNumber"))

val rdd1 = rdd.map( x => x.split(",")).map( x => x.slice(3,x.size))

rdd1.take(5)
res2: Array[Array[String]] = Array(Array(619, France, Female, 42, 2, 0, 1, 1, 1, 101348.88, 1), Array(608, Spain, Female, 41, 1, 83807.86, 1, 0, 1, 112542.58, 0), Array(502, France, Female, 42, 8, 159660.8, 3, 1, 0, 113931.57, 1), Array(699, France, Female, 39, 1, 0, 2, 0, 0, 93826.63, 0), Array(850, Spain, Female, 43, 2, 125510.82, 1, 1, 1, 79084.1, 0))

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  print(f"$idx%.0f -> "); println(categories)
  
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
    val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories - 1) else Array.ofDim[Double](1)
    if (numCategories > 2) { 
      if (categoryIdx > 0) categoryFeatures(categoryIdx - 1) = 1.0
    }
    else categoryFeatures(0) = categoryIdx
    categoryFeatures
    })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd1,1,2)
1 -> Map(Germany -> 2, France -> 0, Spain -> 1)
2 -> Map(Male -> 0, Female -> 1)

concat.take(2)
res43: Array[Array[Double]] = Array(Array(0.0, 0.0, 1.0), Array(1.0, 0.0, 1.0))

val rdd2 = rdd1.map( x => {
  val y = Array(x(0),x(3),x(4),x(5),x(6),x(7),x(8),x(9),x(10))
  y.map( z => z.toDouble)
})

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(2)
res45: Array[Array[Double]] = Array(Array(0.0, 0.0, 1.0, 619.0, 42.0, 2.0, 0.0, 1.0, 1.0, 1.0, 101348.88, 1.0), Array(1.0, 0.0, 1.0, 608.0, 41.0, 1.0, 83807.86, 1.0, 0.0, 1.0, 112542.58, 0.0))

val categ_country = rdd1.map(x => x(1)).distinct.zipWithIndex.collectAsMap
categ_country: scala.collection.immutable.Map[String,Long] = Map(France -> 0, Spain -> 1, Germany -> 2)

val categ_gender = rdd1.map(x => x(2)).distinct.zipWithIndex.collectAsMap
categ_gender: scala.collection.immutable.Map[String,Long] = Map(Male -> 0, Female -> 1)

val rdd2_dt = rdd1.map( x => {
  val y = Array(x(0),categ_country(x(1)),categ_gender(x(2)),x(3),x(4),x(5),x(6),x(7),x(8),x(9),x(10))
  y.map( z => z.toString.toDouble)
})

rdd2_dt.take(2)
res44: Array[Array[Double]] = Array(Array(619.0, 0.0, 1.0, 42.0, 2.0, 0.0, 1.0, 1.0, 1.0, 101348.88, 1.0), Array(608.0, 1.0, 1.0, 41.0, 1.0, 83807.86, 1.0, 0.0, 1.0, 112542.58, 0.0))

val data = vect.zip(rdd2_dt)

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val train_rdd = sets(0)
val test_rdd = sets(1)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint  

val trainSet = train_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache
testSet.cache

---- calculate cosine column similarities to check multicolinearity

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = vect.map( x => Vectors.dense(x.slice(0, x.size - 1)))
val matrix = new RowMatrix(vectors)

val colsims = matrix.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0
var j = 0.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    i = x._1._1
    j = i + 1
    while (j < x._1._2) { 
      print("0.0000 ")
      j = j + 1
    }
    print(f"$sim%.4f ")
  } else {
      j = j + 1
      while (j < x._1._2) { 
        print("0.0000 ")
        j = j + 1
      }
      print(f"$sim%.4f ")
    }
})
// Geography,Gender,CreditScore,Age,Tenure,Balance,NumOfProducts,HasCrCard,IsActiveMember,EstimatedSalary
0.0000 0.3246 0.4929 0.4802 0.4328 0.3117 0.4680 0.4117 0.3673 0.4287
       0.3534 0.4962 0.4942 0.4336 0.6076 0.4650 0.4257 0.3472 0.4388
              0.6670 0.6561 0.5784 0.5166 0.6358 0.5638 0.4721 0.5874
                     0.9549 0.8568 0.7670 0.9252 0.8304 0.7125 0.8576
                            0.8351 0.7528 0.8997 0.8094 0.7085 0.8363
                                   0.6673 0.8121 0.7337 0.6118 0.7530
                                          0.6560 0.6458 0.5517 0.6759
                                                 0.7858 0.6733 0.8130
                                                        0.5983 0.7256
                                                               0.6183

// CreditScore x Age = 0.9549 can be considered multicolinear
// CreditScore x Tenure = 0.8568 can be considered multicolinear
// CreditScore x NumOfProducts = 0.9252 can be considered multicolinear
// CreditScore x EstimatedSalary= 0.8576 can be considered multicolinear
// Age x NumOfProducts = 0.8997 can be considered multicolinear

---- MLlib logistic regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AreaPR, AreaROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
    model.optimizer.setNumIterations(numIter).setStepSize(step)
    val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainSet, testSet)
iter, step -> pred / total, AreaPR, AreaROC
100, 0.100 -> 2310 / 2899, 0.2032, 0.5000
100, 0.010 -> 589 / 2899, 0.2032, 0.5000
100, 0.001 -> 2310 / 2899, 0.2032, 0.5000
300, 0.100 -> 763 / 2899, 0.1934, 0.4713
300, 0.010 -> 2310 / 2899, 0.2032, 0.5000
300, 0.001 -> 1388 / 2899, 0.2435, 0.5888
500, 0.100 -> 2310 / 2899, 0.2032, 0.5000
500, 0.010 -> 2182 / 2899, 0.2501, 0.5261
500, 0.001 -> 2310 / 2899, 0.2032, 0.5000
 
---- MLlib SVM regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AreaPR, AreaROC") 
  for(numIter <- iterNums) {
    val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(100,300,500), trainSet, testSet)
iter -> pred / total, AreaPR, AreaROC
100 -> 1836 / 2899, 0.2460, 0.5631
300 -> 591 / 2899, 0.2033, 0.5004
500 -> 2310 / 2899, 0.2032, 0.5000


---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res53: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,850.0,92.0,10.0,222267.63,4.0,1.0,1.0,199992.48]

matrixSummary.min
res54: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,350.0,18.0,0.0,0.0,1.0,0.0,0.0,90.07]

matrixSummary.mean
res55: org.apache.spark.mllib.linalg.Vector = [0.2505280946345585,0.24996479369102942,0.45331643430502744,650.2272919307158,38.907477820025264,5.0229545134488065,76816.17311927915,1.5300661878608626,0.7048303055907619,0.5124630333755809,99857.7737051122]

matrixSummary.variance
res56: org.apache.spark.mllib.linalg.Vector = [0.187790214034524,0.18750880157724265,0.24785554901015724,9389.265514200513,109.60143844052907,8.3976420301842,3.883662355131386E9,0.34096209421009877,0.20807384796031506,0.24987986218961425,3.3323936250122147E9]

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache
testScaled.cache

----- with MLlib logistic regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AreaPR, AreaROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
    model.optimizer.setNumIterations(numIter).setStepSize(step)
    val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainScaled, testScaled)
iter, step -> pred / total, AreaPR, AreaROC
100, 0.100 -> 1888 / 2899, 0.3294, 0.7034
100, 0.010 -> 1878 / 2899, 0.3269, 0.7006
100, 0.001 -> 1878 / 2899, 0.3269, 0.7006
300, 0.100 -> 1888 / 2899, 0.3294, 0.7034
300, 0.010 -> 1878 / 2899, 0.3269, 0.7006
300, 0.001 -> 1878 / 2899, 0.3269, 0.7006
500, 0.100 -> 1888 / 2899, 0.3294, 0.7034
500, 0.010 -> 1878 / 2899, 0.3269, 0.7006
500, 0.001 -> 1878 / 2899, 0.3269, 0.7006


----- with MLlib SVM regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AreaPR, AreaROC") 
  for(numIter <- iterNums) {
    val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(100,300,500), trainScaled, testScaled)
iter -> pred / total, AreaPR, AreaROC
100 -> 1918 / 2899, 0.3377, 0.7124
300 -> 1918 / 2899, 0.3377, 0.7124
500 -> 1918 / 2899, 0.3377, 0.7124

----- with MLlib Decision tree regression ----------------------

val trainSet = train_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 1->3 , 2->2 )

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, AreaPR, AreaROC") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AreaPR, AreaROC
gini, 10, 32 -> 2416 / 2922, 0.4989, 0.6938
gini, 10, 48 -> 2406 / 2922, 0.4898, 0.6940
gini, 10, 64 -> 2424 / 2922, 0.5065, 0.7028 *
gini, 20, 32 -> 2268 / 2922, 0.4019, 0.6820
gini, 20, 48 -> 2277 / 2922, 0.4049, 0.6803
gini, 20, 64 -> 2298 / 2922, 0.4168, 0.6854
gini, 30, 32 -> 2268 / 2922, 0.4019, 0.6820
gini, 30, 48 -> 2277 / 2922, 0.4049, 0.6803
gini, 30, 64 -> 2297 / 2922, 0.4162, 0.6852
entropy, 10, 32 -> 2443 / 2922, 0.5280, 0.6916
entropy, 10, 48 -> 2432 / 2922, 0.5154, 0.6917
entropy, 10, 64 -> 2444 / 2922, 0.5271, 0.7010
entropy, 20, 32 -> 2332 / 2922, 0.4396, 0.6989
entropy, 20, 48 -> 2327 / 2922, 0.4323, 0.6868
entropy, 20, 64 -> 2337 / 2922, 0.4432, 0.7012
entropy, 30, 32 -> 2327 / 2922, 0.4358, 0.6960
entropy, 30, 48 -> 2326 / 2922, 0.4319, 0.6872
entropy, 30, 64 -> 2334 / 2922, 0.4413, 0.7006


val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "gini", 10, 64)

model.toDebugString
res9: String =
"DecisionTreeModel classifier of depth 10 with 581 nodes
  If (feature 3 <= 42.5)
   If (feature 6 <= 2.5)
    If (feature 6 <= 1.5)
     If (feature 8 <= 0.5)
      If (feature 3 <= 38.5)
       If (feature 1 in {0.0,1.0})
        If (feature 5 <= 57348.325)
         If (feature 0 <= 686.5)
          If (feature 5 <= 6229.595)
           Predict: 0.0
          Else (feature 5 > 6229.595)
           If (feature 0 <= 629.5)
            Predict: 0.0
           Else (feature 0 > 629.5)
            Predict: 1.0
         Else (feature 0 > 686.5)
          If (feature 9 <= 121340.76000000001)
           If (feature 9 <= 38261.36)
            Predict: 1.0
           Else (feature 9 > 38261.36)
            Predict: 0.0
          Else (feature 9 > 121340.76000000001)
           If...

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res8: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 2424
validPredicts.count                            // 2922
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.5064752783626146
metrics.areaUnderROC  // 0.702825283271146