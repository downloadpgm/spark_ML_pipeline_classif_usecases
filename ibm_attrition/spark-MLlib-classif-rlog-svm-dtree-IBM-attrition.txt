---- Feature extraction & Data Munging --------------

val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("attrition/HR-Employee-Attrition.csv")

scala> df.printSchema
root
 |-- Age: integer (nullable = true)
 |-- Attrition: string (nullable = true)
 |-- BusinessTravel: string (nullable = true)
 |-- DailyRate: integer (nullable = true)
 |-- Department: string (nullable = true)
 |-- DistanceFromHome: integer (nullable = true)
 |-- Education: integer (nullable = true)
 |-- EducationField: string (nullable = true)
 |-- EmployeeCount: integer (nullable = true)
 |-- EmployeeNumber: integer (nullable = true)
 |-- EnvironmentSatisfaction: integer (nullable = true)
 |-- Gender: string (nullable = true)
 |-- HourlyRate: integer (nullable = true)
 |-- JobInvolvement: integer (nullable = true)
 |-- JobLevel: integer (nullable = true)
 |-- JobRole: string (nullable = true)
 |-- JobSatisfaction: integer (nullable = true)
 |-- MaritalStatus: string (nullable = true)
 |-- MonthlyIncome: integer (nullable = true)
 |-- MonthlyRate: integer (nullable = true)
 |-- NumCompaniesWorked: integer (nullable = true)
 |-- Over18: string (nullable = true)
 |-- OverTime: string (nullable = true)
 |-- PercentSalaryHike: integer (nullable = true)
 |-- PerformanceRating: integer (nullable = true)
 |-- RelationshipSatisfaction: integer (nullable = true)
 |-- StandardHours: integer (nullable = true)
 |-- StockOptionLevel: integer (nullable = true)
 |-- TotalWorkingYears: integer (nullable = true)
 |-- TrainingTimesLastYear: integer (nullable = true)
 |-- WorkLifeBalance: integer (nullable = true)
 |-- YearsAtCompany: integer (nullable = true)
 |-- YearsInCurrentRole: integer (nullable = true)
 |-- YearsSinceLastPromotion: integer (nullable = true)
 |-- YearsWithCurrManager: integer (nullable = true)


val df1 = df.select("BusinessTravel","Department","EducationField","Gender","JobRole","MaritalStatus","OverTime","Education","Age","DistanceFromHome","EnvironmentSatisfaction","JobInvolvement","JobSatisfaction","MonthlyIncome","NumCompaniesWorked","PerformanceRating","RelationshipSatisfaction","StockOptionLevel","TotalWorkingYears","TrainingTimesLastYear","WorkLifeBalance","YearsAtCompany","YearsInCurrentRole","YearsSinceLastPromotion","YearsWithCurrManager","Attrition")

scala> df1.printSchema
root
 |-- BusinessTravel: string (nullable = true)
 |-- Department: string (nullable = true)
 |-- EducationField: string (nullable = true)
 |-- Gender: string (nullable = true)
 |-- JobRole: string (nullable = true)
 |-- MaritalStatus: string (nullable = true)
 |-- OverTime: string (nullable = true)
 |-- Education: integer (nullable = true)
 |-- Age: integer (nullable = true)
 |-- DistanceFromHome: integer (nullable = true)
 |-- EnvironmentSatisfaction: integer (nullable = true)
 |-- JobInvolvement: integer (nullable = true)
 |-- JobSatisfaction: integer (nullable = true)
 |-- MonthlyIncome: integer (nullable = true)
 |-- NumCompaniesWorked: integer (nullable = true)
 |-- PerformanceRating: integer (nullable = true)
 |-- RelationshipSatisfaction: integer (nullable = true)
 |-- StockOptionLevel: integer (nullable = true)
 |-- TotalWorkingYears: integer (nullable = true)
 |-- TrainingTimesLastYear: integer (nullable = true)
 |-- WorkLifeBalance: integer (nullable = true)
 |-- YearsAtCompany: integer (nullable = true)
 |-- YearsInCurrentRole: integer (nullable = true)
 |-- YearsSinceLastPromotion: integer (nullable = true)
 |-- YearsWithCurrManager: integer (nullable = true)
 |-- Attrition: string (nullable = true)

df1.first
res0: org.apache.spark.sql.Row = [Travel_Rarely,Sales,Life Sciences,Female,Sales Executive,Single,Yes,2,41,1,2,3,4,5993,8,3,1,0,8,0,1,6,4,0,5,Yes]

val rdd = df1.rdd.map( x => x.toSeq ).map( x => x.map( y => { y.toString } ))

rdd.take(2)
res4: Array[Seq[String]] = Array(ArrayBuffer(Travel_Rarely, Sales, Life Sciences, Female, Sales Executive, Single, Yes, 2, 41, 1, 2, 3, 4, 5993, 8, 3, 1, 0, 8, 0, 1, 6, 4, 0, 5, Yes), ArrayBuffer(Travel_Frequently, Research & Development, Life Sciences, Male, Research Scientist, Married, No, 1, 49, 8, 3, 2, 2, 5130, 1, 4, 4, 1, 10, 3, 3, 10, 7, 1, 7, No))

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
	val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
	if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
    categoryFeatures
	})
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val rdd1 = rdd.map( x => x.toArray )
val concat = mergeArray(rdd1,0,1,2,3,4,5,6)

concat.take(2)
res5: Array[Array[Double]] = Array(Array(0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0), Array(0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0))

val categories = rdd1.map( x => x(x.size-1)).distinct.zipWithIndex.collectAsMap
categories: scala.collection.Map[String,Long] = Map(No -> 0, Yes -> 1)

val rdd2 = rdd1.map( x => x.slice(7,x.size)).map( y => y.map( x => {
                     try { x.toDouble } catch { case _ : Throwable => categories(x).toString.toDouble } }))

rdd2.take(2)
res6: Array[Array[Double]] = Array(Array(2.0, 41.0, 1.0, 2.0, 3.0, 4.0, 5993.0, 8.0, 3.0, 1.0, 0.0, 8.0, 0.0, 1.0, 6.0, 4.0, 0.0, 5.0, 1.0), Array(1.0, 49.0, 8.0, 3.0, 2.0, 2.0, 5130.0, 1.0, 4.0, 4.0, 1.0, 10.0, 3.0, 3.0, 10.0, 7.0, 1.0, 7.0, 0.0))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(2)
res7: Array[Array[Double]] = Array(Array(0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 2.0, 41.0, 1.0, 2.0, 3.0, 4.0, 5993.0, 8.0, 3.0, 1.0, 0.0, 8.0, 0.0, 1.0, 6.0, 4.0, 0.0, 5.0, 1.0), Array(0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 49.0, 8.0, 3.0, 2.0, 2.0, 5130.0, 1.0, 4.0, 4.0, 1.0, 10.0, 3.0, 3.0, 10.0, 7.0, 1.0, 7.0, 0.0))

val categ_travel = rdd.map( x => x(0)).distinct.zipWithIndex.collectAsMap
categ_travel: scala.collection.Map[String,Long] = Map(Travel_Rarely -> 2, Non-Travel -> 0, Travel_Frequently -> 1)

val categ_dept = rdd.map( x => x(1)).distinct.zipWithIndex.collectAsMap
categ_dept: scala.collection.Map[String,Long] = Map(Research & Development -> 2, Sales -> 1, Human Resources -> 0)

val categ_educ_field = rdd.map( x => x(2)).distinct.zipWithIndex.collectAsMap
categ_educ_field: scala.collection.Map[String,Long] = Map(Marketing -> 4, Medical -> 3, Other -> 2, Life Sciences -> 5, Technical Degree -> 1, Human Resources -> 0)

val categ_gender = rdd.map( x => x(3)).distinct.zipWithIndex.collectAsMap
categ_gender: scala.collection.Map[String,Long] = Map(Male -> 0, Female -> 1)

val categ_jobrole = rdd.map( x => x(4)).distinct.zipWithIndex.collectAsMap
categ_jobrole: scala.collection.Map[String,Long] = Map(Sales Representative -> 7, Healthcare Representative -> 3, Laboratory Technician -> 5, Sales Executive -> 1, Research Director -> 8, Manager -> 4, Manufacturing Director -> 6, Research Scientist -> 2, Human Resources -> 0)

val categ_marital = rdd.map( x => x(5)).distinct.zipWithIndex.collectAsMap
categ_marital: scala.collection.Map[String,Long] = Map(Divorced -> 2, Single -> 0, Married -> 1)

val categ_overtime = rdd.map( x => x(6)).distinct.zipWithIndex.collectAsMap
categ_overtime: scala.collection.Map[String,Long] = Map(No -> 0, Yes -> 1)

val rdd2_dt = rdd.map( x => Array(categ_travel(x(0)),categ_dept(x(1)),categ_educ_field(x(2)),categ_gender(x(3)),categ_jobrole(x(4)),categ_marital(x(5)),categ_overtime(x(6)))).
                  map( x => x.map( y => y.toDouble )).
                  zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

rdd2_dt.take(2)
res9: Array[Array[Double]] = Array(Array(2.0, 1.0, 5.0, 1.0, 1.0, 0.0, 1.0, 2.0, 41.0, 1.0, 2.0, 3.0, 4.0, 5993.0, 8.0, 3.0, 1.0, 0.0, 8.0, 0.0, 1.0, 6.0, 4.0, 0.0, 5.0, 1.0), Array(1.0, 2.0, 5.0, 0.0, 2.0, 1.0, 0.0, 1.0, 49.0, 8.0, 3.0, 2.0, 2.0, 5130.0, 1.0, 4.0, 4.0, 1.0, 10.0, 3.0, 3.0, 10.0, 7.0, 1.0, 7.0, 0.0))

val data = vect.zip(rdd2_dt)

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val train_rdd = sets(0)
val test_rdd = sets(1)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint  

val trainSet = train_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

---- MLlib logistic regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainSet, testSet)
iter, step -> pred / total, AuPR, AuROC
100, 0.100 -> 67 / 425, 0.1576, 0.5000
100, 0.010 -> 67 / 425, 0.1576, 0.5000
100, 0.001 -> 358 / 425, 0.1576, 0.5000
300, 0.100 -> 358 / 425, 0.1576, 0.5000
300, 0.010 -> 358 / 425, 0.1576, 0.5000
300, 0.001 -> 358 / 425, 0.1576, 0.5000
500, 0.100 -> 358 / 425, 0.1576, 0.5000
500, 0.010 -> 358 / 425, 0.1576, 0.5000
500, 0.001 -> 358 / 425, 0.1576, 0.5000

---- MLlib SVM regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(100,300,500), trainSet, testSet)
iter -> pred / total, AuPR, AuROC
100 -> 67 / 425, 0.1576, 0.5000
300 -> 358 / 425, 0.1576, 0.5000
500 -> 358 / 425, 0.1576, 0.5000

----- Estimation is not so good. But analyze the individual statistics and standardize 

import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics

matrixSummary.max
res29: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,60.0,29.0,4.0,4.0,4.0,19973.0,9.0,4.0,4.0,3.0,40.0,6.0,4.0,40.0,17.0,15.0,17.0]

matrixSummary.min
res30: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,18.0,1.0,1.0,1.0,1.0,1009.0,0.0,3.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0]

matrixSummary.mean
res31: org.apache.spark.mllib.linalg.Vector = [0.10204081632653061,0.19047619047619047,0.7074829931972789,0.04081632653061224,0.30017006802721086,0.6590136054421769,0.017006802721088437,0.09268707482993198,0.05612244897959184,0.3120748299319728,0.1096938775510204,0.41241496598639454,0.592687074829932,0.407312925170068,0.03231292517006803,0.2193877551020408,0.20153061224489796,0.08928571428571429,0.07397959183673469,0.18197278911564627,0.09268707482993198,0.05357142857142857,0.055272108843537414,0.32142857142857145,0.4557823129251701,0.2227891156462585,0.7193877551020408,0.28061224489795916,36.983843537414984,9.070578231292508,2.721938775510202,2.7142857142857175,2.7142857142857095,6542.769557823131,2.676870748299323,3.1573129251700633,2.7244897959183665,0.7908163265306128,11.36309523809...

matrixSummary.variance
res32: org.apache.spark.mllib.linalg.Vector = [0.09170646982197134,0.15432624113475177,0.20712693588073527,0.03918367346938775,0.21024677956288898,0.22490591981473443,0.01673179910261977,0.08416775220726588,0.05301780286582718,0.21486684035316256,0.09774424663482414,0.24253509914604138,0.2416145607179042,0.2416145607179042,0.03129541178173397,0.1714025184541902,0.16105297438124186,0.08138297872340426,0.06856491532783326,0.14898538138659717,0.08416775220726588,0.05074468085106383,0.05226154291503836,0.21829787234042553,0.24825589810392243,0.1733014908090896,0.20204081632653062,0.20204081632653062,82.78527066145611,64.84692936749151,1.186445940078161,0.5055319148936174,1.2204255319148947,2.28501059868512E7,5.99251990157765,0.13267839050513822,1.1870082501085546,0.7221623968736437,60.66549...

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache

----- with MLlib logistic regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainScaled, testScaled)
iter, step -> pred / total, AuPR, AuROC
100, 0.100 -> 269 / 425, 0.2599, 0.6972
100, 0.010 -> 254 / 425, 0.2453, 0.6823
100, 0.001 -> 254 / 425, 0.2453, 0.6823
300, 0.100 -> 267 / 425, 0.2574, 0.6944
300, 0.010 -> 254 / 425, 0.2453, 0.6823
300, 0.001 -> 254 / 425, 0.2453, 0.6823
500, 0.100 -> 267 / 425, 0.2574, 0.6944
500, 0.010 -> 254 / 425, 0.2453, 0.6823
500, 0.001 -> 254 / 425, 0.2453, 0.6823

----- with MLlib SVM regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

scala> iterateSVM(Array(100,300,500), trainScaled, testScaled)
100 -> 285 / 425, 0.2908, 0.7377
300 -> 290 / 425, 0.2985, 0.7447
500 -> 290 / 425, 0.2985, 0.7447

----- MLlib DecisionTree regression --------------

val trainSet = train_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int](0->3, 1->3, 2->6, 3->2, 4->9, 5->3, 6->2)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, accuracy") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AuPR, AuROC
gini, 10, 32 -> 349 / 425, 0.3650, 0.6815
gini, 10, 48 -> 346 / 425, 0.3378, 0.6531
gini, 10, 64 -> 352 / 425, 0.3758, 0.6797
gini, 20, 32 -> 346 / 425, 0.3523, 0.6774
gini, 20, 48 -> 342 / 425, 0.3255, 0.6536
gini, 20, 64 -> 348 / 425, 0.3572, 0.6741
gini, 30, 32 -> 346 / 425, 0.3523, 0.6774
gini, 30, 48 -> 342 / 425, 0.3255, 0.6536
gini, 30, 64 -> 348 / 425, 0.3572, 0.6741
entropy, 10, 32 -> 343 / 425, 0.3176, 0.6368
entropy, 10, 48 -> 352 / 425, 0.3789, 0.6857
entropy, 10, 64 -> 349 / 425, 0.3617, 0.6755
entropy, 20, 32 -> 333 / 425, 0.2868, 0.6289
entropy, 20, 48 -> 344 / 425, 0.3478, 0.6806 *
entropy, 20, 64 -> 342 / 425, 0.3367, 0.6718
entropy, 30, 32 -> 333 / 425, 0.2868, 0.6289
entropy, 30, 48 -> 344 / 425, 0.3478, 0.6806
entropy, 30, 64 -> 342 / 425, 0.3367, 0.6718


val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "entropy", 20, 48)

model.toDebugString
res18: String =
"DecisionTreeModel classifier of depth 15 with 245 nodes
  If (feature 6 in {0.0})
   If (feature 18 <= 2.5)
    If (feature 5 in {2.0})
     Predict: 0.0
    Else (feature 5 not in {2.0})
     If (feature 2 in {0.0,1.0,2.0,4.0})
      If (feature 8 <= 31.5)
       Predict: 1.0
      Else (feature 8 > 31.5)
       If (feature 1 in {1.0,2.0})
        Predict: 0.0
       Else (feature 1 not in {1.0,2.0})
        Predict: 1.0
     Else (feature 2 not in {0.0,1.0,2.0,4.0})
      If (feature 4 in {0.0,2.0})
       If (feature 20 <= 3.5)
        Predict: 0.0
       Else (feature 20 > 3.5)
        Predict: 1.0
      Else (feature 4 not in {0.0,2.0})
       If (feature 13 <= 2719.0)
        If (feature 8 <= 32.5)
         If (feature 20 <= 2.5)
          Predict: 1.0
         El...

