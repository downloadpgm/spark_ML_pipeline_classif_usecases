---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("spark/data/diabetes_pred/diabetes_data_upload.csv").map( x => x.split(","))

val hdr = rdd.take(1)(0)
hdr: Array[String] = Array(Age, Gender, Polyuria, Polydipsia, sudden weight loss, weakness, Polyphagia, Genital thrush, visual blurring, Itching, Irritability, delayed healing, partial paresis, muscle stiffness, Alopecia, Obesity, class)

val rdd1 = rdd.filter( x => ! x.contains("Polydipsia"))

rdd1.take(10).map( x => x.mkString(", ")).foreach(println)
40, Male, No, Yes, No, Yes, No, No, No, Yes, No, Yes, No, Yes, Yes, Yes, Positive
58, Male, No, No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, No, Positive
41, Male, Yes, No, No, Yes, Yes, No, No, Yes, No, Yes, No, Yes, Yes, No, Positive
45, Male, No, No, Yes, Yes, Yes, Yes, No, Yes, No, Yes, No, No, No, No, Positive
60, Male, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Positive
55, Male, Yes, Yes, No, Yes, Yes, No, Yes, Yes, No, Yes, No, Yes, Yes, Yes, Positive
57, Male, Yes, Yes, No, Yes, Yes, Yes, No, No, No, Yes, Yes, No, No, No, Positive
66, Male, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, No, Yes, Yes, No, No, Positive
67, Male, Yes, Yes, No, Yes, Yes, Yes, No, Yes, Yes, No, Yes, Yes, No, Yes, Positive
70, Male, No, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, No, No, No, Yes, No, Positive


val categ_zerone = Map( "No" -> 0, "Yes" -> 1, "Female" -> 0, "Male" -> 1, "Negative" -> 0, "Positive" -> 1)
categ_zerone: scala.collection.immutable.Map[String,Int] = Map(Negative -> 0, No -> 0, Yes -> 1, Positive -> 1, Female -> 0, Male -> 1)

val rdd2 = rdd1.map( y => y.map( x => {
                    try { x.toDouble } 
					catch { case _ : Throwable => categ_zerone(x).toString.toDouble } }))

rdd2.take(10).map( x => x.mkString(", ")).foreach(println)
40.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0
58.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0
41.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0
45.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0
60.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0
55.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0
57.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0
66.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0
67.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0
70.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0


---- Splitting dataset as train/test sets  --------------

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = rdd2.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)
   val f = x.slice(0, arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })
 
val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache
testSet.cache


----- Analyze the individual statistics and possibly standardize 

import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics

matrixSummary.max.toArray.foreach( x => print(f"$x%.2f\t"))
90.00   1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00   1.00     1.00    1.00    1.00    1.00    1.00

matrixSummary.min.toArray.foreach( x => print(f"$x%.2f\t"))
16.00   0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   0.00     0.00    0.00    0.00    0.00    0.00

matrixSummary.mean.toArray.foreach( x => print(f"$x%.2f\t"))
48.19   0.62    0.50    0.47    0.42    0.59    0.47    0.21    0.47    0.47   0.24     0.43    0.45    0.39    0.33    0.18

matrixSummary.variance.toArray.foreach( x => print(f"$x%.2f\t"))
150.76  0.24    0.25    0.25    0.24    0.24    0.25    0.17    0.25    0.25   0.18     0.25    0.25    0.24    0.22    0.15


----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache
testScaled.cache


---- MLlib logistic regression STD --------------

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(100).setRegParam(0.01)
val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res18: Double = 0.9515151515151515

metrics.areaUnderROC
res19: Double = 0.8787878787878788

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res20: Double = 0.8545454545454545

metrics1.confusionMatrix
res21: org.apache.spark.mllib.linalg.Matrix =
66.0  0.0
24.0  75.0


---- Hyperparameter tunning with logistic regression STD --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], regSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step, reg  -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes; reg <- regSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step).setRegParam(reg)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, reg, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(10, 20, 40, 100),Array(1, 0.1, 0.01, 0.001),Array(0.1, 0.01), trainScaled, testScaled)
iter, step, reg  -> pred / total, AuPR, AuROC
10, 1.000, 0.100 -> 142 / 165, 0.9535, 0.8838 *
10, 1.000, 0.010 -> 142 / 165, 0.9535, 0.8838
10, 0.100, 0.100 -> 137 / 165, 0.9336, 0.8561
10, 0.100, 0.010 -> 137 / 165, 0.9336, 0.8561
10, 0.010, 0.100 -> 137 / 165, 0.9336, 0.8561
10, 0.010, 0.010 -> 137 / 165, 0.9336, 0.8561
10, 0.001, 0.100 -> 137 / 165, 0.9336, 0.8561
10, 0.001, 0.010 -> 137 / 165, 0.9336, 0.8561
20, 1.000, 0.100 -> 141 / 165, 0.9515, 0.8788
20, 1.000, 0.010 -> 141 / 165, 0.9515, 0.8788
20, 0.100, 0.100 -> 138 / 165, 0.9455, 0.8636
20, 0.100, 0.010 -> 138 / 165, 0.9455, 0.8636
20, 0.010, 0.100 -> 137 / 165, 0.9336, 0.8561
20, 0.010, 0.010 -> 137 / 165, 0.9336, 0.8561
20, 0.001, 0.100 -> 137 / 165, 0.9336, 0.8561
20, 0.001, 0.010 -> 137 / 165, 0.9336, 0.8561
40, 1.000, 0.100 -> 141 / 165, 0.9515, 0.8788
40, 1.000, 0.010 -> 141 / 165, 0.9515, 0.8788
40, 0.100, 0.100 -> 138 / 165, 0.9455, 0.8636
40, 0.100, 0.010 -> 138 / 165, 0.9455, 0.8636
40, 0.010, 0.100 -> 137 / 165, 0.9336, 0.8561
40, 0.010, 0.010 -> 137 / 165, 0.9336, 0.8561
40, 0.001, 0.100 -> 137 / 165, 0.9336, 0.8561
40, 0.001, 0.010 -> 137 / 165, 0.9336, 0.8561
100, 1.000, 0.100 -> 141 / 165, 0.9515, 0.8788
100, 1.000, 0.010 -> 141 / 165, 0.9515, 0.8788
100, 0.100, 0.100 -> 139 / 165, 0.9475, 0.8687
100, 0.100, 0.010 -> 139 / 165, 0.9475, 0.8687
100, 0.010, 0.100 -> 137 / 165, 0.9336, 0.8561
100, 0.010, 0.010 -> 137 / 165, 0.9336, 0.8561
100, 0.001, 0.100 -> 137 / 165, 0.9336, 0.8561
100, 0.001, 0.010 -> 137 / 165, 0.9336, 0.8561


val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(10).setStepSize(1.0).setRegParam(0.1)

val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))

validPredicts.take(20)
res24: Array[(Double, Double)] = Array((0.0,1.0), (0.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (0.0,1.0), (1.0,1.0), (0.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (0.0,1.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res25: Double = 0.9535353535353536

metrics.areaUnderROC
res26: Double = 0.8838383838383839

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res27: Double = 0.8606060606060606

metrics1.confusionMatrix
res28: org.apache.spark.mllib.linalg.Matrix =
66.0  0.0
23.0  76.0


---- MLlib SVM regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(10, 20, 40, 100), trainScaled, testScaled)
iter -> pred / total, AuPR, AuROC
10 -> 141 / 165, 0.9515, 0.8788
20 -> 142 / 165, 0.9535, 0.8838 *
40 -> 143 / 165, 0.9556, 0.8889
100 -> 141 / 165, 0.9515, 0.8788


val model = SVMWithSGD.train(trainScaled, 20)

val validPredicts = testScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res35: Array[(Double, Double)] = Array((0.0,1.0), (0.0,1.0), (1.0,1.0), (1.0,1.0), (0.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (0.0,1.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res36: Double = 0.9535353535353536

metrics.areaUnderROC
res37: Double = 0.8838383838383839

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res38: Double = 0.8606060606060606

metrics1.confusionMatrix
res39: org.apache.spark.mllib.linalg.Matrix =
66.0  0.0
23.0  76.0


---- MLlib Decision Tree regression --------------

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]()

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, AreaPR, AreaROC") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AreaPR, AreaROC
gini, 10, 32 -> 159 / 165, 0.9641, 0.9621
gini, 10, 48 -> 159 / 165, 0.9641, 0.9621
gini, 10, 64 -> 159 / 165, 0.9641, 0.9621
gini, 20, 32 -> 159 / 165, 0.9641, 0.9621
gini, 20, 48 -> 159 / 165, 0.9641, 0.9621
gini, 20, 64 -> 159 / 165, 0.9641, 0.9621
gini, 30, 32 -> 159 / 165, 0.9641, 0.9621
gini, 30, 48 -> 159 / 165, 0.9641, 0.9621
gini, 30, 64 -> 159 / 165, 0.9641, 0.9621
entropy, 10, 32 -> 159 / 165, 0.9717, 0.9646
entropy, 10, 48 -> 159 / 165, 0.9717, 0.9646
entropy, 10, 64 -> 159 / 165, 0.9717, 0.9646
entropy, 20, 32 -> 159 / 165, 0.9717, 0.9646
entropy, 20, 48 -> 159 / 165, 0.9717, 0.9646
entropy, 20, 64 -> 159 / 165, 0.9717, 0.9646
entropy, 30, 32 -> 159 / 165, 0.9717, 0.9646
entropy, 30, 48 -> 159 / 165, 0.9717, 0.9646
entropy, 30, 64 -> 159 / 165, 0.9717, 0.9646

val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "entropy", 10, 32)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res41: Array[(Double, Double)] = Array((0.0,1.0), (0.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (0.0,1.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res42: Double = 0.9717171717171718

metrics.areaUnderROC
res43: Double = 0.9646464646464646

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)
metrics1: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@5f986fcb

metrics1.accuracy
res44: Double = 0.9636363636363636

metrics1.confusionMatrix
res45: org.apache.spark.mllib.linalg.Matrix =
64.0  2.0
4.0   95.0