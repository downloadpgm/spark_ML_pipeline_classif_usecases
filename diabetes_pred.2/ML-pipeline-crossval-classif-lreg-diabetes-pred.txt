---- Exploratory Data Analysis --------------

val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("staging/diabetes_data_upload.csv")

val df1 = df.na.replace(Array("Polyuria","Polydipsia","sudden weight loss","weakness","Polyphagia","Genital thrush","visual blurring","Itching","Irritability","delayed healing","partial paresis","muscle stiffness","Alopecia","Obesity"),Map("No" -> "0.0", "Yes" -> "1.0")).
      na.replace(Array("class"),Map("Negative" -> "0.0", "Positive" -> "1.0")).
      na.replace(Array("Gender"),Map("Female" -> "0.0", "Male" -> "1.0")).
      withColumn("label", 'class)
  
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val types = df1.dtypes
types: Array[(String, String)] = Array((Age,IntegerType), (Gender,StringType), (Polyuria,StringType), (Polydipsia,StringType), (sudden weight loss,StringType), (weakness,StringType), (Polyphagia,StringType), (Genital thrush,StringType), (visual blurring,StringType), (Itching,StringType), (Irritability,StringType), (delayed healing,StringType), (partial paresis,StringType), (muscle stiffness,StringType), (Alopecia,StringType), (Obesity,StringType), (class,StringType), (label,StringType))

val df2 = df1.select(types.map{ case(c,t) => col(c).cast(DoubleType)}: _*)
df2: org.apache.spark.sql.DataFrame = [Age: double, Gender: double ... 16 more fields]

df2.printSchema
root
 |-- Age: double (nullable = true)
 |-- Gender: double (nullable = true)
 |-- Polyuria: double (nullable = true)
 |-- Polydipsia: double (nullable = true)
 |-- sudden weight loss: double (nullable = true)
 |-- weakness: double (nullable = true)
 |-- Polyphagia: double (nullable = true)
 |-- Genital thrush: double (nullable = true)
 |-- visual blurring: double (nullable = true)
 |-- Itching: double (nullable = true)
 |-- Irritability: double (nullable = true)
 |-- delayed healing: double (nullable = true)
 |-- partial paresis: double (nullable = true)
 |-- muscle stiffness: double (nullable = true)
 |-- Alopecia: double (nullable = true)
 |-- Obesity: double (nullable = true)
 |-- class: double (nullable = true)
 |-- label: double (nullable = true)


// ----- logistic regression model hyperparameter tunning

val Array(trainingData, testData) = df2.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

import org.apache.spark.ml.feature.VectorAssembler
val va = new VectorAssembler().setOutputCol("features").setInputCols(df2.columns.diff(Array("class","label")))

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(100).setFitIntercept(true).setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(va,stdScaler,lr))

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(1, 0.1, 0.01, 0.001)).
addGrid(lr.fitIntercept).
addGrid(lr.maxIter, Array(10, 20, 40, 100)).build()

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bce = new BinaryClassificationEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(bce.setMetricName("areaUnderROC")).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val cvmodel = cv.fit(trainingData)

-- CV hyperparameter evaluation

cvmodel.getEstimatorParamMaps.zip(cvmodel.avgMetrics)
res11: Array[(org.apache.spark.ml.param.ParamMap, Double)] =
Array(({
        logreg_6bee34affaec-fitIntercept: true,
        logreg_6bee34affaec-maxIter: 10,
        logreg_6bee34affaec-regParam: 1.0
},0.9564730221285483), ({
        logreg_6bee34affaec-fitIntercept: true,
        logreg_6bee34affaec-maxIter: 10,
        logreg_6bee34affaec-regParam: 0.1
},0.9709532142925806), ({
        logreg_6bee34affaec-fitIntercept: true,
        logreg_6bee34affaec-maxIter: 10,
        logreg_6bee34affaec-regParam: 0.01
},0.9730688497870031), ({
        logreg_6bee34affaec-fitIntercept: true,
        logreg_6bee34affaec-maxIter: 10,
        logreg_6bee34affaec-regParam: 0.001
},0.9721376565800625), ({
        logreg_6bee34affaec-fitIntercept: false,
        logreg_6bee34affaec-maxIter: 10,
        logreg_6bee34affaec-regParam: 1.0
},0.9564910458176685), ({
        logreg_6bee34affaec-fitIntercept: false,
        ...

-- extract best LR model 

import org.apache.spark.ml.PipelineModel
val bestmodel = cvmodel.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.classification.LogisticRegressionModel
val lrmodel = bestmodel.stages.last.asInstanceOf[LogisticRegressionModel]

lrmodel.getRegParam
res11: Double = 0.01

lrmodel.getMaxIter
res12: Int = 10

lrmodel.getThreshold
res13: Double = 0.5

lrmodel.getFitIntercept
res14: Boolean = true


-- collecting feature importance

val featureImp = va.getInputCols.zip(lrmodel.coefficients.toArray)

val columns = Array("feature", "Importance")
val featureImpDF = spark.createDataFrame(featureImp).toDF(columns: _*)

featureImpDF.orderBy($"Importance".desc).show()
+------------------+--------------------+
|           feature|          Importance|
+------------------+--------------------+
|        Polydipsia|   1.356735467133985|
|          Polyuria|  1.2689238148780568|
|      Irritability|  0.7600925048588555|
|    Genital thrush|  0.5183595100151677|
|   partial paresis| 0.45647416230809595|
|sudden weight loss| 0.34628972221973175|
|        Polyphagia| 0.24582484890294778|
|   visual blurring| 0.22977277137907454|
|          weakness| 0.17787239339663163|
|           Obesity|-0.09336910202277421|
|               Age|-0.09726027644785243|
|          Alopecia| -0.1496818928945016|
|  muscle stiffness|-0.18137708194084315|
|   delayed healing|-0.39375546555300983|
|           Itching| -0.5180627231386878|
|            Gender| -1.2338681432002692|
+------------------+--------------------+


import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary

val trainingSummary = lrmodel.summary

trainingSummary.objectiveHistory
res16: Array[Double] = Array(0.6730116670092521, 0.33824232040017166, 0.2899609595749758, 0.24511563870522957, 0.2261389515983281, 0.2223248903075804, 0.21995977431666675, 0.21986328991327583, 0.21982346585352386, 0.2197955821748887, 0.21979228387808522)

val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]

binarySummary.areaUnderROC
res17: Double = 0.9799524613748669

binarySummary.accuracy
res19: Double = 0.9315068493150684


-- collecting metric performance

val pred = bestmodel.transform(testData)

val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res19: Double = 0.9717638430509716

bceval.setMetricName("areaUnderPR").evaluate(pred)
res20: Double = 0.9879640674546408

val validPredicts = pred.select("label","prediction").rdd.map( row => (row.getDouble(0),row.getDouble(1)) )

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)

metrics.accuracy
res22: Double = 0.9483870967741935

metrics.confusionMatrix
res22: org.apache.spark.mllib.linalg.Matrix =
52.0  6.0
2.0   95.0
