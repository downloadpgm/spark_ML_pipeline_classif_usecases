---- Feature extraction & Data Munging --------------

val raw = sc.textFile("census/adult.data").map(x => x.split(",")).map( x => x.trim()))

raw.take(2)
res3: Array[Array[String]] = Array(Array(39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K), Array(50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K))

val orderingDesc = Ordering.by[(String, Int), Int](_._2)

---- Replacing the "?" with most frequent category

raw.map( x => (x(1),1)).reduceByKey( _+_ ).top(10)(orderingDesc)
res1: Array[(String, Int)] = Array((Private,22696), (Self-emp-not-inc,2541), (Local-gov,2093), (?,1836), (State-gov,1298), (Self-emp-inc,1116), (Federal-gov,960), (Without-pay,14), (Never-worked,7))

raw.map( x => (x(6),1)).reduceByKey( _+_ ).top(10)(orderingDesc)
res4: Array[(String, Int)] = Array((Prof-specialty,4140), (Craft-repair,4099), (Exec-managerial,4066), (Adm-clerical,3770), (Sales,3650), (Other-service,3295), (Machine-op-inspct,2002), (?,1843), (Transport-moving,1597), (Handlers-cleaners,1370))

raw.map( x => (x(13),1)).reduceByKey( _+_ ).top(10)(orderingDesc)
res3: Array[(String, Int)] = Array((United-States,29170), (Mexico,643), (?,583), (Philippines,198), (Germany,137), (Canada,121), (Puerto-Rico,114), (El-Salvador,106), (India,100), (Cuba,95))

val rdd = raw.map(x => Array(x(0),x(1).replace("?","Private"),x(2),x(3),x(4),x(5),x(6).replace("?","Prof-specialty"),
                             x(7),x(8),x(9),x(10),x(11),x(12),x(13).replace("?","United-States"),x(14)))
							 
rdd.take(2)
res4: Array[Array[String]] = Array(Array(39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K), Array(50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K))

val categ_workclass = rdd.map(x => x(1)).distinct.zipWithIndex.collectAsMap
categ_workclass: scala.collection.Map[String,Long] = Map(Without-pay -> 5, Never-worked -> 7, Self-emp-inc -> 2, Local-gov -> 0, Self-emp-not-inc -> 4, Federal-gov -> 3, State-gov -> 1, Private -> 6)

val categ_education = rdd.map(x => x(3)).distinct.zipWithIndex.collectAsMap
categ_education: scala.collection.Map[String,Long] = Map(7th-8th -> 3, Preschool -> 8, Prof-school -> 0, Doctorate -> 9, Assoc-acdm -> 15, 11th -> 4, 1st-4th -> 7, Some-college -> 2, 9th -> 12, 10th -> 14, 5th-6th -> 6, HS-grad -> 1, Bachelors -> 11, Masters -> 13, 12th -> 10, Assoc-voc -> 5)

val categ_marital = rdd.map(x => x(5)).distinct.zipWithIndex.collectAsMap
categ_marital: scala.collection.Map[String,Long] = Map(Married-spouse-absent -> 0, Never-married -> 3, Married-civ-spouse -> 4, Widowed -> 5, Divorced -> 2, Separated -> 6, Married-AF-spouse -> 1)

val categ_occupation = rdd.map(x => x(6)).distinct.zipWithIndex.collectAsMap
categ_occupation: scala.collection.Map[String,Long] = Map(Handlers-cleaners -> 9, Transport-moving -> 7, Farming-fishing -> 13, Other-service -> 2, Prof-specialty -> 1, Protective-serv -> 8, Priv-house-serv -> 12, Machine-op-inspct -> 10, Tech-support -> 4, Sales -> 5, Exec-managerial -> 11, Craft-repair -> 0, Armed-Forces -> 6, Adm-clerical -> 3)

val categ_relatship = rdd.map(x => x(7)).distinct.zipWithIndex.collectAsMap
categ_relatship: scala.collection.Map[String,Long] = Map(Wife -> 1, Not-in-family -> 0, Own-child -> 3, Other-relative -> 2, Unmarried -> 5, Husband -> 4)

val categ_race = rdd.map(x => x(8)).distinct.zipWithIndex.collectAsMap
categ_race: scala.collection.Map[String,Long] = Map(White -> 4, Asian-Pac-Islander -> 0, Other -> 1, Black -> 3, Amer-Indian-Eskimo -> 2)

val categ_sex = rdd.map(x => x(9)).distinct.zipWithIndex.collectAsMap
categ_sex: scala.collection.Map[String,Long] = Map(Male -> 1, Female -> 0)

val categ_country = rdd.map(x => x(13)).distinct.zipWithIndex.collectAsMap
categ_country: scala.collection.Map[String,Long] = Map(Hong -> 7, Vietnam -> 11, Canada -> 3, Taiwan -> 14, Thailand -> 37, Outlying-US(Guam-USVI-etc) -> 18, Peru -> 8, Honduras -> 6, Laos -> 22, Columbia -> 13, England -> 40, Guatemala -> 36, Germany -> 28, Scotland -> 15, Ireland -> 33, France -> 27, Italy -> 25, Poland -> 17, Haiti -> 32, Ecuador -> 21, China -> 24, Greece -> 26, Portugal -> 1, South -> 19, Philippines -> 35, India -> 39, Trinadad&Tobago -> 38, Dominican-Republic -> 34, Hungary -> 0, United-States -> 2, Mexico -> 20, Cuba -> 31, Holand-Netherlands -> 30, Cambodia -> 9, Japan -> 5, Puerto-Rico -> 29, Nicaragua -> 23, El-Salvador -> 10, Jamaica -> 4, Iran -> 12, Yugoslavia -> 16)

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
	val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
	if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
    categoryFeatures
	})
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,1,3,5,6,7,8,9,13)

val categories = rdd.map(x => x(14)).distinct.zipWithIndex.collectAsMap
categories: scala.collection.Map[String,Long] = Map(<=50K -> 1, >50K -> 0)

// remove x(4) : education-num
val rdd1 = rdd.map(x => {
  val y = Array(x(0),x(2),x(10),x(11),x(12),categories(x(14)))
  y.map(  z => z.toString.toDouble)
})

val vect = concat.zip(rdd1).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(2)
res5: Array[Array[Double]] = Array(Array(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.0, 77516.0, 2174.0, 0.0, 40.0, 1.0), Array(0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...

val rdd1_dt = rdd.map( x => {
  val y = Array(x(0),categ_workclass(x(1)),x(2),categ_education(x(3)),categ_marital(x(5)),categ_occupation(x(6)),categ_relatship(x(7)),categ_race(x(8)),categ_sex(x(9)),x(10),x(11),x(12),categ_country(x(13)),categories(x(14)))
  y.map( z => z.toString.toDouble)
})

rdd1_dt.take(2)
res6: Array[Array[Double]] = Array(Array(39.0, 1.0, 77516.0, 11.0, 3.0, 3.0, 0.0, 4.0, 1.0, 2174.0, 0.0, 40.0, 2.0, 1.0), Array(50.0, 4.0, 83311.0, 11.0, 4.0, 11.0, 4.0, 4.0, 1.0, 0.0, 0.0, 13.0, 2.0, 1.0))

val data = vect.zip(rdd1_dt)

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val train_rdd = sets(0)
val test_rdd = sets(1)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint  

val trainSet = train_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

---- MLlib logistic regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainSet, testSet)
iter, step -> pred / total, AuPR, AuROC
100, 0.100 -> 7344 / 9644, 0.7611, 0.5032
100, 0.010 -> 7340 / 9644, 0.7608, 0.5024
100, 0.001 -> 7340 / 9644, 0.7608, 0.5024
300, 0.100 -> 7436 / 9644, 0.7686, 0.5236
300, 0.010 -> 7391 / 9644, 0.7649, 0.5134
300, 0.001 -> 7391 / 9644, 0.7649, 0.5134
500, 0.100 -> 7412 / 9644, 0.7667, 0.5182
500, 0.010 -> 7386 / 9644, 0.7645, 0.5123
500, 0.001 -> 7394 / 9644, 0.7651, 0.5140

---- MLlib SVM regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(100,300,500), trainSet, testSet)
iter -> pred / total, AuPR, AuROC
100 -> 2315 / 9644, 0.7600, 0.5000
300 -> 7377 / 9644, 0.7638, 0.5104
500 -> 7406 / 9644, 0.7661, 0.5166

---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res11: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,90.0,1484705.0,99999.0,4356.0,99.0]

matrixSummary.min
res11: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,90.0,1484705.0,99999.0,4356.0,99.0]

matrixSummary.min
res12: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,17.0,12285.0,0.0,0.0,1.0]

matrixSummary.mean
res13: org.apache.spark.mllib.linalg.Vector = [0.06401361434742768,0.03940306322817123,0.034210411484923854,0.029672295675699262,0.07941702666143038,4.7999301828337043E-4,0.75258541693939,2.1817864467425929E-4,0.01762883448968015,0.32272985120216435,0.22293493912815815,0.020203342496836408,0.03757036261290745,0.04171575686171838,0.010647117860103854,0.005149016014312519,0.001832700615263778,0.013527075969804075,0.012567089933237335,0.16236854736658377,0.016101583976960335,0.05341013221625867,0.029192302657415892,0.03242134659859493,0.012567089933237335,8.290788497621852E-4,0.13823798926561068,0.3267443382641707,0.4600078544312083,0.03032683160972204,0.03128681764628878,0.12558362787450364,0.18532094078631584,0.10149670550246542,0.11423833835144216,0.027621416415761227,0.1109220229523934...

matrixSummary.variance
res14: org.apache.spark.mllib.linalg.Vector = [0.05991848611242467,0.03785211354097405,0.033041501020938197,0.028793106953243348,0.07311315288340883,4.7978356069117135E-4,0.18620873250333134,2.1814056147567763E-4,0.01731881440330639,0.21858483245369142,0.17324251160735907,0.019796031263046115,0.03616040835181911,0.03997729692966474,0.0105342164095887,0.005122727182331608,0.001829421651865898,0.013344676490197275,0.0124096996901391,0.13601093712903395,0.01584301429187038,0.05055969620209895,0.028341348818388405,0.03137157180473223,0.0124096996901391,8.284276270664391E-4,0.11913344606417112,0.21999207519104075,0.24841146790828253,0.029408398151127738,0.0303092752551759,0.10981717223674195,0.15098367797523637,0.09119910381398436,0.10119235600436047,0.026859645811172245,0.09862263124694606,...

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache

----- with MLlib logistic regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainScaled, testScaled)
iter, step -> pred / total, AuPR, AuROC
100, 0.100 -> 6871 / 9644, 0.9282, 0.7851
100, 0.010 -> 6885 / 9644, 0.9249, 0.7822
100, 0.001 -> 6887 / 9644, 0.9249, 0.7824
300, 0.100 -> 6874 / 9644, 0.9286, 0.7856
300, 0.010 -> 6885 / 9644, 0.9249, 0.7822
300, 0.001 -> 6887 / 9644, 0.9249, 0.7824
500, 0.100 -> 6874 / 9644, 0.9286, 0.7856
500, 0.010 -> 6885 / 9644, 0.9249, 0.7822
500, 0.001 -> 6887 / 9644, 0.9249, 0.7824

----- with MLlib SVM regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

scala> iterateSVM(Array(100,300,500), trainScaled, testScaled)
iter -> pred / total, AuPR, AuROC
100 -> 7118 / 9644, 0.9241, 0.7921
300 -> 7154 / 9644, 0.9242, 0.7938
500 -> 7154 / 9644, 0.9242, 0.7938

---- MLlib Decision Tree regression --------------

val trainSet = train_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 1->8, 3->16, 4->7, 5->14, 6->6, 7->5, 8->2, 12->41)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, AuPR, AuROC") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AuPR, AuROC
gini, 10, 48 -> 8191 / 9644, 0.8739, 0.7606
gini, 10, 64 -> 8206 / 9644, 0.8748, 0.7627
gini, 20, 48 -> 7892 / 9644, 0.8788, 0.7600
gini, 20, 64 -> 7939 / 9644, 0.8781, 0.7603
gini, 30, 48 -> 7776 / 9644, 0.8688, 0.7403
gini, 30, 64 -> 7818 / 9644, 0.8702, 0.7438
entropy, 10, 48 -> 8176 / 9644, 0.8722, 0.7573
entropy, 10, 64 -> 8207 / 9644, 0.8728, 0.7592
entropy, 20, 48 -> 7926 / 9644, 0.8819, 0.7661
entropy, 20, 64 -> 8026 / 9644, 0.8863, 0.7761 *
entropy, 30, 48 -> 7826 / 9644, 0.8721, 0.7471
entropy, 30, 64 -> 7839 / 9644, 0.8744, 0.7513

val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "entropy", 20, 64)

model.toDebugString
res22: String =
"DecisionTreeModel classifier of depth 20 with 3091 nodes
  If (feature 4 in {0.0,5.0,1.0,6.0,2.0,4.0})
   If (feature 4 in {0.0,5.0,1.0,6.0,2.0})
    If (feature 9 <= 7565.5)
     If (feature 11 <= 43.5)
      If (feature 3 in {0.0,5.0,10.0,14.0,6.0,9.0,13.0,12.0,7.0,3.0,11.0,8.0,4.0,15.0})
       If (feature 9 <= 4243.5)
        If (feature 11 <= 37.5)
         If (feature 3 in {8.0,9.0,0.0})
          If (feature 4 in {0.0,6.0})
           If (feature 0 <= 36.5)
            Predict: 1.0
           Else (feature 0 > 36.5)
            Predict: 0.0
          Else (feature 4 not in {0.0,6.0})
           Predict: 1.0
         Else (feature 3 not in {8.0,9.0,0.0})
          If (feature 10 <= 2358.0)
           If (feature 1 in {0.0,1.0,2.0,7.0,3.0,4.0})
            If (feat...

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res23: Array[(Double, Double)] = Array((1.0,1.0), (1.0,1.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (0.0,1.0), (0.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 8026
validPredicts.count                            // 9644
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.8862505110735878
metrics.areaUnderROC  // 0.7761364584079283
