---- Feature extraction & Data Munging --------------

val rdd2x = sc.textFile("spark/data/obesity/ObesityDataSet_raw_and_data_sinthetic.csv").map( x => x.split(","))

rdd2x.first
res0: Array[String] = Array(Gender, Age, Height, Weight, family_history_with_overweight, FAVC, FCVC, NCP, CAEC, SMOKE, CH2O, SCC, FAF, TUE, CALC, MTRANS, NObeyesdad)

val rdd1x = rdd2x.filter( x => x(4) != "family_history_with_overweight")

rdd1x.map( x => Array(x(0),x(4),x(5),x(8),x(9),x(11),x(14),x(15),x(1),x(2),x(3),x(6),x(7),x(10),x(12),x(13),x(16))).take(2)
res1: Array[Array[String]] = Array(Array(Female, yes, no, Sometimes, no, no, no, Public_Transportation, 21, 1.62, 64, 2, 3, 2, 0, 1, Normal_Weight), Array(Female, yes, no, Sometimes, yes, yes, Sometimes, Public_Transportation, 21, 1.52, 56, 3, 3, 3, 3, 0, Normal_Weight))

val rdd = rdd1x.map( x => Array(x(0),x(4),x(5),x(8),x(9),x(11),x(14),x(15),x(1),x(2),x(3),x(6),x(7),x(10),x(12),x(13),x(16)))

rdd.take(2)
res2: Array[Array[String]] = Array(Array(Female, yes, no, Sometimes, no, no, no, Public_Transportation, 21, 1.62, 64, 2, 3, 2, 0, 1, Normal_Weight), Array(Female, yes, no, Sometimes, yes, yes, Sometimes, Public_Transportation, 21, 1.52, 56, 3, 3, 3, 3, 0, Normal_Weight))


rdd1x.map( x => x(16)).distinct.take(10)  // NObeyesdad
res3: Array[String] = Array(Normal_Weight, Obesity_Type_III, Insufficient_Weight, Overweight_Level_II, Obesity_Type_I, Obesity_Type_II, Overweight_Level_I)

rdd1x.map( x => x(15)).distinct.take(10)  // MTRANS
res4: Array[String] = Array(Motorbike, Public_Transportation, Walking, Bike, Automobile)

rdd1x.map( x => x(14)).distinct.take(10)  // CALC
res5: Array[String] = Array(Sometimes, Frequently, no, Always)

rdd1x.map( x => x(8)).distinct.take(10)  // CAEC
res6: Array[String] = Array(Sometimes, Frequently, no, Always)

rdd1x.map( x => x(0)).distinct.take(10)  // Gender
res7: Array[String] = Array(Female, Male)

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
	val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
	if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
    categoryFeatures
	})
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,0,1,2,3,4,5,6,7)

concat.take(2)
res0: Array[Array[Double]] = Array(Array(0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0), Array(0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0))

val categories = rdd.map( x => x(x.size-1)).distinct.zipWithIndex.collectAsMap
categories: scala.collection.Map[String,Long] = Map(Overweight_Level_II -> 3, Obesity_Type_III -> 1, Normal_Weight -> 0, Overweight_Level_I -> 6, Obesity_Type_I -> 4, Obesity_Type_II -> 5, Insufficient_Weight -> 2)

val rdd2 = rdd.map( x => x.slice(8,x.size)).map( y => y.map( x => {
                     try { x.toDouble } catch { case _ : Throwable => categories(x).toString.toDouble } }))
					 
rdd2.take(5)
res1: Array[Array[Double]] = Array(Array(21.0, 1.62, 64.0, 2.0, 3.0, 2.0, 0.0, 1.0, 0.0), Array(21.0, 1.52, 56.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0), Array(23.0, 1.8, 77.0, 2.0, 3.0, 2.0, 2.0, 1.0, 0.0), Array(27.0, 1.8, 87.0, 3.0, 3.0, 2.0, 2.0, 0.0, 6.0), Array(22.0, 1.78, 89.8, 2.0, 1.0, 2.0, 0.0, 0.0, 3.0))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(2)
res2: Array[Array[Double]] = Array(Array(0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 21.0, 1.62, 64.0, 2.0, 3.0, 2.0, 0.0, 1.0, 0.0), Array(0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 21.0, 1.52, 56.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0))


val categ_gender = rdd.map( x => x(0)).distinct.zipWithIndex.collectAsMap
categ_gender: scala.collection.Map[String,Long] = Map(Male -> 1, Female -> 0)

val categ_yesno = rdd.map( x => x(1)).distinct.zipWithIndex.collectAsMap
categ_yesno: scala.collection.Map[String,Long] = Map(no -> 1, yes -> 0)

val categ_frequency = rdd.map( x => x(3)).distinct.zipWithIndex.collectAsMap
categ_frequency: scala.collection.Map[String,Long] = Map(Frequently -> 1, Always -> 3, no -> 2, Sometimes -> 0)

val categ_transp = rdd.map( x => x(7)).distinct.zipWithIndex.collectAsMap
categ_transp: scala.collection.Map[String,Long] = Map(Public_Transportation -> 1, Motorbike -> 0, Walking -> 2, Automobile -> 4, Bike -> 3)

val rdd2_dt = rdd.map( x => {
  val y = Array(categ_gender(x(0)),categ_yesno(x(1)),categ_yesno(x(2)),categ_frequency(x(3)),categ_yesno(x(4)),categ_yesno(x(5)),categ_frequency(x(6)),categ_transp(x(7)),x(8),x(9),x(10),x(11),x(12),x(13),x(14),x(15),categories(x(16)))
  y.map( z => z.toString.toDouble)
  })
  
rdd2_dt.take(2)
res22: Array[Array[Double]] = Array(Array(0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 21.0, 1.62, 64.0, 2.0, 3.0, 2.0, 0.0, 1.0, 0.0), Array(0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 21.0, 1.52, 56.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0))

val data = vect.zip(rdd2_dt)

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val train_rdd = sets(0)
val test_rdd = sets(1)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint  

val trainSet = train_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

---- MLlib Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(7).run(trainSet)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res25: Array[(Double, Double)] = Array((3.0,0.0), (0.0,6.0), (3.0,0.0), (0.0,4.0), (6.0,3.0), (2.0,0.0), (3.0,4.0), (6.0,0.0), (4.0,4.0), (3.0,3.0), (6.0,3.0), (0.0,0.0), (6.0,0.0), (6.0,0.0), (2.0,0.0), (2.0,0.0), (2.0,2.0), (6.0,0.0), (0.0,3.0), (6.0,5.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 515
validPredicts.count                            // 640
val accuracy = metrics.accuracy   // 0.8046875

metrics.confusionMatrix
res28: org.apache.spark.mllib.linalg.Matrix =
43.0  1.0   16.0  11.0  0.0   0.0   14.0
0.0   99.0  0.0   0.0   1.0   0.0   0.0
2.0   0.0   76.0  0.0   0.0   0.0   0.0
9.0   0.0   0.0   71.0  8.0   0.0   13.0
3.0   2.0   0.0   5.0   89.0  6.0   0.0
1.0   0.0   0.0   1.0   4.0   83.0  1.0
14.0  0.0   0.0   10.0  3.0   0.0   54.


---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res30: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,61.0,1.975663,173.0,3.0,4.0,3.0,3.0,2.0]

matrixSummary.min
res32: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,14.0,1.45,39.0,1.0,1.0,1.0,0.0,0.0]

matrixSummary.mean
res33: org.apache.spark.mllib.linalg.Vector = [0.5105370496261047,0.18354860639021073,0.1142080217539089,0.8382053025152957,0.1142080217539089,0.024473147518694765,0.023113528212100613,0.9802855200543847,0.9551325628823929,0.6648538409245411,0.03331067301155676,0.30115567641060503,6.798096532970768E-4,0.004758667573079538,0.7532290958531611,0.02515295717199184,0.003399048266485384,0.21346023113528212,24.190304554724676,1.7021906329027878,86.6447656274643,2.427807407205987,2.6981458524813045,2.0094059218218905,1.0162040679809652,0.6600881998640377]


matrixSummary.variance
res34: org.apache.spark.mllib.linalg.Vector = [0.25005896308217374,0.14996046005077762,0.1012333689424104,0.1357094299310479,0.1012333689424104,0.023890453530154412,0.022594653088971822,0.019338966041889224,0.04288350282329111,0.22297479154816244,0.032222977566281445,0.21060410568034146,6.798096532970768E-4,0.004739244440128193,0.18600147060863775,0.024536966384106327,0.0033897991555561723,0.16800917511804178,40.02015236906922,0.008663399400503273,706.6786568283504,0.27842227757615684,0.6068350188689434,0.38101349173680243,0.6939720015387695,0.3656478510610262]

---- Apply standardization to dataset -------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, false).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))

---- MLlib Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(7).run(trainScaled)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

validPredicts.take(20)
res36: Array[(Double, Double)] = Array((0.0,0.0), (0.0,6.0), (0.0,0.0), (4.0,4.0), (6.0,3.0), (2.0,0.0), (5.0,4.0), (2.0,0.0), (4.0,4.0), (3.0,3.0), (4.0,3.0), (0.0,0.0), (0.0,0.0), (6.0,0.0), (2.0,0.0), (0.0,0.0), (2.0,2.0), (6.0,0.0), (3.0,3.0), (1.0,5.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 566
validPredicts.count                            // 640
val accuracy = metrics.accuracy   // 0.884375

metrics.confusionMatrix
res39: org.apache.spark.mllib.linalg.Matrix =
42.0  0.0    31.0  0.0   0.0    0.0   12.0
0.0   100.0  0.0   0.0   0.0    0.0   0.0
4.0   0.0    74.0  0.0   0.0    0.0   0.0
0.0   1.0    0.0   92.0  4.0    0.0   4.0
0.0   1.0    0.0   1.0   101.0  2.0   0.0
0.0   4.0    0.0   0.0   1.0    85.0  0.0
6.0   0.0    1.0   1.0   1.0    0.0   72.0

---- MLlib Decision Tree regression --------------

val trainSet = train_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 0->2, 1->2, 2->2, 3->4, 4->2, 5->2, 6->4, 7->5)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, accuracy") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 7, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new MulticlassMetrics(validPredicts)
    println("%s, %d, %d ->  %d / %d, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.accuracy))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, accuracy
gini, 10, 32 ->  600 / 640, 0.9375
gini, 10, 48 ->  583 / 640, 0.9109
gini, 10, 64 ->  589 / 640, 0.9203
gini, 20, 32 ->  597 / 640, 0.9328
gini, 20, 48 ->  583 / 640, 0.9109
gini, 20, 64 ->  589 / 640, 0.9203
gini, 30, 32 ->  597 / 640, 0.9328
gini, 30, 48 ->  583 / 640, 0.9109
gini, 30, 64 ->  589 / 640, 0.9203
entropy, 10, 32 ->  591 / 640, 0.9234
entropy, 10, 48 ->  601 / 640, 0.9391
entropy, 10, 64 ->  604 / 640, 0.9438  *
entropy, 20, 32 ->  591 / 640, 0.9234
entropy, 20, 48 ->  601 / 640, 0.9391
entropy, 20, 64 ->  604 / 640, 0.9438
entropy, 30, 32 ->  591 / 640, 0.9234
entropy, 30, 48 ->  601 / 640, 0.9391
entropy, 30, 64 ->  604 / 640, 0.9438


val model = DecisionTree.trainClassifier(trainSet, 7, categoricalFeaturesInfo, "entropy", 10, 64)

model.toDebugString
res43: String =
"DecisionTreeModel classifier of depth 10 with 149 nodes
  If (feature 10 <= 97.96574849999999)
   If (feature 10 <= 60.0589965)
    If (feature 9 <= 1.660162)
     If (feature 10 <= 46.827811)
      If (feature 11 <= 2.0002329999999997)
       If (feature 10 <= 42.034996)
        Predict: 2.0
       Else (feature 10 > 42.034996)
        If (feature 3 in {0.0})
         Predict: 0.0
        Else (feature 3 not in {0.0})
         Predict: 2.0
      Else (feature 11 > 2.0002329999999997)
       If (feature 7 in {2.0,1.0})
        Predict: 2.0
       Else (feature 7 not in {2.0,1.0})
        Predict: 0.0
     Else (feature 10 > 46.827811)
      If (feature 9 <= 1.530133)
       If (feature 10 <= 56.1476885)
        If (feature 5 in {0.0})
         If (feature 1 in {0.0})
  ...


val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res44: Array[(Double, Double)] = Array((0.0,0.0), (6.0,6.0), (0.0,0.0), (4.0,4.0), (6.0,3.0), (0.0,0.0), (4.0,4.0), (0.0,0.0), (4.0,4.0), (3.0,3.0), (3.0,3.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (2.0,2.0), (0.0,0.0), (3.0,3.0), (4.0,5.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 604
validPredicts.count                            // 640
val accuracy = metrics.accuracy   // 0.94375

metrics.confusionMatrix
res47: org.apache.spark.mllib.linalg.Matrix =
78.0  0.0    2.0   0.0   0.0    0.0   5.0
0.0   100.0  0.0   0.0   0.0    0.0   0.0
4.0   0.0    74.0  0.0   0.0    0.0   0.0
0.0   0.0    0.0   93.0  2.0    0.0   6.0
0.0   0.0    0.0   0.0   104.0  1.0   0.0
0.0   1.0    0.0   0.0   3.0    86.0  0.0
6.0   0.0    0.0   6.0   0.0    0.0   69.0