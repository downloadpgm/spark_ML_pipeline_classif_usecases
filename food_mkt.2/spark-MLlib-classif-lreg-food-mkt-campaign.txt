---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("spark/data/ifood_mkt/ml_project1_data.csv").map( x => x.split(","))

rdd.take(2)
res1: Array[Array[String]] = Array(Array(ID, Year_Birth, Education, Marital_Status, Income, Kidhome, Teenhome, Dt_Customer, Recency, MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds, NumDealsPurchases, NumWebPurchases, NumCatalogPurchases, NumStorePurchases, NumWebVisitsMonth, AcceptedCmp3, AcceptedCmp4, AcceptedCmp5, AcceptedCmp1, AcceptedCmp2, Complain, Z_CostContact, Z_Revenue, Response), Array(5524, 1957, Graduation, Single, 58138, 0, 0, 2012-09-04, 58, 635, 88, 546, 172, 88, 88, 3, 8, 10, 4, 7, 0, 0, 0, 0, 0, 0, 3, 11, 1))

val hdr = rdd.take(1)(0)
hdr: Array[String] = Array(ID, Year_Birth, Education, Marital_Status, Income, Kidhome, Teenhome, Dt_Customer, Recency, MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds, NumDealsPurchases, NumWebPurchases, NumCatalogPurchases, NumStorePurchases, NumWebVisitsMonth, AcceptedCmp3, AcceptedCmp4, AcceptedCmp5, AcceptedCmp1, AcceptedCmp2, Complain, Z_CostContact, Z_Revenue, Response)

val rdd1 = rdd.filter( x => ! x.contains("Year_Birth"))

rdd1.take(10).map( x => x.mkString(", ")).foreach(println)
5524, 1957, Graduation, Single, 58138, 0, 0, 2012-09-04, 58, 635, 88, 546, 172, 88, 88, 3, 8, 10, 4, 7, 0, 0, 0, 0, 0, 0, 3, 11, 1
2174, 1954, Graduation, Single, 46344, 1, 1, 2014-03-08, 38, 11, 1, 6, 2, 1, 6, 2, 1, 1, 2, 5, 0, 0, 0, 0, 0, 0, 3, 11, 0
4141, 1965, Graduation, Together, 71613, 0, 0, 2013-08-21, 26, 426, 49, 127, 111, 21, 42, 1, 8, 2, 10, 4, 0, 0, 0, 0, 0, 0, 3, 11, 0
6182, 1984, Graduation, Together, 26646, 1, 0, 2014-02-10, 26, 11, 4, 20, 10, 3, 5, 2, 2, 0, 4, 6, 0, 0, 0, 0, 0, 0, 3, 11, 0
5324, 1981, PhD, Married, 58293, 1, 0, 2014-01-19, 94, 173, 43, 118, 46, 27, 15, 5, 5, 3, 6, 5, 0, 0, 0, 0, 0, 0, 3, 11, 0
7446, 1967, Master, Together, 62513, 0, 1, 2013-09-09, 16, 520, 42, 98, 0, 42, 14, 2, 6, 4, 10, 6, 0, 0, 0, 0, 0, 0, 3, 11, 0
965, 1971, Graduation, Divorced, 55635, 0, 1, 2012-11-13, 34, 235, 65, 164, 50, 49, 27, 4, 7, 3, 7, 6, 0, 0, 0, 0, 0, 0, 3, 11, 0
6177, 1985, PhD, Married, 33454, 1, 0, 2013-05-08, 32, 76, 10, 56, 3, 1, 23, 2, 4, 0, 4, 8, 0, 0, 0, 0, 0, 0, 3, 11, 0
4855, 1974, PhD, Together, 30351, 1, 0, 2013-06-06, 19, 14, 0, 24, 3, 3, 2, 1, 3, 0, 2, 9, 0, 0, 0, 0, 0, 0, 3, 11, 1
5899, 1950, PhD, Together, 5648, 1, 1, 2014-03-13, 68, 28, 0, 6, 1, 1, 13, 1, 1, 0, 0, 20, 1, 0, 0, 0, 0, 0, 3, 11, 0

// Education
rdd1.map( x => (x(2),1)).reduceByKey(_+_).take(10)
res3: Array[(String, Int)] = Array((Graduation,1127), (PhD,486), (Basic,54), (Master,370), (2n Cycle,203))

// Marital_Status
rdd1.map( x => (x(3),1)).reduceByKey(_+_).take(10)
res4: Array[(String, Int)] = Array((Single,480), (Married,864), (Together,580), (Widow,77), (Divorced,232), (YOLO,2), (Alone,3), (Absurd,2))

// Response
rdd1.map( x => (x(28),1)).reduceByKey(_+_).take(10)
res7: Array[(String, Int)] = Array((0,1906), (1,334))


---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  print(hdr(idx) + " : ")
  println(categories)
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
    val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories - 1) else Array.ofDim[Double](1)
    if (numCategories > 2) { 
      if (categoryIdx > 0) categoryFeatures(categoryIdx - 1) = 1.0
    }
    else categoryFeatures(0) = categoryIdx
    categoryFeatures
    })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd1,2,3)
Education : Map(Graduation -> 0, Basic -> 2, 2n Cycle -> 4, Master -> 3, PhD -> 1)
Marital_Status : Map(Widow -> 3, Together -> 2, Absurd -> 7, Alone -> 6, Divorced -> 4, YOLO -> 5, Single -> 0, Married -> 1)
concat: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[28] at map at <console>:30

concat.take(2)
res9: Array[Array[Double]] = Array(Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0))

import java.time._
import java.time.format.DateTimeFormatter
import java.time.temporal.ChronoUnit;

val rdd2 = rdd1.map( x => {
  val dt1 = LocalDate.parse(x(7),DateTimeFormatter.ofPattern("yyyy-MM-dd"))
  val dt2 = LocalDate.parse("2022-12-12",DateTimeFormatter.ofPattern("yyyy-MM-dd"))
  val numdays = ChronoUnit.DAYS.between(dt1, dt2)

  val y = Array(2020 - x(1).toString.toDouble) ++ x.slice(4,7) ++ Array(numdays) ++ x.slice(8,x.size)
  y.map( z => { 
                val t = z.toString
                if (t == "") 0.0 else t.toDouble
       })
})

rdd2.take(2)
res9: Array[Array[Double]] = Array(Array(63.0, 58138.0, 0.0, 0.0, 3751.0, 58.0, 635.0, 88.0, 546.0, 172.0, 88.0, 88.0, 3.0, 8.0, 10.0, 4.0, 7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 1.0), Array(66.0, 46344.0, 1.0, 1.0, 3201.0, 38.0, 11.0, 1.0, 6.0, 2.0, 1.0, 6.0, 2.0, 1.0, 1.0, 2.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(10).map( x => x.mkString(", ")).foreach(println)
0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 63.0, 58138.0, 0.0, 0.0, 3751.0, 58.0, 635.0, 88.0, 546.0, 172.0, 88.0, 88.0, 3.0, 8.0, 10.0, 4.0, 7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 1.0
0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.0, 46344.0, 1.0, 1.0, 3201.0, 38.0, 11.0, 1.0, 6.0, 2.0, 1.0, 6.0, 2.0, 1.0, 1.0, 2.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 55.0, 71613.0, 0.0, 0.0, 3400.0, 26.0, 426.0, 49.0, 127.0, 111.0, 21.0, 42.0, 1.0, 8.0, 2.0, 10.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 36.0, 26646.0, 1.0, 0.0, 3227.0, 26.0, 11.0, 4.0, 20.0, 10.0, 3.0, 5.0, 2.0, 2.0, 0.0, 4.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.0, 58293.0, 1.0, 0.0, 3249.0, 94.0, 173.0, 43.0, 118.0, 46.0, 27.0, 15.0, 5.0, 5.0, 3.0, 6.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 53.0, 62513.0, 0.0, 1.0, 3381.0, 16.0, 520.0, 42.0, 98.0, 0.0, 42.0, 14.0, 2.0, 6.0, 4.0, 10.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 49.0, 55635.0, 0.0, 1.0, 3681.0, 34.0, 235.0, 65.0, 164.0, 50.0, 49.0, 27.0, 4.0, 7.0, 3.0, 7.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 35.0, 33454.0, 1.0, 0.0, 3505.0, 32.0, 76.0, 10.0, 56.0, 3.0, 1.0, 23.0, 2.0, 4.0, 0.0, 4.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 46.0, 30351.0, 1.0, 0.0, 3476.0, 19.0, 14.0, 0.0, 24.0, 3.0, 3.0, 2.0, 1.0, 3.0, 0.0, 2.0, 9.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 1.0
1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 70.0, 5648.0, 1.0, 1.0, 3196.0, 68.0, 28.0, 0.0, 6.0, 1.0, 1.0, 13.0, 1.0, 1.0, 0.0, 0.0, 20.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0


---- calculate cosine column similarities to check multicolinearity

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = vect.map( x => Vectors.dense(x.slice(0, x.size-1)))
val matrix = new RowMatrix(vectors)

val colsims = matrix.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0
var j = 0.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    i = x._1._1
    j = i + 1
    while (j < x._1._2) { 
      print("0.00 ")
      j = j + 1
    }
    print(f"$sim%.2f ")
  } else {
      j = j + 1
      while (j < x._1._2) { 
        print("0.00 ")
        j = j + 1
      }
      print(f"$sim%.2f ")
    }
})




---- Splitting dataset as train/test sets  --------------

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)
   val f = x.slice(0, arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache
testSet.cache


----- Analyze the individual statistics and possibly standardize 

import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics

matrixSummary.max.toArray.foreach( x => print(f"$x%.2f\t"))
1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    127.00  666666.00   2.00    2.00    3787.00 99.00   1493.00 199.00  1582.00 259.00  263.00  362.00  15.00   27.00       22.00   13.00   20.00   1.00    1.00    1.00    1.00    1.00    1.00    3.00    11.00

matrixSummary.min.toArray.foreach( x => print(f"$x%.2f\t"))
0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    24.00   0.000.00    0.00    3088.00 0.00    0.00    0.00    1.00    0.00    0.00    0.00    0.00    0.00    0.000.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    3.00    11.00

matrixSummary.mean.toArray.foreach( x => print(f"$x%.2f\t"))
0.21    0.02    0.16    0.09    0.38    0.26    0.04    0.10    0.00    0.00    0.00    50.82   52037.93    0.44    0.49    3443.19 49.41   298.84  26.23   165.94  38.36   28.69   44.83   2.26    4.082.63    5.80    5.27    0.07    0.07    0.08    0.06    0.01    0.01    3.00    11.00

matrixSummary.variance.toArray.foreach( x => print(f"$x%.2f\t"))
0.17    0.02    0.14    0.08    0.24    0.19    0.04    0.09    0.00    0.00    0.00    144.87  718114889.92        0.28    0.29    41149.18        824.74  110060.14       1531.51 47318.61        3026.91     1828.86 2759.28 3.37    7.55    7.78    10.54   5.71    0.07    0.06    0.07    0.06    0.010.01    0.00    0.00


----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache
testScaled.cache

---- MLlib logistic regression STD --------------

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(100).setRegParam(0.01)
val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res16: Double = 0.35062471612581747

metrics.areaUnderROC
res17: Double = 0.801683262209578

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res18: Double = 0.7165932452276065

metrics1.confusionMatrix
res19: org.apache.spark.mllib.linalg.Matrix =
385.0  185.0
8.0    103.0


---- Hyperparameter tunning with logistic regression STD --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], regSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step, reg  -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes; reg <- regSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step).setRegParam(reg)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, reg, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(10, 20, 40, 100),Array(1, 0.1, 0.01, 0.001),Array(0.1, 0.01), trainScaled, testScaled)
iter, step, reg  -> pred / total, AuPR, AuROC
10, 1.000, 0.100 -> 493 / 681, 0.3456, 0.7843
10, 1.000, 0.010 -> 491 / 681, 0.3450, 0.7862
10, 0.100, 0.100 -> 465 / 681, 0.2882, 0.7090
10, 0.100, 0.010 -> 466 / 681, 0.2892, 0.7098
10, 0.010, 0.100 -> 455 / 681, 0.2674, 0.6784
10, 0.010, 0.010 -> 455 / 681, 0.2674, 0.6784
10, 0.001, 0.100 -> 453 / 681, 0.2618, 0.6694
10, 0.001, 0.010 -> 453 / 681, 0.2618, 0.6694
20, 1.000, 0.100 -> 491 / 681, 0.3487, 0.7934
20, 1.000, 0.010 -> 487 / 681, 0.3476, 0.7972
20, 0.100, 0.100 -> 472 / 681, 0.2972, 0.7187
20, 0.100, 0.010 -> 472 / 681, 0.2972, 0.7187
20, 0.010, 0.100 -> 455 / 681, 0.2674, 0.6784
20, 0.010, 0.010 -> 455 / 681, 0.2674, 0.6784
20, 0.001, 0.100 -> 453 / 681, 0.2618, 0.6694
20, 0.001, 0.010 -> 453 / 681, 0.2618, 0.6694
40, 1.000, 0.100 -> 495 / 681, 0.3574, 0.8042
40, 1.000, 0.010 -> 488 / 681, 0.3506, 0.8017
40, 0.100, 0.100 -> 473 / 681, 0.3021, 0.7269
40, 0.100, 0.010 -> 475 / 681, 0.3042, 0.7286
40, 0.010, 0.100 -> 455 / 681, 0.2674, 0.6784
40, 0.010, 0.010 -> 455 / 681, 0.2674, 0.6784
40, 0.001, 0.100 -> 453 / 681, 0.2618, 0.6694
40, 0.001, 0.010 -> 453 / 681, 0.2618, 0.6694
100, 1.000, 0.100 -> 495 / 681, 0.3574, 0.8042 *
100, 1.000, 0.010 -> 488 / 681, 0.3506, 0.8017
100, 0.100, 0.100 -> 493 / 681, 0.3363, 0.7662
100, 0.100, 0.010 -> 494 / 681, 0.3394, 0.7707
100, 0.010, 0.100 -> 455 / 681, 0.2674, 0.6784
100, 0.010, 0.010 -> 455 / 681, 0.2674, 0.6784
100, 0.001, 0.100 -> 453 / 681, 0.2618, 0.6694
100, 0.001, 0.010 -> 453 / 681, 0.2618, 0.6694



val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(100).setStepSize(1.0).setRegParam(0.1)

val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))

validPredicts.take(20)
res26: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,0.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res27: Double = 0.3573780544790919

metrics.areaUnderROC
res28: Double = 0.8041963015647225

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res29: Double = 0.7268722466960352

metrics1.confusionMatrix
res30: org.apache.spark.mllib.linalg.Matrix =
393.0  177.0
9.0    102.0


---- MLlib SVM regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(10, 20, 40, 100), trainScaled, testScaled)
iter -> pred / total, AuPR, AuROC
10 -> 519 / 681, 0.3567, 0.7600
20 -> 529 / 681, 0.3710, 0.7651 *
40 -> 533 / 681, 0.3664, 0.7468
100 -> 552 / 681, 0.3968, 0.7490


val model = SVMWithSGD.train(trainScaled, 20)

val validPredicts = testScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res48: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,1.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res49: Double = 0.37095187506986016

metrics.areaUnderROC
res50: Double = 0.7651019440493124

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res51: Double = 0.7767988252569751

metrics1.confusionMatrix
res52: org.apache.spark.mllib.linalg.Matrix =
446.0  124.0
28.0   83.0



----- with MLlib Decision tree regression ----------------------

val categ_education = rdd1.map(x => x(2)).distinct.zipWithIndex.collectAsMap
categ_education: scala.collection.Map[String,Long] = Map(Graduation -> 0, Basic -> 2, 2n Cycle -> 4, Master -> 3, PhD -> 1)

val categ_marital = rdd1.map(x => x(3)).distinct.zipWithIndex.collectAsMap
categ_marital: scala.collection.Map[String,Long] = Map(Widow -> 6, Together -> 5, Alone -> 3, Absurd -> 0, Divorced -> 7, YOLO -> 2, Single -> 1, Married -> 4)

val rdd2_dt = rdd1.map( x => {
  val y = Array(2020 - x(1).toString.toDouble,categ_education(x(2)),categ_marital(x(3)),x(4),x(5),x(6)) ++ x.slice(8,x.size)
  y.map( z => { 
                val t = z.toString
                if (t == "") 0.0 else t.toDouble
       })
})

rdd2_dt.take(10).map( x => x.mkString(", ")).foreach(println)
63.0, 0.0, 0.0, 58138.0, 0.0, 0.0, 58.0, 635.0, 88.0, 546.0, 172.0, 88.0, 88.0, 3.0, 8.0, 10.0, 4.0, 7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 1.0
66.0, 0.0, 0.0, 46344.0, 1.0, 1.0, 38.0, 11.0, 1.0, 6.0, 2.0, 1.0, 6.0, 2.0, 1.0, 1.0, 2.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
55.0, 0.0, 2.0, 71613.0, 0.0, 0.0, 26.0, 426.0, 49.0, 127.0, 111.0, 21.0, 42.0, 1.0, 8.0, 2.0, 10.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
36.0, 0.0, 2.0, 26646.0, 1.0, 0.0, 26.0, 11.0, 4.0, 20.0, 10.0, 3.0, 5.0, 2.0, 2.0, 0.0, 4.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
39.0, 1.0, 1.0, 58293.0, 1.0, 0.0, 94.0, 173.0, 43.0, 118.0, 46.0, 27.0, 15.0, 5.0, 5.0, 3.0, 6.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
53.0, 3.0, 2.0, 62513.0, 0.0, 1.0, 16.0, 520.0, 42.0, 98.0, 0.0, 42.0, 14.0, 2.0, 6.0, 4.0, 10.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
49.0, 0.0, 4.0, 55635.0, 0.0, 1.0, 34.0, 235.0, 65.0, 164.0, 50.0, 49.0, 27.0, 4.0, 7.0, 3.0, 7.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
35.0, 1.0, 1.0, 33454.0, 1.0, 0.0, 32.0, 76.0, 10.0, 56.0, 3.0, 1.0, 23.0, 2.0, 4.0, 0.0, 4.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0
46.0, 1.0, 2.0, 30351.0, 1.0, 0.0, 19.0, 14.0, 0.0, 24.0, 3.0, 3.0, 2.0, 1.0, 3.0, 0.0, 2.0, 9.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 1.0
70.0, 1.0, 2.0, 5648.0, 1.0, 1.0, 68.0, 28.0, 0.0, 6.0, 1.0, 1.0, 13.0, 1.0, 1.0, 0.0, 0.0, 20.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 11.0, 0.0

val data = rdd2_dt.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)
   val f = x.slice(0, arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })
 
val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache
testSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 1 -> 5, 2 -> 8)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, AuPR, AuROC") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AuPR, AuROC
gini, 10, 32 -> 564 / 681, 0.3627, 0.6289
gini, 10, 48 -> 569 / 681, 0.3832, 0.6333
gini, 10, 64 -> 569 / 681, 0.3864, 0.6406
gini, 20, 32 -> 558 / 681, 0.3585, 0.6527
gini, 20, 48 -> 558 / 681, 0.3666, 0.6672 *
gini, 20, 64 -> 552 / 681, 0.3521, 0.6656
gini, 30, 32 -> 558 / 681, 0.3585, 0.6527
gini, 30, 48 -> 558 / 681, 0.3666, 0.6672
gini, 30, 64 -> 552 / 681, 0.3521, 0.6656
entropy, 10, 32 -> 565 / 681, 0.3759, 0.6480
entropy, 10, 48 -> 564 / 681, 0.3742, 0.6507
entropy, 10, 64 -> 570 / 681, 0.4028, 0.6705
entropy, 20, 32 -> 550 / 681, 0.3336, 0.6421
entropy, 20, 48 -> 550 / 681, 0.3404, 0.6529
entropy, 20, 64 -> 554 / 681, 0.3574, 0.6673
entropy, 30, 32 -> 550 / 681, 0.3336, 0.6421
entropy, 30, 48 -> 550 / 681, 0.3404, 0.6529
entropy, 30, 64 -> 554 / 681, 0.3574, 0.6673


val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "gini", 20, 48)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res58: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res59: Double = 0.3665696132656485

metrics.areaUnderROC
res60: Double = 0.6672119487908961

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)
metrics1: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@5f986fcb

metrics1.accuracy
res61: Double = 0.8193832599118943

metrics1.confusionMatrix
res62: org.apache.spark.mllib.linalg.Matrix =
509.0  61.0
62.0   49.0