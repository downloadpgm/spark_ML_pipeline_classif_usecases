
val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("staging/ml_project1_data.csv")

val df1 = df.where("Income is not null").
withColumn("age",lit(2020)-col("Year_Birth")).
withColumn("enrolled_to_date",datediff(current_date(),col("Dt_Customer"))).
withColumn("label", 'Response).
drop("ID", "Year_Birth", "Dt_Customer","Z_CostContact","Z_Revenue")

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val types = df1.dtypes
types: Array[(String, String)] = Array((Education,StringType), (Marital_Status,StringType), (Income,IntegerType), (Kidhome,IntegerType), (Teenhome,IntegerType), (Recency,IntegerType), (MntWines,IntegerType), (MntFruits,IntegerType), (MntMeatProducts,IntegerType), (MntFishProducts,IntegerType), (MntSweetProducts,IntegerType), (MntGoldProds,IntegerType), (NumDealsPurchases,IntegerType), (NumWebPurchases,IntegerType), (NumCatalogPurchases,IntegerType), (NumStorePurchases,IntegerType), (NumWebVisitsMonth,IntegerType), (AcceptedCmp3,IntegerType), (AcceptedCmp4,IntegerType), (AcceptedCmp5,IntegerType), (AcceptedCmp1,IntegerType), (AcceptedCmp2,IntegerType), (Complain,IntegerType), (Z_CostContact,IntegerType), (Z_Revenue,IntegerType), (Response,IntegerType), (age,IntegerType), (enrolled_to_dat...

val df2 = df1.select(types.map{ case(c,t) =>  if (t == "IntegerType") col(c).cast(DoubleType) else col(c) }: _*)
df2: org.apache.spark.sql.DataFrame = [Education: string, Marital_Status: string ... 26 more fields]

df2.printSchema
root
 |-- Education: string (nullable = true)
 |-- Marital_Status: string (nullable = true)
 |-- Income: double (nullable = true)
 |-- Kidhome: double (nullable = true)
 |-- Teenhome: double (nullable = true)
 |-- Recency: double (nullable = true)
 |-- MntWines: double (nullable = true)
 |-- MntFruits: double (nullable = true)
 |-- MntMeatProducts: double (nullable = true)
 |-- MntFishProducts: double (nullable = true)
 |-- MntSweetProducts: double (nullable = true)
 |-- MntGoldProds: double (nullable = true)
 |-- NumDealsPurchases: double (nullable = true)
 |-- NumWebPurchases: double (nullable = true)
 |-- NumCatalogPurchases: double (nullable = true)
 |-- NumStorePurchases: double (nullable = true)
 |-- NumWebVisitsMonth: double (nullable = true)
 |-- AcceptedCmp3: double (nullable = true)
 |-- AcceptedCmp4: double (nullable = true)
 |-- AcceptedCmp5: double (nullable = true)
 |-- AcceptedCmp1: double (nullable = true)
 |-- AcceptedCmp2: double (nullable = true)
 |-- Complain: double (nullable = true)
 |-- Response: double (nullable = true)
 |-- age: double (nullable = true)
 |-- enrolled_to_date: double (nullable = true)
 |-- label: double (nullable = true)


// ----- logistic regression model hyperparameter tunning
 
val Array(trainingData, testData) = df2.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}

val dfInd1 = new StringIndexer().setInputCol("Education").setOutputCol("EducationCat").setHandleInvalid("skip")
val dfInd2 = new StringIndexer().setInputCol("Marital_Status").setOutputCol("Marital_StatusCat").setHandleInvalid("skip")

val dfOne1 = new OneHotEncoder().setInputCol("EducationCat").setOutputCol("EducationVect")
val dfOne2 = new OneHotEncoder().setInputCol("Marital_StatusCat").setOutputCol("Marital_StatusVect")

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("EducationVect","Marital_StatusVect","Income","Kidhome","Teenhome","Recency","MntWines","MntFruits","MntMeatProducts","MntFishProducts","MntSweetProducts","MntGoldProds","NumDealsPurchases","NumWebPurchases","NumCatalogPurchases","NumStorePurchases","NumWebVisitsMonth","AcceptedCmp1","AcceptedCmp2","AcceptedCmp3","AcceptedCmp4","AcceptedCmp5","Complain","age","enrolled_to_date"))

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression()
lr.setRegParam(0.01).setMaxIter(500).setFitIntercept(true).setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.Pipeline

val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfOne1,dfOne2,va,stdScaler,lr))


import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(1, 0.1, 0.01)).
addGrid(lr.fitIntercept).
addGrid(lr.maxIter, Array(10,20,40,100)).build()

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new BinaryClassificationEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val cvmodel = cv.fit(trainingData)

-- CV hyperparameter evaluation

cvmodel.getEstimatorParamMaps.zip(cvmodel.avgMetrics)
res15: Array[(org.apache.spark.ml.param.ParamMap, Double)] =
Array(({
        logreg_c2265d91d0bd-fitIntercept: true,
        logreg_c2265d91d0bd-maxIter: 10,
        logreg_c2265d91d0bd-regParam: 1.0
},0.8345473494893922), ({
        logreg_c2265d91d0bd-fitIntercept: true,
        logreg_c2265d91d0bd-maxIter: 10,
        logreg_c2265d91d0bd-regParam: 0.1
},0.8902543126010336), ({
        logreg_c2265d91d0bd-fitIntercept: true,
        logreg_c2265d91d0bd-maxIter: 10,
        logreg_c2265d91d0bd-regParam: 0.01
},0.8930276105563424), ({
        logreg_c2265d91d0bd-fitIntercept: true,
        logreg_c2265d91d0bd-maxIter: 20,
        logreg_c2265d91d0bd-regParam: 1.0
},0.8345473494893922), ({
        logreg_c2265d91d0bd-fitIntercept: true,
        logreg_c2265d91d0bd-maxIter: 20,
        logreg_c2265d91d0bd-regParam: 0.1
},0.8902543126010336), ({
        logreg_c2265d91d0bd-fitIntercept: true,
        logr...

-- extract best LR model 

import org.apache.spark.ml.PipelineModel
val bestmodel = cvmodel.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.classification.LogisticRegressionModel
val lrmodel = bestmodel.stages.last.asInstanceOf[LogisticRegressionModel]

lrmodel.getRegParam
res1: Double = 0.01

lrmodel.getMaxIter
res2: Int = 40

lrmodel.getThreshold
res3: Double = 0.5

lrmodel.getFitIntercept
res4: Boolean = true

lrmodel.getStandardization
res5: Boolean = true

-- collecting feature importance

println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
0.02884562166435312
0.3388248781972615
0.1097751916896525
-0.059853862215928304
-0.21984296281093038
-0.19876761149360686
0.25163933763617846
0.14667611078621948
0.1415250883069771
0.05076664476823373
0.027048395965663788
0.030929487784584507
0.13273367738032393
-0.38801533890759676
-0.7511142918307736
0.056383183489026455
0.025683982567519126
0.42207195089365057
0.009457689687682857
0.010365662309619562
0.14740708547656392
0.10049143490874639
0.2191685505749494
0.11966371066442175
-0.4976229685765491
0.3031717162156207
0.4140256811085109
0.18512345626292775
0.5206222540521228
0.2423905917491402
0.39958908348213507
0.012798164906430855
0.011176755160571162
0.5853848907460171

import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary

val trainingSummary = lrmodel.summary

trainingSummary.objectiveHistory
res22: Array[Double] = Array(0.4412277687383864, 0.4128795633260116, 0.31614441668146137, 0.29589482881206935, 0.28793313294850137, 0.27404058159749994, 0.2721056677940921, 0.270572223418258, 0.27052181575205375, 0.2704843189929246, 0.2704404063975789, 0.2704116127228022, 0.27040398369086727, 0.27040189716458896, 0.2704014808661235, 0.27040134015291273, 0.27040128892269716, 0.27040127209692344, 0.2704012676465076, 0.2704012663381451, 0.2704012652426234, 0.2704012649599443, 0.2704012647230106, 0.2704012645595825, 0.27040126447247703, 0.27040126442080026, 0.27040126441296153, 0.2704012644116769)

val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]

binarySummary.areaUnderROC
res22: Double = 0.9112649719664255

binarySummary.accuracy
res24: Double = 0.8975826972010178

-- collecting metric performance

val pred = bestmodel.transform(testData)

val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res24: Double = 0.8921985815602869

bceval.setMetricName("areaUnderPR").evaluate(pred)
res25: Double = 0.5684198143079666

val validPredicts = pred.select("label","prediction").rdd.map( row => (row.getDouble(0),row.getDouble(1)) )

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)

metrics.confusionMatrix
res27: org.apache.spark.mllib.linalg.Matrix =
541.0  45.0
23.0   35.0