---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("mobile/mobile_pricing_train.csv")

val rdd1 = rdd.filter( x => ! x.contains("battery_power")).map( x => x.split(","))

val rdd2 = rdd1.map( x => x.map( y => y.toString.toDouble))

rdd2.take(2)
res3: Array[Array[Double]] = Array(Array(842.0, 0.0, 2.2, 0.0, 1.0, 0.0, 7.0, 0.6, 188.0, 2.0, 2.0, 20.0, 756.0, 2549.0, 9.0, 7.0, 19.0, 0.0, 0.0, 1.0, 1.0), Array(1021.0, 1.0, 0.5, 1.0, 0.0, 1.0, 53.0, 0.7, 136.0, 3.0, 6.0, 905.0, 1988.0, 2631.0, 17.0, 3.0, 7.0, 1.0, 1.0, 0.0, 2.0))

			 
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = rdd2.map( x => {
  val arr_size = x.size - 1
  val l = x(arr_size)
  val f = Vectors.dense(x.slice(0, arr_size))
  LabeledPoint(l,f)
})

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

---- MLlib MultiClass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(4).run(trainSet)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res5: Array[(Double, Double)] = Array((2.0,2.0), (2.0,2.0), (0.0,0.0), (3.0,3.0), (2.0,3.0), (1.0,1.0), (3.0,3.0), (1.0,1.0), (0.0,0.0), (3.0,3.0), (1.0,2.0), (3.0,3.0), (1.0,1.0), (1.0,1.0), (3.0,3.0), (1.0,2.0), (2.0,2.0), (0.0,0.0), (3.0,3.0), (1.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 438
validPredicts.count                            // 579
val accuracy = metrics.accuracy   // 0.7564766839378239

metrics.confusionMatrix
res8: org.apache.spark.mllib.linalg.Matrix =
128.0  10.0   0.0   0.0
11.0   107.0  33.0  3.0
0.0    26.0   82.0  31.0
0.0    0.0    27.0  121.0

---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = data.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res31: org.apache.spark.mllib.linalg.Vector = [1998.0,1.0,3.0,1.0,19.0,1.0,64.0,1.0,200.0,8.0,20.0,1960.0,1998.0,3998.0,19.0,18.0,20.0,1.0,1.0,1.0]

matrixSummary.min
res32: org.apache.spark.mllib.linalg.Vector = [501.0,0.0,0.5,0.0,0.0,0.0,2.0,0.1,80.0,1.0,0.0,0.0,500.0,256.0,5.0,0.0,2.0,0.0,0.0,0.0]

matrixSummary.mean
res33: org.apache.spark.mllib.linalg.Vector = [1238.5185000000013,0.495,1.5222500000000012,0.5095,4.309500000000002,0.5215,32.04650000000006,0.5017500000000009,140.24900000000034,4.520500000000002,9.916499999999997,645.1080000000014,1251.515500000002,2124.2129999999984,12.306499999999978,5.76700000000001,11.011000000000008,0.7615,0.503,0.507]

matrixSummary.variance
res34: org.apache.spark.mllib.linalg.Vector = [193088.35983766877,0.2501000500250125,0.6658628689344657,0.25003476738369185,18.84813381690848,0.2496625812906453,329.2669712356179,0.08318352926463232,1253.1355667833914,5.234196848424216,36.775915707854,196941.40804002012,186796.36194072035,1176643.6064342165,17.75143346673334,18.978200100050042,29.854806403201607,0.18170860430215108,0.25011605802901454,0.2500760380190095]

---- Standardizing the features --------------

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, false).fit(trainSet.map(x => x.features))
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))
trainScaled.cache

---- MLlib Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(4).run(trainScaled)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

validPredicts.take(20)
res10: Array[(Double, Double)] = Array((3.0,2.0), (3.0,2.0), (0.0,0.0), (2.0,3.0), (3.0,3.0), (0.0,1.0), (3.0,3.0), (0.0,1.0), (0.0,0.0), (3.0,3.0), (3.0,2.0), (3.0,3.0), (1.0,1.0), (0.0,1.0), (3.0,3.0), (3.0,2.0), (3.0,2.0), (0.0,0.0), (3.0,3.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 343
validPredicts.count                            // 579
val accuracy = metrics.accuracy   // 0.5924006908462867

metrics.confusionMatrix
res12: org.apache.spark.mllib.linalg.Matrix =
138.0  0.0   0.0   0.0
129.0  24.0  1.0   0.0
0.0    1.0   35.0  103.0
0.0    0.0   2.0   146.0

---- MLlib Decision Tree regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int](1 -> 2, 3 -> 2, 5 -> 2, 17 -> 2, 18 -> 2, 19 -> 2)
categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map(5 -> 2, 1 -> 2, 17 -> 2, 3 -> 2, 18 -> 2, 19 -> 2)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, accuracy") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 4, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new MulticlassMetrics(validPredicts)
    println("%s, %d, %d ->  %d / %d, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.accuracy))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, accuracy
gini, 10, 32 ->  485 / 579, 0.8377
gini, 10, 48 ->  492 / 579, 0.8497
gini, 10, 64 ->  493 / 579, 0.8515
gini, 20, 32 ->  484 / 579, 0.8359
gini, 20, 48 ->  489 / 579, 0.8446
gini, 20, 64 ->  493 / 579, 0.8515
gini, 30, 32 ->  484 / 579, 0.8359
gini, 30, 48 ->  489 / 579, 0.8446
gini, 30, 64 ->  493 / 579, 0.8515
entropy, 10, 32 ->  502 / 579, 0.8670
entropy, 10, 48 ->  505 / 579, 0.8722 *
entropy, 10, 64 ->  498 / 579, 0.8601
entropy, 20, 32 ->  501 / 579, 0.8653
entropy, 20, 48 ->  502 / 579, 0.8670
entropy, 20, 64 ->  498 / 579, 0.8601
entropy, 30, 32 ->  501 / 579, 0.8653
entropy, 30, 48 ->  502 / 579, 0.8670
entropy, 30, 64 ->  498 / 579, 0.8601


val model = DecisionTree.trainClassifier(trainSet, 4, categoricalFeaturesInfo, "entropy", 10, 32)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res14: Array[(Double, Double)] = Array((2.0,2.0), (2.0,2.0), (0.0,0.0), (3.0,3.0), (3.0,3.0), (1.0,1.0), (3.0,3.0), (1.0,1.0), (0.0,0.0), (3.0,3.0), (2.0,2.0), (3.0,3.0), (1.0,1.0), (2.0,1.0), (3.0,3.0), (2.0,2.0), (2.0,2.0), (0.0,0.0), (3.0,3.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)

metrics.confusionMatrix
res16: org.apache.spark.mllib.linalg.Matrix =
132.0  6.0    0.0    0.0
7.0    124.0  23.0   0.0
0.0    14.0   109.0  16.0
0.0    0.0    11.0   137.0