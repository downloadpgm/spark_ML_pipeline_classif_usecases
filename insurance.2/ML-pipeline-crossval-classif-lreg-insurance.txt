---- Exploratory Data Analysis --------------

val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("insurance/train.csv")

df.printSchema
root
 |-- id: integer (nullable = true)
 |-- Gender: string (nullable = true)
 |-- Age: integer (nullable = true)
 |-- Driving_License: integer (nullable = true)
 |-- Region_Code: double (nullable = true)
 |-- Previously_Insured: integer (nullable = true)
 |-- Vehicle_Age: string (nullable = true)
 |-- Vehicle_Damage: string (nullable = true)
 |-- Annual_Premium: double (nullable = true)
 |-- Policy_Sales_Channel: double (nullable = true)
 |-- Vintage: integer (nullable = true)
 |-- Response: integer (nullable = true)

df.show(10)
+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+
| id|Gender|Age|Driving_License|Region_Code|Previously_Insured|Vehicle_Age|Vehicle_Damage|Annual_Premium|Policy_Sales_Channel|Vintage|Response|
+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+
|  1|  Male| 44|              1|       28.0|                 0|  > 2 Years|           Yes|       40454.0|                26.0|    217|       1|
|  2|  Male| 76|              1|        3.0|                 0|   1-2 Year|            No|       33536.0|                26.0|    183|       0|
|  3|  Male| 47|              1|       28.0|                 0|  > 2 Years|           Yes|       38294.0|                26.0|     27|       1|
|  4|  Male| 21|              1|       11.0|                 1|   < 1 Year|            No|       28619.0|               152.0|    203|       0|
|  5|Female| 29|              1|       41.0|                 1|   < 1 Year|            No|       27496.0|               152.0|     39|       0|
|  6|Female| 24|              1|       33.0|                 0|   < 1 Year|           Yes|        2630.0|               160.0|    176|       0|
|  7|  Male| 23|              1|       11.0|                 0|   < 1 Year|           Yes|       23367.0|               152.0|    249|       0|
|  8|Female| 56|              1|       28.0|                 0|   1-2 Year|           Yes|       32031.0|                26.0|     72|       1|
|  9|Female| 24|              1|        3.0|                 1|   < 1 Year|            No|       27619.0|               152.0|     28|       0|
| 10|Female| 32|              1|        6.0|                 1|   < 1 Year|            No|       28771.0|               152.0|     80|       0|
+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+
only showing top 10 rows

df.describe().show
+-------+------------------+------+------------------+--------------------+------------------+-------------------+-----------+--------------+------------------+--------------------+------------------+-------------------+
|summary|                id|Gender|               Age|     Driving_License|       Region_Code| Previously_Insured|Vehicle_Age|Vehicle_Damage|    Annual_Premium|Policy_Sales_Channel|           Vintage|           Response|
+-------+------------------+------+------------------+--------------------+------------------+-------------------+-----------+--------------+------------------+--------------------+------------------+-------------------+
|  count|            381109|381109|            381109|              381109|            381109|             381109|     381109|        381109|            381109|              381109|            381109|             381109|
|   mean|          190555.0|  null|38.822583565331705|  0.9978693759528114|26.388807401557035| 0.4582101183650871|       null|          null|30564.389581458323|  112.03429465061177|154.34739667654136|0.12256336113815208|
| stddev|110016.83620776715|  null| 15.51161101809548|0.046109544207799495| 13.22988802578841|0.49825119888722824|       null|          null|17213.155056979947|   54.20399477485719| 83.67130362658658|0.32793576478642744|
|    min|                 1|Female|                20|                   0|               0.0|                  0|   1-2 Year|            No|            2630.0|                 1.0|                10|                  0|
|    max|            381109|  Male|                85|                   1|              52.0|                  1|  > 2 Years|           Yes|          540165.0|               163.0|               299|                  1|
+-------+------------------+------+------------------+--------------------+------------------+-------------------+-----------+--------------+------------------+--------------------+------------------+-------------------+

df.groupBy('Gender).count.show
+------+------+
|Gender| count|
+------+------+
|Female|175020|
|  Male|206089|
+------+------+

df.groupBy('Driving_License).count.show  // removed from analysis due to predominantly "1"
+---------------+------+
|Driving_License| count|
+---------------+------+
|              1|380297|
|              0|   812|
+---------------+------+

df.groupBy('Previously_Insured).count.show
+------------------+------+
|Previously_Insured| count|
+------------------+------+
|                 1|174628|
|                 0|206481|
+------------------+------+

df.groupBy('Vehicle_Age).count.show
+-----------+------+
|Vehicle_Age| count|
+-----------+------+
|  > 2 Years| 16007|
|   < 1 Year|164786|
|   1-2 Year|200316|
+-----------+------+

df.groupBy('Vehicle_Damage).count.show
+--------------+------+
|Vehicle_Damage| count|
+--------------+------+
|            No|188696|
|           Yes|192413|
+--------------+------+

df.groupBy('Response).count.show
+--------+------+
|Response| count|
+--------+------+
|       1| 46710|
|       0|334399|
+--------+------+


df.select("Gender","Age","Previously_Insured","Vehicle_Age","Vehicle_Damage","Vintage","Response").show(10)
+------+---+------------------+-----------+--------------+-------+--------+
|Gender|Age|Previously_Insured|Vehicle_Age|Vehicle_Damage|Vintage|Response|
+------+---+------------------+-----------+--------------+-------+--------+
|  Male| 44|                 0|  > 2 Years|           Yes|    217|       1|
|  Male| 76|                 0|   1-2 Year|            No|    183|       0|
|  Male| 47|                 0|  > 2 Years|           Yes|     27|       1|
|  Male| 21|                 1|   < 1 Year|            No|    203|       0|
|Female| 29|                 1|   < 1 Year|            No|     39|       0|
|Female| 24|                 0|   < 1 Year|           Yes|    176|       0|
|  Male| 23|                 0|   < 1 Year|           Yes|    249|       0|
|Female| 56|                 0|   1-2 Year|           Yes|     72|       1|
|Female| 24|                 1|   < 1 Year|            No|     28|       0|
|Female| 32|                 1|   < 1 Year|            No|     80|       0|
+------+---+------------------+-----------+--------------+-------+--------+
only showing top 10 rows


import org.apache.spark.sql.types._

val df1 = df.select("Gender","Age","Previously_Insured","Vehicle_Age","Vehicle_Damage","Vintage","Response").
             withColumn("label", df.col("Response").cast(DoubleType))

df1.describe().show
+-------+------+------------------+-------------------+-----------+--------------+------------------+-------------------+-------------------+
|summary|Gender|               Age| Previously_Insured|Vehicle_Age|Vehicle_Damage|           Vintage|           Response|              label|
+-------+------+------------------+-------------------+-----------+--------------+------------------+-------------------+-------------------+
|  count|381109|            381109|             381109|     381109|        381109|            381109|             381109|             381109|
|   mean|  null|38.822583565331705| 0.4582101183650871|       null|          null|154.34739667654136|0.12256336113815208|0.12256336113815208|
| stddev|  null| 15.51161101809548|0.49825119888722824|       null|          null| 83.67130362658658|0.32793576478642744|0.32793576478642744|
|    min|Female|                20|                  0|   1-2 Year|            No|                10|                  0|                0.0|
|    max|  Male|                85|                  1|  > 2 Years|           Yes|               299|                  1|                1.0|
+-------+------+------------------+-------------------+-----------+--------------+------------------+-------------------+-------------------+


---- Feature extraction & Data Munging --------------

import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}

val dfInd1 = new StringIndexer().setInputCol("Gender").setOutputCol("GenderIdx")
val dfInd2 = new StringIndexer().setInputCol("Vehicle_Age").setOutputCol("Vehicle_AgeIdx")
val dfInd3 = new StringIndexer().setInputCol("Vehicle_Damage").setOutputCol("Vehicle_DamageIdx")

val dfOne2 = new OneHotEncoder().setInputCol("Vehicle_AgeIdx").setOutputCol("Vehicle_AgeVect")

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("GenderIdx","Previously_Insured","Vehicle_AgeVect","Vehicle_DamageIdx","Age","Vintage"))

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfOne2,va))

val df2 = pipeline.fit(df1).transform(df1)

df2.printSchema
root
 |-- Gender: string (nullable = true)
 |-- Age: integer (nullable = true)
 |-- Previously_Insured: integer (nullable = true)
 |-- Vehicle_Age: string (nullable = true)
 |-- Vehicle_Damage: string (nullable = true)
 |-- Vintage: integer (nullable = true)
 |-- Response: integer (nullable = true)
 |-- label: double (nullable = true)
 |-- GenderIdx: double (nullable = false)
 |-- Vehicle_AgeIdx: double (nullable = false)
 |-- Vehicle_DamageIdx: double (nullable = false)
 |-- Vehicle_AgeVect: vector (nullable = true)
 |-- features: vector (nullable = true)

df2.show(10)
+------+---+------------------+-----------+--------------+-------+--------+-----+---------+--------------+-----------------+---------------+--------------------+
|Gender|Age|Previously_Insured|Vehicle_Age|Vehicle_Damage|Vintage|Response|label|GenderIdx|Vehicle_AgeIdx|Vehicle_DamageIdx|Vehicle_AgeVect|            features|
+------+---+------------------+-----------+--------------+-------+--------+-----+---------+--------------+-----------------+---------------+--------------------+
|  Male| 44|                 0|  > 2 Years|           Yes|    217|       1|  1.0|      0.0|           2.0|              0.0|      (2,[],[])|(7,[5,6],[44.0,21...|
|  Male| 76|                 0|   1-2 Year|            No|    183|       0|  0.0|      0.0|           0.0|              1.0|  (2,[0],[1.0])|[0.0,1.0,0.0,1.0,...|
|  Male| 47|                 0|  > 2 Years|           Yes|     27|       1|  1.0|      0.0|           2.0|              0.0|      (2,[],[])|(7,[5,6],[47.0,27...|
|  Male| 21|                 1|   < 1 Year|            No|    203|       0|  0.0|      0.0|           1.0|              1.0|  (2,[1],[1.0])|[0.0,0.0,1.0,1.0,...|
|Female| 29|                 1|   < 1 Year|            No|     39|       0|  0.0|      1.0|           1.0|              1.0|  (2,[1],[1.0])|[1.0,0.0,1.0,1.0,...|
|Female| 24|                 0|   < 1 Year|           Yes|    176|       0|  0.0|      1.0|           1.0|              0.0|  (2,[1],[1.0])|[1.0,0.0,1.0,0.0,...|
|  Male| 23|                 0|   < 1 Year|           Yes|    249|       0|  0.0|      0.0|           1.0|              0.0|  (2,[1],[1.0])|(7,[2,5,6],[1.0,2...|
|Female| 56|                 0|   1-2 Year|           Yes|     72|       1|  1.0|      1.0|           0.0|              0.0|  (2,[0],[1.0])|[1.0,1.0,0.0,0.0,...|
|Female| 24|                 1|   < 1 Year|            No|     28|       0|  0.0|      1.0|           1.0|              1.0|  (2,[1],[1.0])|[1.0,0.0,1.0,1.0,...|
|Female| 32|                 1|   < 1 Year|            No|     80|       0|  0.0|      1.0|           1.0|              1.0|  (2,[1],[1.0])|[1.0,0.0,1.0,1.0,...|
+------+---+------------------+-----------+--------------+-------+--------+-----+---------+--------------+-----------------+---------------+--------------------+
only showing top 10 rows


// calculate pearson correlation to check multicolinearity

import org.apache.spark.ml.stat.Correlation
import org.apache.spark.ml.linalg.Matrix
import org.apache.spark.sql.Row

val corr = Correlation.corr(df2, "features", "pearson").head match {
   case Row(coeff: Matrix) => coeff
}
corr: org.apache.spark.ml.linalg.Matrix =
1.0                    -0.14763318901465458    ... (7 total)
-0.14763318901465458   1.0                     ...
0.1662796237533659     -0.9187044529428631     ...
0.09160591787308084    -0.284716998344344      ...
0.08193215942681609    -0.2790765023157333     ...
-0.1455450343090122    0.6929102320552705      ...
0.0025168965793046327  -0.0026323646297304105  ...


corr.toDense.rowIter.foreach( x => {
  val size = x.size
  for ( i <- Range(0,size)) { 
    val elem = x(i)
    print(f"$elem%.3f\t") 
  }
  println
})
// "GenderIdx","Vehicle_AgeVect","Vehicle_DamageIdx","Previously_Insured","Age","Vintage"
1.000   -0.148  0.166   0.092   0.082   -0.146  0.003
-0.148  1.000   -0.919  -0.285  -0.279  0.693   -0.003
0.166   -0.919  1.000   0.371   0.359   -0.788  0.002
0.092   -0.285  0.371   1.000   0.824   -0.268  0.002
0.082   -0.279  0.359   0.824   1.000   -0.255  0.003
-0.146  0.693   -0.788  -0.268  -0.255  1.000   -0.001
0.003   -0.003  0.002   0.002   0.003   -0.001  1.000

// Previously_Insured x Vehicle_DamageIdx = 0.824 evidence of multicolinearity


// ----- building the logistic regression model

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(100).setFitIntercept(true).setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfOne2,va,stdScaler,lr))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

val model = pipeline.fit(trainingData)

-- collecting feature importance

import org.apache.spark.ml.classification.LogisticRegressionModel

val lrmodel = model.stages.last.asInstanceOf[LogisticRegressionModel]

val featureImp = va.getInputCols.zip(lrmodel.coefficients.toArray)

val columns = Array("feature", "Importance")
val featureImpDF = spark.createDataFrame(featureImp).toDF(columns: _*)

featureImpDF.orderBy($"Importance".desc).show()
+------------------+--------------------+
|           feature|          Importance|
+------------------+--------------------+
|   Vehicle_AgeVect|0.023106591961439268|
|         GenderIdx|-0.04158566782003466|
|           Vintage|-0.19969071075167913|
| Vehicle_DamageIdx|  -0.457489538486766|
|               Age| -0.8311999890611236|
|Previously_Insured| -0.8688144767274414|
+------------------+--------------------+

// -----  metrics extracted from model

import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary

val trainingSummary = lrmodel.summary

trainingSummary.objectiveHistory
res9: Array[Double] = Array(0.3708846226233835, 0.33685430214030465, 0.30227544437873005, 0.298320219708928, 0.2896949257733661, 0.28804458032757, 0.28775879283092826, 0.2877324176315952, 0.2877305733436464, 0.2877297600713503, 0.2877296412298722, 0.2877295232400794, 0.2877295218682795, 0.28772952181166794, 0.28772952180114564, 0.2877295217984978, 0.2877295217949088, 0.28772952179227873)

val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]

binarySummary.areaUnderROC
res10: Double = 0.8317853026115388

binarySummary.accuracy
res11: Double = 0.8780032324474727


// -----  metrics on test data

val pred = model.transform(testData)

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res12: Double = 0.8302504393136768

bceval.setMetricName("areaUnderPR").evaluate(pred)
res13: Double = 0.3100637714079966

val predRDD = pred.select("prediction","label").rdd.map( row => (row.getDouble(0),row.getDouble(1))).cache

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

metrics.accuracy
res16: Double = 0.8761163249969416

metrics.confusionMatrix
res15: org.apache.spark.mllib.linalg.Matrix =
100261.0  0.0
14177.0   0.0

// ----- logistic regression model hyperparameter tunning

import org.apache.spark.ml.classification.LogisticRegression

val lr = new LogisticRegression

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(1, 0.1, 0.01, 0.001)).
addGrid(lr.fitIntercept).
addGrid(lr.maxIter, Array(10, 20, 40, 100)).build()

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val cv = new CrossValidator().
setEstimator(lr).
setEvaluator(new BinaryClassificationEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfOne2,va,stdScaler,cv))
val pipelinemodel = pipeline.fit(trainingData)

import org.apache.spark.ml.tuning.CrossValidatorModel
val cvmodel = pipelinemodel.stages.last.asInstanceOf[CrossValidatorModel]

-- CV hyperparameter evaluation

cvmodel.getEstimatorParamMaps.zip(cvmodel.avgMetrics)
Array(({
        logreg_27c496ace2b1-fitIntercept: true,
        logreg_27c496ace2b1-maxIter: 10,
        logreg_27c496ace2b1-regParam: 1.0
},0.7955118374535833), ({
        logreg_27c496ace2b1-fitIntercept: true,
        logreg_27c496ace2b1-maxIter: 20,
        logreg_27c496ace2b1-regParam: 1.0
},0.7954005271757213), ({
        logreg_27c496ace2b1-fitIntercept: true,
        logreg_27c496ace2b1-maxIter: 40,
        logreg_27c496ace2b1-regParam: 1.0
},0.7954005271757213), ({
        logreg_27c496ace2b1-fitIntercept: true,
        logreg_27c496ace2b1-maxIter: 100,
        logreg_27c496ace2b1-regParam: 1.0
},0.7954005271757213), ({
        logreg_27c496ace2b1-fitIntercept: false,
        logreg_27c496ace2b1-maxIter: 10,
        logreg_27c496ace2b1-regParam: 1.0
},0.7807481690992496), ({
        logreg_27c496ace2b1-fitIntercept: false,
        lo...

-- extract best LR model 

import org.apache.spark.ml.classification.LogisticRegressionModel
val lrmodel = cvmodel.bestModel.asInstanceOf[LogisticRegressionModel]

lrmodel.getRegParam
res18: Double = 0.001

lrmodel.getMaxIter
res19: Int = 20

lrmodel.getThreshold
res20: Double = 0.5

lrmodel.getFitIntercept
res21: Boolean = true


-- collecting feature importance

val featureImp = va.getInputCols.zip(lrmodel.coefficients.toArray)

val columns = Array("feature", "Importance")
val featureImpDF = spark.createDataFrame(featureImp).toDF(columns: _*)

featureImpDF.orderBy($"Importance".desc).show()
+------------------+--------------------+
|           feature|          Importance|
+------------------+--------------------+
|           Vintage| -0.0208215833288414|
|         GenderIdx|-0.08796767948011633|
|   Vehicle_AgeVect|-0.17125731567159422|
| Vehicle_DamageIdx|  -1.381189395073796|
|               Age|  -2.029771252312052|
|Previously_Insured| -2.9362784665081922|
+------------------+--------------------+


import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary

val trainingSummary = lrmodel.summary

trainingSummary.objectiveHistory
res23: Array[Double] = Array(0.3708846226233835, 0.2961173280863467, 0.29391827708070684, 0.29209736867850306, 0.2839053747256762, 0.2828173599616276, 0.2818193941585891, 0.2811563132670453, 0.2806811251305354, 0.27941988851761546, 0.27802118417441674, 0.2778109900855152, 0.2774251450946239, 0.27740787833600455, 0.27734641800864474, 0.2773252425021461, 0.2772497716265444, 0.2772175428585191, 0.2771567628111116, 0.27712182615825354, 0.27711763357859964)

val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]

binarySummary.areaUnderROC
res24: Double = 0.8332566420801288

binarySummary.accuracy
res25: Double = 0.8780032324474727

-- collecting metric performance

val pred = pipelinemodel.transform(testData)

val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res26: Double = 0.8316437801928718

bceval.setMetricName("areaUnderPR").evaluate(pred)
res27: Double = 0.312444305972456

val validPredicts = pred.select("prediction","label").rdd.map( row => (row.getDouble(0),row.getDouble(1))).cache

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)

metrics.accuracy
res28: Double = 0.8761163249969416

metrics.confusionMatrix
res29: org.apache.spark.mllib.linalg.Matrix =
100261.0  0.0
14177.0   0.0
