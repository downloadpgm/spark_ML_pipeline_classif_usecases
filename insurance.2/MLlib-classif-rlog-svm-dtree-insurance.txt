---- Feature extraction & Data Munging --------------

val rdd1 = sc.textFile("insurance/train.csv").map( x => x.split(","))

rdd1.first
res0: Array[String] = Array(id, Gender, Age, Driving_License, Region_Code, Previously_Insured, Vehicle_Age, Vehicle_Damage, Annual_Premium, Policy_Sales_Channel, Vintage, Response)

val hdr1 = rdd1.take(1)
hdr: Array[Array[String]] = Array(Array(id, Gender, Age, Driving_License, Region_Code, Previously_Insured, Vehicle_Age, Vehicle_Damage, Annual_Premium, Policy_Sales_Channel, Vintage, Response))

val rdd = rdd1.filter( x => x(3) != "Driving_License")

rdd.take(10).map( x => x.mkString(",")).foreach(println)
1,Male,44,1,28.0,0,> 2 Years,Yes,40454.0,26.0,217,1
2,Male,76,1,3.0,0,1-2 Year,No,33536.0,26.0,183,0
3,Male,47,1,28.0,0,> 2 Years,Yes,38294.0,26.0,27,1
4,Male,21,1,11.0,1,< 1 Year,No,28619.0,152.0,203,0
5,Female,29,1,41.0,1,< 1 Year,No,27496.0,152.0,39,0
6,Female,24,1,33.0,0,< 1 Year,Yes,2630.0,160.0,176,0
7,Male,23,1,11.0,0,< 1 Year,Yes,23367.0,152.0,249,0
8,Female,56,1,28.0,0,1-2 Year,Yes,32031.0,26.0,72,1
9,Female,24,1,3.0,1,< 1 Year,No,27619.0,152.0,28,0
10,Female,32,1,6.0,1,< 1 Year,No,28771.0,152.0,80,0

// Gender
rdd.map( x => (x(1),1)).reduceByKey( _+_ ).take(10)
res10: Array[(String, Int)] = Array((Male,206089), (Female,175020))

// Driving_License
rdd.map( x => (x(3),1)).reduceByKey( _+_ ).take(10)
res11: Array[(String, Int)] = Array((0,812), (1,380297))

// Region_Code
rdd.map( x => x(4)).distinct.count
res13: Long = 53

rdd.map( x => (x(4),1)).reduceByKey( _+_ ).take(10)
res12: Array[(String, Int)] = Array((49.0,1832), (37.0,5501), (16.0,2007), (7.0,3279), (8.0,33877), (42.0,591), (46.0,19749), (12.0,3198), (47.0,7436), (26.0,2587))

// Previously_Insured
rdd.map( x => (x(5),1)).reduceByKey( _+_ ).take(10)
res14: Array[(String, Int)] = Array((0,206481), (1,174628))

// Vehicle_Age
rdd.map( x => (x(6),1)).reduceByKey( _+_ ).take(10)
res15: Array[(String, Int)] = Array((< 1 Year,164786), (1-2 Year,200316), (> 2 Years,16007))

// Vehicle_Damage
rdd.map( x => (x(7),1)).reduceByKey( _+_ ).take(10)
res16: Array[(String, Int)] = Array((No,188696), (Yes,192413))

// Policy_Sales_Channel
rdd.map( x => x(9)).distinct.count
res17: Long = 155

// Gender, Previously_Insured, Vehicle_Age, Vehicle_Damage, Age, Vintage, Response
val rdd = rdd1.filter( x => x(3) != "Driving_License").map( x => Array(x(1),x(5),x(6),x(7),x(2),x(10),x(11)))

rdd.take(10).map( x => x.mkString(",")).foreach(println)
Male,0,> 2 Years,Yes,44,217,1
Male,0,1-2 Year,No,76,183,0
Male,0,> 2 Years,Yes,47,27,1
Male,1,< 1 Year,No,21,203,0
Female,1,< 1 Year,No,29,39,0
Female,0,< 1 Year,Yes,24,176,0
Male,0,< 1 Year,Yes,23,249,0
Female,0,1-2 Year,Yes,56,72,1
Female,1,< 1 Year,No,24,28,0
Female,1,< 1 Year,No,32,80,0

val hdr = hdr1.map( x => Array(x(1),x(5),x(6),x(7),x(2),x(10),x(11))).take(1)(0)

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  print(hdr(idx) + " : ")
  println(categories)
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
	  if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,0,1,2,3)
Gender : Map(Female -> 0, Male -> 1)
Previously_Insured : Map(0 -> 0, 1 -> 1)
Vehicle_Age : Map(< 1 Year -> 0, > 2 Years -> 1, 1-2 Year -> 2)
Vehicle_Damage : Map(No -> 0, Yes -> 1)
concat: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[65] at map at <console>:30

concat.take(5)
res7: Array[Array[Double]] = Array(Array(1.0, 0.0, 0.0, 1.0, 0.0, 1.0), Array(1.0, 0.0, 0.0, 0.0, 1.0, 0.0), Array(1.0, 0.0, 0.0, 1.0, 0.0, 1.0), Array(1.0, 1.0, 1.0, 0.0, 0.0, 0.0), Array(0.0, 1.0, 1.0, 0.0, 0.0, 0.0))

val rdd2 = rdd.map( x => x.slice(4,x.size)).map( x => x.map( y => y.toDouble))

rdd2.take(5)
res8: Array[Array[Double]] = Array(Array(44.0, 217.0, 1.0), Array(76.0, 183.0, 0.0), Array(47.0, 27.0, 1.0), Array(21.0, 203.0, 0.0), Array(29.0, 39.0, 0.0))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(10).map( x => x.mkString(",")).foreach(println)
1.0,0.0,0.0,1.0,0.0,1.0,44.0,217.0,1.0
1.0,0.0,0.0,0.0,1.0,0.0,76.0,183.0,0.0
1.0,0.0,0.0,1.0,0.0,1.0,47.0,27.0,1.0
1.0,1.0,1.0,0.0,0.0,0.0,21.0,203.0,0.0
0.0,1.0,1.0,0.0,0.0,0.0,29.0,39.0,0.0
0.0,0.0,1.0,0.0,0.0,1.0,24.0,176.0,0.0
1.0,0.0,1.0,0.0,0.0,1.0,23.0,249.0,0.0
0.0,0.0,0.0,0.0,1.0,1.0,56.0,72.0,1.0
0.0,1.0,1.0,0.0,0.0,0.0,24.0,28.0,0.0
0.0,1.0,1.0,0.0,0.0,0.0,32.0,80.0,0.0


import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)
   val f = x.slice(0, arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache
testSet.cache

---- MLlib logistic regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(100).setRegParam(0.01)
val lr = model.run(trainSet)

val validPredicts = testSet.map(x => (lr.predict(x.features),x.label))
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res9: Double = 0.12155356753218416

metrics.areaUnderROC
res15: Double = 0.5

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res12: Double = 0.8784464324678158

metrics1.confusionMatrix
res11: org.apache.spark.mllib.linalg.Matrix =
100648.0  0.0
13927.0   0.0

----- Estimation is not so good. But analyze the individual statistics and standardize 

import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics

matrixSummary.max.toArray.foreach( x => print(f"$x%.2f\t"))
1.00    1.00    1.00    1.00    1.00    1.00    85.00   299.00

matrixSummary.min.toArray.foreach( x => print(f"$x%.2f\t"))
0.00    0.00    0.00    0.00    0.00    0.00    20.00   10.00

matrixSummary.mean.toArray.foreach( x => print(f"$x%.2f\t"))
0.54    0.46    0.43    0.04    0.53    0.50    38.81   154.27

matrixSummary.variance.toArray.foreach( x => print(f"$x%.2f\t"))
0.25    0.25    0.25    0.04    0.25    0.25    240.45  6997.90

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache
testScaled.cache

---- MLlib logistic regression STD --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(100).setRegParam(0.01)
val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res16: Double = 0.2312018812896256

metrics.areaUnderROC
res17: Double = 0.767444620594742

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res18: Double = 0.6008815186559022

metrics1.confusionMatrix
res19: org.apache.spark.mllib.linalg.Matrix =
55093.0  45555.0
174.0    13753.0

---- Hyperparameter tunning with logistic regression STD --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], regSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step, reg  -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes; reg <- regSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step).setRegParam(reg)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, reg, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(10, 20, 40, 100),Array(1, 0.1, 0.01, 0.001),Array(0.1, 0.01), trainScaled, testScaled)
iter, step, reg  -> pred / total, AuPR, AuROC
10, 1.000, 0.100 -> 67547 / 114575, 0.2265, 0.7615
10, 1.000, 0.010 -> 67547 / 114575, 0.2265, 0.7615
10, 0.100, 0.100 -> 67495 / 114575, 0.2263, 0.7613
10, 0.100, 0.010 -> 67495 / 114575, 0.2263, 0.7613
10, 0.010, 0.100 -> 67495 / 114575, 0.2263, 0.7613
10, 0.010, 0.010 -> 67495 / 114575, 0.2263, 0.7613
10, 0.001, 0.100 -> 67495 / 114575, 0.2263, 0.7613
10, 0.001, 0.010 -> 67495 / 114575, 0.2263, 0.7613
20, 1.000, 0.100 -> 68232 / 114575, 0.2290, 0.7647
20, 1.000, 0.010 -> 68232 / 114575, 0.2290, 0.7647
20, 0.100, 0.100 -> 67495 / 114575, 0.2263, 0.7613
20, 0.100, 0.010 -> 67495 / 114575, 0.2263, 0.7613
20, 0.010, 0.100 -> 67495 / 114575, 0.2263, 0.7613
20, 0.010, 0.010 -> 67495 / 114575, 0.2263, 0.7613
20, 0.001, 0.100 -> 67495 / 114575, 0.2263, 0.7613
20, 0.001, 0.010 -> 67495 / 114575, 0.2263, 0.7613
40, 1.000, 0.100 -> 68767 / 114575, 0.2309, 0.7671
40, 1.000, 0.010 -> 68767 / 114575, 0.2309, 0.7671
40, 0.100, 0.100 -> 67495 / 114575, 0.2263, 0.7613
40, 0.100, 0.010 -> 67495 / 114575, 0.2263, 0.7613
40, 0.010, 0.100 -> 67495 / 114575, 0.2263, 0.7613
40, 0.010, 0.010 -> 67495 / 114575, 0.2263, 0.7613
40, 0.001, 0.100 -> 67495 / 114575, 0.2263, 0.7613
40, 0.001, 0.010 -> 67495 / 114575, 0.2263, 0.7613
100, 1.000, 0.100 -> 68846 / 114575, 0.2312, 0.7674 *
100, 1.000, 0.010 -> 68846 / 114575, 0.2312, 0.7674
100, 0.100, 0.100 -> 67495 / 114575, 0.2263, 0.7613
100, 0.100, 0.010 -> 67495 / 114575, 0.2263, 0.7613
100, 0.010, 0.100 -> 67495 / 114575, 0.2263, 0.7613
100, 0.010, 0.010 -> 67495 / 114575, 0.2263, 0.7613
100, 0.001, 0.100 -> 67495 / 114575, 0.2263, 0.7613
100, 0.001, 0.010 -> 67495 / 114575, 0.2263, 0.7613

val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(100).setStepSize(1.0).setRegParam(0.1)

val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))

validPredicts.take(20)
res22: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res25: Double = 0.22694108937878488

metrics.areaUnderROC
res26: Double = 0.7620901950635248

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res27: Double = 0.5907135064368317

metrics1.confusionMatrix
res28: org.apache.spark.mllib.linalg.Matrix =
53914.0  46734.0
160.0    13767.0

----- Hyperparameter tunning with SVM regression STD ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(10, 20, 40, 100), trainScaled, testScaled)
iter -> pred / total, AuPR, AuROC
10 -> 70753 / 114575, 0.2372, 0.7730
20 -> 69985 / 114575, 0.2337, 0.7683 *
40 -> 69796 / 114575, 0.2329, 0.7674
100 -> 69796 / 114575, 0.2329, 0.7674

val model = SVMWithSGD.train(trainScaled, 20)

val validPredicts = testScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res30: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res33: Double = 0.22694108937878488

metrics.areaUnderROC
res34: Double = 0.7620901950635248

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res35: Double = 0.610822605280384

metrics1.confusionMatrix
res36: org.apache.spark.mllib.linalg.Matrix =
56387.0  44261.0
329.0    13598.0


----- MLlib DecisionTree regression --------------

val categ_gender = rdd.map( x => x(0)).distinct.zipWithIndex.collectAsMap
categ_gender: scala.collection.Map[String,Long] = Map(Male -> 1, Female -> 0)

val categ_veh_age = rdd.map( x => x(2)).distinct.zipWithIndex.collectAsMap
categ_veh_age: scala.collection.Map[String,Long] = Map(> 2 Years -> 1, < 1 Year -> 0, 1-2 Year -> 2)

val categ_veh_damaged = rdd.map( x => x(3)).distinct.zipWithIndex.collectAsMap
categ_veh_damaged: scala.collection.Map[String,Long] = Map(No -> 0, Yes -> 1)

val rdd2_dt = rdd.map( x => Array(categ_gender(x(0)),x(1),categ_veh_age(x(2)),categ_veh_damaged(x(3)),x(4),x(5),x(6))).
                  map( x => x.map( y => y.toString.toDouble ))
				  
rdd2_dt.take(10).map( x => x.mkString(",")).foreach(println)
1.0,0.0,1.0,1.0,44.0,217.0,1.0
1.0,0.0,2.0,0.0,76.0,183.0,0.0
1.0,0.0,1.0,1.0,47.0,27.0,1.0
1.0,1.0,0.0,0.0,21.0,203.0,0.0
0.0,1.0,0.0,0.0,29.0,39.0,0.0
0.0,0.0,0.0,1.0,24.0,176.0,0.0
1.0,0.0,0.0,1.0,23.0,249.0,0.0
0.0,0.0,2.0,1.0,56.0,72.0,1.0
0.0,1.0,0.0,0.0,24.0,28.0,0.0
0.0,1.0,0.0,0.0,32.0,80.0,0.0


val data = rdd2_dt.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)
   val f = x.slice(0, arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })
 
val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache
testSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int](0->2, 1->2, 2->3, 3->2)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, AuPR, AuROC") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AuPR, AuROC
gini, 10, 32 -> 100634 / 114575, 0.1858, 0.5001
gini, 10, 48 -> 100636 / 114575, 0.2108, 0.5002
gini, 10, 64 -> 100631 / 114575, 0.2276, 0.5004
gini, 20, 32 -> 100341 / 114575, 0.2418, 0.5102
gini, 20, 48 -> 99928 / 114575, 0.2212, 0.5150
gini, 20, 64 -> 99727 / 114575, 0.2118, 0.5157
gini, 30, 32 -> 100324 / 114575, 0.2437, 0.5111
gini, 30, 48 -> 99869 / 114575, 0.2237, 0.5169
gini, 30, 64 -> 99474 / 114575, 0.2129, 0.5201 *
entropy, 10, 32 -> 100626 / 114575, 0.2423, 0.5008
entropy, 10, 48 -> 100637 / 114575, 0.2674, 0.5007
entropy, 10, 64 -> 100640 / 114575, 0.2766, 0.5007
entropy, 20, 32 -> 100330 / 114575, 0.2382, 0.5098
entropy, 20, 48 -> 99963 / 114575, 0.2185, 0.5136
entropy, 20, 64 -> 99797 / 114575, 0.2150, 0.5156
entropy, 30, 32 -> 100309 / 114575, 0.2411, 0.5110
entropy, 30, 48 -> 99864 / 114575, 0.2237, 0.5170
entropy, 30, 64 -> 99476 / 114575, 0.2134, 0.5202


val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "gini", 30, 64)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res42: Array[(Double, Double)] = Array((0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res46: Double = 0.2129006532096614

metrics.areaUnderROC
res47: Double = 0.5200592809559803

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)
metrics1: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@5f986fcb

metrics1.accuracy
res48: Double = 0.8681998690813877

metrics1.confusionMatrix
res49: org.apache.spark.mllib.linalg.Matrix =
98637.0  2011.0
13090.0  837.0