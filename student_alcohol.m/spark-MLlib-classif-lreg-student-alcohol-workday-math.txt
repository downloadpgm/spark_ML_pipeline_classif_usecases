---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("student_alcohol/student-mat.csv").map(x => x.split(","))

rdd.first
res0: Array[String] = Array(GP, F, 18, U, GT3, A, 4, 4, at_home, teacher, course, mother, 2, 2, 0, yes, no, no, no, yes, yes, no, no, 4, 3, 4, 1, 1, 3, 6, 5, 6, 6)

val categ_yesno = rdd.map(x => x(15)).distinct.zipWithIndex.collectAsMap
categ_yesno: scala.collection.Map[String,Long] = Map(no -> 1, yes -> 0)

val categ_sex = rdd.map(x => x(1)).distinct.zipWithIndex.collectAsMap
categ_sex: scala.collection.Map[String,Long] = Map(M -> 1, F -> 0)

val categ_address = rdd.map(x => x(3)).distinct.zipWithIndex.collectAsMap
categ_address: scala.collection.Map[String,Long] = Map(R -> 0, U -> 1)

val categ_famsize = rdd.map(x => x(4)).distinct.zipWithIndex.collectAsMap
categ_famsize: scala.collection.Map[String,Long] = Map(LE3 -> 0, GT3 -> 1)

val categ_pstatus = rdd.map(x => x(5)).distinct.zipWithIndex.collectAsMap
categ_pstatus: scala.collection.Map[String,Long] = Map(A -> 1, T -> 0)

val categ_mjob = rdd.map(x => x(8)).distinct.zipWithIndex.collectAsMap
categ_mjob: scala.collection.Map[String,Long] = Map(at_home -> 4, services -> 2, health -> 0, teacher -> 1, other -> 3)

val categ_fjob = rdd.map(x => x(9)).distinct.zipWithIndex.collectAsMap
categ_fjob: scala.collection.Map[String,Long] = Map(at_home -> 4, services -> 2, health -> 0, teacher -> 1, other -> 3)

val categ_reason = rdd.map(x => x(10)).distinct.zipWithIndex.collectAsMap
categ_reason: scala.collection.Map[String,Long] = Map(reputation -> 3, course -> 2, home -> 1, other -> 0)

val categ_guardian = rdd.map(x => x(11)).distinct.zipWithIndex.collectAsMap
categ_guardian: scala.collection.Map[String,Long] = Map(mother -> 2, father -> 0, other -> 1)

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
	val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
	if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
    categoryFeatures
	})
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,8,9,10,11)

concat.first
res3: Array[Double] = Array(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0)

val rdd1 = rdd.map( x => { 
  val y = Array(categ_sex(x(1)),x(2),categ_address(x(3)),categ_famsize(x(4)),categ_pstatus(x(5)),x(6),x(7),x(12),x(13),x(14),categ_yesno(x(15)),categ_yesno(x(16)),categ_yesno(x(17)),categ_yesno(x(18)),categ_yesno(x(19)),categ_yesno(x(20)),categ_yesno(x(21)),categ_yesno(x(22)),x(23),x(24),x(25),x(28),x(29),x(30),x(31),x(32),x(26))
  y.map( z => z.toString.toDouble )
})
  
val vect = concat.zip(rdd1).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.first
res43: Array[Double] = Array(0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 18.0, 1.0, 1.0, 1.0, 4.0, 4.0, 2.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 4.0, 3.0, 4.0, 3.0, 6.0, 5.0, 6.0, 6.0, 1.0)

val rdd1_dt = rdd.map( x => { 
  val y = Array(categ_sex(x(1)),x(2),categ_address(x(3)),categ_famsize(x(4)),categ_pstatus(x(5)),x(6),x(7),categ_mjob(x(8)),categ_fjob(x(9)),categ_reason(x(10)),categ_guardian(x(11)),x(12),x(13),x(14),categ_yesno(x(15)),categ_yesno(x(16)),categ_yesno(x(17)),categ_yesno(x(18)),categ_yesno(x(19)),categ_yesno(x(20)),categ_yesno(x(21)),categ_yesno(x(22)),x(23),x(24),x(25),x(28),x(29),x(30),x(31),x(32),x(26))
  y.map( z => z.toString.toDouble )
})

val data = vect.zip(rdd1_dt)

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val train_rdd = sets(0)
val test_rdd = sets(1)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val trainSet = train_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1) - 1
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
  
val testSet = test_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1) - 1
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

---- MLlib Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(5).run(trainSet)

val validPredicts =  testSet.map(p => (model.predict(p.features),p.label))

validPredicts.take(20)
res46: Array[(Double, Double)] = Array((3.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (2.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (2.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (2.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,4.0), (2.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 83
validPredicts.count                            // 123
val accuracy = metrics.accuracy   // 0.6747967479674797

metrics.confusionMatrix
res49: org.apache.spark.mllib.linalg.Matrix =
75.0  6.0  7.0  2.0  0.0
12.0  3.0  2.0  2.0  0.0
2.0   2.0  4.0  0.0  1.0
0.0   0.0  0.0  1.0  0.0
1.0   1.0  1.0  1.0  0.0

---- MLlib Naive Bayes regression --------------

import org.apache.spark.mllib.classification.NaiveBayes
val model = NaiveBayes.train(trainSet)

val validPredicts =  testSet.map(p => (model.predict(p.features),p.label))

validPredicts.take(20)
res50: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,4.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 82
validPredicts.count                            // 123
val accuracy = metrics.accuracy   // 0.6666666666666666

metrics.confusionMatrix
res53: org.apache.spark.mllib.linalg.Matrix =
77.0  10.0  0.0  1.0  2.0
13.0  5.0   0.0  0.0  1.0
6.0   1.0   0.0  1.0  1.0
0.0   0.0   0.0  0.0  1.0
4.0   0.0   0.0  0.0  0.

---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res14: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,22.0,1.0,1.0,1.0,4.0,4.0,4.0,4.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,5.0,5.0,5.0,56.0,19.0,19.0,19.0]

matrixSummary.min
res55: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,4.0,0.0,0.0]

matrixSummary.mean
res56: org.apache.spark.mllib.linalg.Vector = [0.06985294117647059,0.16176470588235295,0.29044117647058826,0.3382352941176471,0.13970588235294118,0.03676470588235294,0.08455882352941177,0.3014705882352941,0.5220588235294118,0.05514705882352941,0.09926470588235294,0.3088235294117647,0.3639705882352941,0.22794117647058823,0.21323529411764705,0.08088235294117647,0.7058823529411765,0.4632352941176471,16.705882352941178,0.7941176470588235,0.6985294117647058,0.10294117647058823,2.812500000000001,2.562499999999999,1.4375,2.0073529411764706,0.34191176470588236,0.8676470588235294,0.36764705882352944,0.5294117647058824,0.4963235294117647,0.1875,0.04779411764705882,0.16911764705882354,0.6617647058823529,3.9338235294117636,3.2426470588235294,3.091911764705882,3.470588235294118,5.761029411764706,10....

matrixSummary.variance
res57: org.apache.spark.mllib.linalg.Vector = [0.06521326242674191,0.13609724332537443,0.20684556110266986,0.2246581289342305,0.1206316474929455,0.03554373779031908,0.07769426958975471,0.21136314304319515,0.25043412198827875,0.052298133275450395,0.08974115476448882,0.21423920121554157,0.23235022791404386,0.17663338398089862,0.16838506620360322,0.07461471673540264,0.20837855437377903,0.24956587801172128,1.654873019318434,0.16409811156935097,0.21136314304319515,0.0926850444975038,1.2082564575645758,1.162130996309963,0.45364391143911437,0.6862925982200999,0.5800819405252876,0.11525938788799653,0.23334056869980463,0.25005426524853486,0.25090894291295857,0.1529059040590406,0.045677772954200134,0.1410353809420447,0.2246581289342305,0.7705122639461687,1.033156066854786,1.2498236379422623,2.028...

----- Decide to scale features because columns have different scales and then retrain the model 

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, true).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))

---- Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(5).run(trainScaled)

val validPredicts =  testSet.map(p => (model.predict(scaler.transform(p.features)),p.label))

validPredicts.take(20)
res59: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (2.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (2.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,4.0), (1.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 66
validPredicts.count                            // 123
val accuracy = metrics.accuracy   // 0.5365853658536586

metrics.confusionMatrix
res62: org.apache.spark.mllib.linalg.Matrix =
53.0  21.0  9.0  3.0  4.0
4.0   7.0   3.0  3.0  2.0
0.0   3.0   6.0  0.0  0.0
0.0   0.0   1.0  0.0  0.0
0.0   2.0   1.0  1.0  0.0


----- Not helpful on standardizing. Using decision tree model to evaluate performance.

val trainSet = train_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1) - 1
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1) - 1
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int](0->2, 2->2, 3->2, 4->2, 7->5, 8->5, 9->4, 10->3, 14->2, 15->2, 16->2, 17->2, 18->2, 19->2, 20->2, 21->2)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, accuracy") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 5, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new MulticlassMetrics(validPredicts)
    println("%s, %d, %d ->  %d / %d, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.accuracy))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, accuracy
gini, 10, 32 ->  73 / 123, 0.5935  *
gini, 10, 48 ->  73 / 123, 0.5935
gini, 10, 64 ->  73 / 123, 0.5935
gini, 20, 32 ->  67 / 123, 0.5447
gini, 20, 48 ->  67 / 123, 0.5447
gini, 20, 64 ->  67 / 123, 0.5447
gini, 30, 32 ->  67 / 123, 0.5447
gini, 30, 48 ->  67 / 123, 0.5447
gini, 30, 64 ->  67 / 123, 0.5447
entropy, 10, 32 ->  71 / 123, 0.5772
entropy, 10, 48 ->  71 / 123, 0.5772
entropy, 10, 64 ->  71 / 123, 0.5772
entropy, 20, 32 ->  70 / 123, 0.5691
entropy, 20, 48 ->  70 / 123, 0.5691
entropy, 20, 64 ->  70 / 123, 0.5691
entropy, 30, 32 ->  70 / 123, 0.5691
entropy, 30, 48 ->  70 / 123, 0.5691
entropy, 30, 64 ->  70 / 123, 0.5691


val model = DecisionTree.trainClassifier(trainSet, 5, categoricalFeaturesInfo, "gini", 10, 32)

model.toDebugString
res66: String =
"DecisionTreeModel classifier of depth 10 with 127 nodes
  If (feature 24 <= 3.5)
   If (feature 26 <= 47.0)
    If (feature 2 in {0.0})
     If (feature 3 in {0.0})
      If (feature 15 in {0.0})
       If (feature 7 in {2.0})
        If (feature 5 <= 2.5)
         Predict: 1.0
        Else (feature 5 > 2.5)
         Predict: 0.0
       Else (feature 7 not in {2.0})
        Predict: 1.0
      Else (feature 15 not in {0.0})
       If (feature 6 <= 1.5)
        Predict: 1.0
       Else (feature 6 > 1.5)
        Predict: 2.0
     Else (feature 3 not in {0.0})
      If (feature 26 <= 9.5)
       If (feature 8 in {0.0,1.0,2.0,3.0})
        If (feature 13 <= 1.5)
         If (feature 18 in {0.0})
          Predict: 0.0
         Else (feature 18 not in {0.0})
          If (fea...


val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res67: Array[(Double, Double)] = Array((0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (4.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,4.0), (1.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 73
validPredicts.count                            // 123
val accuracy = metrics.accuracy   // 0.5934959349593496

metrics.confusionMatrix
res70: org.apache.spark.mllib.linalg.Matrix =
65.0  20.0  3.0  0.0  2.0
12.0  5.0   2.0  0.0  0.0
4.0   2.0   3.0  0.0  0.0
0.0   0.0   1.0  0.0  0.0
2.0   2.0   0.0  0.0  0.0
