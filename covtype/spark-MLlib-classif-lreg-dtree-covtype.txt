---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("covtype/covtype.csv").map(x => x.split(",").map( y => y.toDouble))
   
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = rdd.map( x => {
   val arr_size = x.size - 1
   val l = x(arr_size)-1
   val f = x.slice(0,arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

---- MLlib MultiClass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}

val model = new LogisticRegressionWithLBFGS().setNumClasses(7).run(trainSet)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res6: Array[(Double, Double)] = Array((1.0,1.0), (1.0,1.0), (1.0,4.0), (1.0,4.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,4.0), (1.0,4.0), (1.0,4.0), (1.0,4.0), (1.0,1.0), (1.0,4.0), (1.0,4.0), (1.0,4.0), (1.0,0.0), (1.0,4.0), (1.0,1.0), (1.0,4.0), (1.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 124182
validPredicts.count                            // 174570
val accuracy = metrics.accuracy   // 0.7113593400927994

metrics.confusionMatrix
res9: org.apache.spark.mllib.linalg.Matrix =
43842.0  18548.0  41.0    0.0    17.0   0.0    1164.0
15290.0  67638.0  1560.0  7.0    154.0  409.0  133.0
0.0      1192.0   8352.0  273.0  37.0   916.0  0.0
0.0      1.0      415.0   282.0  0.0    75.0   0.0
51.0     2490.0   174.0   0.0    71.0   10.0   0.0
3.0      1715.0   2612.0  64.0   64.0   813.0  0.0
2885.0   64.0     24.0    0.0    0.0    0.0    3184.0

---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = data.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res5: org.apache.spark.mllib.linalg.Vector = [3858.0,360.0,66.0,1397.0,601.0,7117.0,254.0,254.0,254.0,7173.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]

matrixSummary.min
res6: org.apache.spark.mllib.linalg.Vector = [1859.0,0.0,0.0,0.0,-173.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]

matrixSummary.mean
res7: org.apache.spark.mllib.linalg.Vector = [2959.3653005445467,155.65680743254862,14.103703537964934,269.4282166289203,46.41885537648035,2350.146611429709,212.14604861861704,223.31871630878567,142.5282627553344,1980.291226342956,0.4488650836815763,0.051434393781884025,0.4360736094951567,0.06362691304138296,0.005216759722690753,0.01295153972723455,0.008301033369362424,0.021335187569275677,0.0027486523514144287,0.011316461622135171,1.8071915898466812E-4,3.0808313769767234E-4,0.001974141670051565,0.056167514612434855,0.0213592834571403,0.05158413251361418,0.030001101526302382,0.0010309597736363448,5.163404542419089E-6,0.00489662864106077,0.005889723448052708,0.0032684350753512835,0.006920683221689053,0.015935987552752783,0.001442311002182399,0.05743943326471743,0.09939897971126242,0.0366...

matrixSummary.variance
res8: org.apache.spark.mllib.linalg.Vector = [78391.45141339858,12524.680948803465,56.07376547212906,45177.22856388467,3398.3340304324047,2431275.7492994578,716.6269466471455,390.80138715337637,1464.9395878861,1753492.95360453,0.2473856461171641,0.0487889808905893,0.2459138398479015,0.059578631521066135,0.0051895540726088225,0.012783819348605379,0.008232140382986836,0.020880033278012394,0.00274110197947163,0.011188418575267011,1.8068681055661014E-4,3.079887525681134E-4,0.0019702478257803227,0.05301281615690202,0.02090310044439243,0.048923293990018206,0.029101085520403268,0.0010298986681759757,5.163386768557519E-6,0.004872660055517335,0.005855044683079604,0.0032577580145503405,0.0068727991944482775,0.015682058844409246,0.001440233219990966,0.05414023795359569,0.08951897661785263,0.035281...

---- Standardizing the features --------------

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, false).fit(trainSet.map(x => x.features))
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))
trainScaled.cache

----- with MLlib Multiclass logistic regression ----------------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}

val model = new LogisticRegressionWithLBFGS().setNumClasses(7).run(trainScaled)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

validPredicts.take(20)
res11: Array[(Double, Double)] = Array((1.0,1.0), (1.0,1.0), (1.0,4.0), (1.0,4.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,4.0), (1.0,4.0), (1.0,4.0), (1.0,4.0), (1.0,1.0), (1.0,4.0), (1.0,4.0), (1.0,4.0), (1.0,0.0), (1.0,4.0), (1.0,1.0), (1.0,4.0), (1.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 125671
validPredicts.count                            // 174570
val accuracy = metrics.accuracy   // 0.7198888697943518

metrics.confusionMatrix
res14: org.apache.spark.mllib.linalg.Matrix =
46333.0  15939.0  33.0    0.0    0.0    54.0   1253.0
17767.0  65646.0  1304.0  4.0    102.0  275.0  93.0
0.0      872.0    8952.0  303.0  71.0   546.0  26.0
0.0      2.0      441.0   249.0  8.0    71.0   2.0
33.0     2629.0   94.0    0.0    36.0   4.0    0.0
0.0      1408.0   2835.0  54.0   41.0   895.0  38.0
2546.0   28.0     23.0    0.0    0.0    0.0    3560.0


----- with MLlib Decision tree regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]()  

def iterateLRwSGD(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, accuracy") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 7, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new MulticlassMetrics(validPredicts)
    println("%s, %d, %d ->  %d / %d, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.accuracy))
  }
}


iterateLRwSGD(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> accuracy
gini, 10, 32 ->  135220 / 174570, 0.7746
gini, 10, 48 ->  135457 / 174570, 0.7759
gini, 10, 64 ->  136181 / 174570, 0.7801
gini, 20, 32 ->  157654 / 174570, 0.9031
gini, 20, 48 ->  157920 / 174570, 0.9046
gini, 20, 64 ->  157835 / 174570, 0.9041
gini, 30, 32 ->  162128 / 174570, 0.9287
gini, 30, 48 ->  162342 / 174570, 0.9300
gini, 30, 64 ->  161957 / 174570, 0.9277
entropy, 10, 32 ->  135024 / 174570, 0.7735
entropy, 10, 48 ->  135295 / 174570, 0.7750
entropy, 10, 64 ->  134666 / 174570, 0.7714
entropy, 20, 32 ->  158496 / 174570, 0.9079
entropy, 20, 48 ->  159350 / 174570, 0.9128
entropy, 20, 64 ->  159149 / 174570, 0.9117
entropy, 30, 32 ->  162590 / 174570, 0.9314
entropy, 30, 48 ->  163148 / 174570, 0.9346
entropy, 30, 64 ->  163326 / 174570, 0.9356

val model = DecisionTree.trainClassifier(trainSet, 7, categoricalFeaturesInfo, "entropy", 30, 64)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res17: Array[(Double, Double)] = Array((0.0,1.0), (1.0,1.0), (4.0,4.0), (1.0,4.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (4.0,4.0), (4.0,4.0), (4.0,4.0), (4.0,4.0), (1.0,1.0), (4.0,4.0), (1.0,4.0), (4.0,4.0), (0.0,0.0), (4.0,4.0), (0.0,1.0), (4.0,4.0), (1.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)

metrics.confusionMatrix
res18: org.apache.spark.mllib.linalg.Matrix =
59384.0  3849.0   6.0     0.0    52.0    9.0     312.0
3829.0   80521.0  232.0   0.0    370.0   197.0   42.0
3.0      249.0    9974.0  91.0   38.0    415.0   0.0
0.0      1.0      83.0    636.0  0.0     53.0    0.0
36.0     340.0    19.0    0.0    2383.0  18.0    0.0
12.0     159.0    409.0   34.0   15.0    4642.0  0.0
325.0    46.0     0.0     0.0    0.0     0.0     5786.0



