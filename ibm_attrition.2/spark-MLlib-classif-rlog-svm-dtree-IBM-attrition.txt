---- Feature extraction & Data Munging --------------

val rdd2x = sc.textFile("spark/data/attrition/HR-Employee-Attrition.csv").map( x => x.split(","))

rdd2x.first
res1: Array[String] = Array(Age, Attrition, BusinessTravel, DailyRate, Department, DistanceFromHome, Education, EducationField, EmployeeCount, EmployeeNumber, EnvironmentSatisfaction, Gender, HourlyRate, JobInvolvement, JobLevel, JobRole, JobSatisfaction, MaritalStatus, MonthlyIncome, MonthlyRate, NumCompaniesWorked, Over18, OverTime, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StandardHours, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)

val rdd1x = rdd2x.filter( x => x(2) != "BusinessTravel" )

rdd1x.map( x => Array(x(2),x(4),x(7),x(11),x(15),x(17),x(22),x(6),x(0),x(5),x(10),x(13),x(16),x(18),x(20),x(24),x(25),x(27),x(28),x(29),x(30),x(31),x(32),x(33),x(34),x(1))).take(2)
res5: Array[Array[String]] = Array(Array(Travel_Rarely, Sales, Life Sciences, Female, Sales Executive, Single, Yes, 2, 41, 1, 2, 3, 4, 5993, 8, 3, 1, 0, 8, 0, 1, 6, 4, 0, 5, Yes), Array(Travel_Frequently, Research & Development, Life Sciences, Male, Research Scientist, Married, No, 1, 49, 8, 3, 2, 2, 5130, 1, 4, 4, 1, 10, 3, 3, 10, 7, 1, 7, No))

val rdd = rdd1x.map( x => Array(x(2),x(4),x(7),x(11),x(15),x(17),x(22),x(6),x(0),x(5),x(10),x(13),x(16),x(18),x(20),x(24),x(25),x(27),x(28),x(29),x(30),x(31),x(32),x(33),x(34),x(1)))

rdd.take(2)
res6: Array[Array[String]] = Array(Array(Travel_Rarely, Sales, Life Sciences, Female, Sales Executive, Single, Yes, 2, 41, 1, 2, 3, 4, 5993, 8, 3, 1, 0, 8, 0, 1, 6, 4, 0, 5, Yes), Array(Travel_Frequently, Research & Development, Life Sciences, Male, Research Scientist, Married, No, 1, 49, 8, 3, 2, 2, 5130, 1, 4, 4, 1, 10, 3, 3, 10, 7, 1, 7, No))


---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
	val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
	if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
    categoryFeatures
	})
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val rdd1 = rdd.map( x => x.toArray )
val concat = mergeArray(rdd1,0,1,2,3,4,5,6)

concat.take(2)
res5: Array[Array[Double]] = Array(Array(0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0), Array(0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0))

val categories = rdd1.map( x => x(x.size-1)).distinct.zipWithIndex.collectAsMap
categories: scala.collection.Map[String,Long] = Map(No -> 0, Yes -> 1)

val rdd2 = rdd1.map( x => x.slice(7,x.size)).map( y => y.map( x => {
                     try { x.toDouble } catch { case _ : Throwable => categories(x).toString.toDouble } }))

rdd2.take(2)
res6: Array[Array[Double]] = Array(Array(2.0, 41.0, 1.0, 2.0, 3.0, 4.0, 5993.0, 8.0, 3.0, 1.0, 0.0, 8.0, 0.0, 1.0, 6.0, 4.0, 0.0, 5.0, 1.0), Array(1.0, 49.0, 8.0, 3.0, 2.0, 2.0, 5130.0, 1.0, 4.0, 4.0, 1.0, 10.0, 3.0, 3.0, 10.0, 7.0, 1.0, 7.0, 0.0))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(2)
res7: Array[Array[Double]] = Array(Array(0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 2.0, 41.0, 1.0, 2.0, 3.0, 4.0, 5993.0, 8.0, 3.0, 1.0, 0.0, 8.0, 0.0, 1.0, 6.0, 4.0, 0.0, 5.0, 1.0), Array(0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 49.0, 8.0, 3.0, 2.0, 2.0, 5130.0, 1.0, 4.0, 4.0, 1.0, 10.0, 3.0, 3.0, 10.0, 7.0, 1.0, 7.0, 0.0))

val categ_travel = rdd.map( x => x(0)).distinct.zipWithIndex.collectAsMap
categ_travel: scala.collection.Map[String,Long] = Map(Travel_Rarely -> 2, Non-Travel -> 0, Travel_Frequently -> 1)

val categ_dept = rdd.map( x => x(1)).distinct.zipWithIndex.collectAsMap
categ_dept: scala.collection.Map[String,Long] = Map(Research & Development -> 2, Sales -> 1, Human Resources -> 0)

val categ_educ_field = rdd.map( x => x(2)).distinct.zipWithIndex.collectAsMap
categ_educ_field: scala.collection.Map[String,Long] = Map(Marketing -> 4, Medical -> 3, Other -> 2, Life Sciences -> 5, Technical Degree -> 1, Human Resources -> 0)

val categ_gender = rdd.map( x => x(3)).distinct.zipWithIndex.collectAsMap
categ_gender: scala.collection.Map[String,Long] = Map(Male -> 0, Female -> 1)

val categ_jobrole = rdd.map( x => x(4)).distinct.zipWithIndex.collectAsMap
categ_jobrole: scala.collection.Map[String,Long] = Map(Sales Representative -> 7, Healthcare Representative -> 3, Laboratory Technician -> 5, Sales Executive -> 1, Research Director -> 8, Manager -> 4, Manufacturing Director -> 6, Research Scientist -> 2, Human Resources -> 0)

val categ_marital = rdd.map( x => x(5)).distinct.zipWithIndex.collectAsMap
categ_marital: scala.collection.Map[String,Long] = Map(Divorced -> 2, Single -> 0, Married -> 1)

val categ_overtime = rdd.map( x => x(6)).distinct.zipWithIndex.collectAsMap
categ_overtime: scala.collection.Map[String,Long] = Map(No -> 0, Yes -> 1)

val rdd2_dt = rdd.map( x => Array(categ_travel(x(0)),categ_dept(x(1)),categ_educ_field(x(2)),categ_gender(x(3)),categ_jobrole(x(4)),categ_marital(x(5)),categ_overtime(x(6)))).
                  map( x => x.map( y => y.toDouble )).
                  zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

rdd2_dt.take(2)
res9: Array[Array[Double]] = Array(Array(2.0, 1.0, 5.0, 1.0, 1.0, 0.0, 1.0, 2.0, 41.0, 1.0, 2.0, 3.0, 4.0, 5993.0, 8.0, 3.0, 1.0, 0.0, 8.0, 0.0, 1.0, 6.0, 4.0, 0.0, 5.0, 1.0), Array(1.0, 2.0, 5.0, 0.0, 2.0, 1.0, 0.0, 1.0, 49.0, 8.0, 3.0, 2.0, 2.0, 5130.0, 1.0, 4.0, 4.0, 1.0, 10.0, 3.0, 3.0, 10.0, 7.0, 1.0, 7.0, 0.0))

val data = vect.zip(rdd2_dt)

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val train_rdd = sets(0)
val test_rdd = sets(1)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint  

val trainSet = train_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

---- MLlib logistic regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainSet, testSet)
iter, step -> pred / total, AuPR, AuROC
100, 0.100 -> 374 / 451, 0.1707, 0.5000
100, 0.010 -> 374 / 451, 0.1707, 0.5000
100, 0.001 -> 374 / 451, 0.1707, 0.5000
300, 0.100 -> 374 / 451, 0.1707, 0.5000
300, 0.010 -> 374 / 451, 0.1707, 0.5000
300, 0.001 -> 374 / 451, 0.1707, 0.5000
500, 0.100 -> 374 / 451, 0.1707, 0.5000
500, 0.010 -> 374 / 451, 0.1707, 0.5000
500, 0.001 -> 77 / 451, 0.1707, 0.5000

---- MLlib SVM regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(100,300,500), trainSet, testSet)
iter -> pred / total, AuPR, AuROC
100 -> 374 / 451, 0.1707, 0.5000
300 -> 370 / 451, 0.2379, 0.5101
500 -> 374 / 451, 0.1707, 0.5000


----- Estimation is not so good. But analyze the individual statistics and standardize 

import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics

matrixSummary.max
res3: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,60.0,29.0,4.0,4.0,4.0,19999.0,9.0,4.0,4.0,3.0,40.0,6.0,4.0,37.0,18.0,15.0,17.0]

matrixSummary.min
res4: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,18.0,1.0,1.0,1.0,1.0,1051.0,0.0,3.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0]

matrixSummary.mean
res17: org.apache.spark.mllib.linalg.Vector = [0.0971540726202159,0.1874386653581943,0.7154072620215898,0.03631010794896958,0.31108930323846906,0.6526005888125613,0.016683022571148183,0.053974484789008834,0.1069676153091266,0.08635917566241413,0.32482826300294404,0.4111874386653582,0.5848871442590775,0.029440628066732092,0.22080471050049066,0.08243375858684986,0.08832188420019627,0.20019627085377822,0.0647693817468106,0.19627085377821393,0.06378802747791953,0.053974484789008834,0.323846908734053,0.4524043179587831,0.2237487733071639,0.267909715407262,2.93817468105986,36.65947006869481,8.988223748773304,2.714425907752699,2.739941118743868,2.7536800785083413,6324.155053974482,2.606476938174679,3.1521099116781164,2.7409224730127586,0.791952894995093,10.935230618253199,2.7742885181550556,2....

matrixSummary.variance
res18: org.apache.spark.mllib.linalg.Vector = [0.08780132299665877,0.15245502447601658,0.20379971118493226,0.03502605698024374,0.21452327197780482,0.22693576467548793,0.016420813964921885,0.05111139816955257,0.09561938107200904,0.07897877459892688,0.21953029955405257,0.24235016031357065,0.24303267389154204,0.028601946127699448,0.17221899817032377,0.07571273504784344,0.08060022634772332,0.1602750105558244,0.060633812185373774,0.15790356507304246,0.05977777820622321,0.05111139816955257,0.21918518675615176,0.2479780053251483,0.17385587395478058,0.19632676590748277,0.9892976472561607,81.29158753815044,63.88001835460245,1.1826109421964994,0.5187469513429516,1.2310154221076564,2.156966162622933E7,5.957954078789829,0.12909917847730065,1.1469582837675527,0.7228821353034968,57.56751004008323,1.7...

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache

----- with MLlib logistic regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainScaled, testScaled)
iter, step -> pred / total, AuPR, AuROC
100, 0.100 -> 294 / 451, 0.3038, 0.7385  *
100, 0.010 -> 280 / 451, 0.2809, 0.7095
100, 0.001 -> 280 / 451, 0.2809, 0.7095
300, 0.100 -> 293 / 451, 0.2999, 0.7320
300, 0.010 -> 280 / 451, 0.2809, 0.7095
300, 0.001 -> 280 / 451, 0.2809, 0.7095
500, 0.100 -> 293 / 451, 0.2999, 0.7320
500, 0.010 -> 280 / 451, 0.2809, 0.7095
500, 0.001 -> 280 / 451, 0.2809, 0.7095

----- with MLlib SVM regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

scala> iterateSVM(Array(100,300,500), trainScaled, testScaled)
iter -> pred / total, AuPR, AuROC
100 -> 295 / 451, 0.3001, 0.7296
300 -> 300 / 451, 0.2993, 0.7208
500 -> 300 / 451, 0.2993, 0.7208


----- MLlib DecisionTree regression --------------

val trainSet = train_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int](0->3, 1->3, 2->6, 3->2, 4->9, 5->3, 6->2)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, accuracy") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, accuracy
gini, 10, 32 -> 352 / 451, 0.2876, 0.5995
gini, 10, 48 -> 356 / 451, 0.3012, 0.6049
gini, 10, 64 -> 359 / 451, 0.3232, 0.6243
gini, 20, 32 -> 349 / 451, 0.2900, 0.6110
gini, 20, 48 -> 352 / 451, 0.3063, 0.6253
gini, 20, 64 -> 357 / 451, 0.3295, 0.6423 *
gini, 30, 32 -> 349 / 451, 0.2900, 0.6110
gini, 30, 48 -> 352 / 451, 0.3063, 0.6253
gini, 30, 64 -> 357 / 451, 0.3295, 0.6423
entropy, 10, 32 -> 357 / 451, 0.3048, 0.6062
entropy, 10, 48 -> 356 / 451, 0.3012, 0.6049
entropy, 10, 64 -> 359 / 451, 0.3300, 0.6346
entropy, 20, 32 -> 357 / 451, 0.3086, 0.6113
entropy, 20, 48 -> 356 / 451, 0.3050, 0.6100
entropy, 20, 64 -> 359 / 451, 0.3300, 0.6346
entropy, 30, 32 -> 357 / 451, 0.3086, 0.6113
entropy, 30, 48 -> 356 / 451, 0.3050, 0.6100
entropy, 30, 64 -> 359 / 451, 0.3300, 0.6346


val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "gini", 20, 64)

model.toDebugString
res25: String =
"DecisionTreeModel classifier of depth 15 with 251 nodes
  If (feature 6 in {0.0})
   If (feature 8 <= 21.5)
    If (feature 13 <= 1688.5)
     If (feature 3 in {0.0})
      Predict: 0.0
     Else (feature 3 not in {0.0})
      If (feature 0 in {0.0})
       Predict: 0.0
      Else (feature 0 not in {0.0})
       Predict: 1.0
    Else (feature 13 > 1688.5)
     If (feature 22 <= 0.5)
      Predict: 1.0
     Else (feature 22 > 0.5)
      If (feature 3 in {1.0})
       Predict: 0.0
      Else (feature 3 not in {1.0})
       If (feature 11 <= 2.5)
        Predict: 1.0
       Else (feature 11 > 2.5)
        Predict: 0.0
   Else (feature 8 > 21.5)
    If (feature 11 <= 1.5)
     If (feature 14 <= 6.5)
      If (feature 4 in {0.0,5.0,1.0,6.0,2.0,3.0,8.0,4.0})
       If (featur...


val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res26: Array[(Double, Double)] = Array((0.0,1.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (1.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 357
validPredicts.count                            // 451
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.3295107482099352
metrics.areaUnderROC  // 0.6422841864018335