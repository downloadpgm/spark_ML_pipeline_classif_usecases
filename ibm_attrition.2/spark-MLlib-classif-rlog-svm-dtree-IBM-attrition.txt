---- Feature extraction & Data Munging --------------

val hdr = sc.textFile("spark/data/attrition/HR-Employee-Attrition.csv").map( x => x.split(",")).take(1)(0)

val rdd2x = sc.textFile("spark/data/attrition/HR-Employee-Attrition.csv")

rdd2x.take(5).mkString("\n")
res9: String =
Age,Attrition,BusinessTravel,DailyRate,Department,DistanceFromHome,Education,EducationField,EmployeeCount,EmployeeNumber,EnvironmentSatisfaction,Gender,HourlyRate,JobInvolvement,JobLevel,JobRole,JobSatisfaction,MaritalStatus,MonthlyIncome,MonthlyRate,NumCompaniesWorked,Over18,OverTime,PercentSalaryHike,PerformanceRating,RelationshipSatisfaction,StandardHours,StockOptionLevel,TotalWorkingYears,TrainingTimesLastYear,WorkLifeBalance,YearsAtCompany,YearsInCurrentRole,YearsSinceLastPromotion,YearsWithCurrManager
41,Yes,Travel_Rarely,1102,Sales,1,2,Life Sciences,1,1,2,Female,94,3,2,Sales Executive,4,Single,5993,19479,8,Y,Yes,11,3,1,80,0,8,0,1,6,4,0,5
49,No,Travel_Frequently,279,Research & Development,8,1,Life Sciences,1,2,3,Male,61,2,2,Research Scientist,2,Married,5130,24907,1,...

val rdd1x = rdd2x.filter( x => ! x.contains("BusinessTravel")).map( x => x.split(","))

rdd1x.count
res14: Long = 1470

val orderingDesc = Ordering.by[(String, Int), Int](_._2)

// BusinessTravel
rdd1x.map( x => x(2)).distinct.count
res13: Long = 3

rdd1x.map( x => (x(2),1)).reduceByKey( _+_ ).top(10)(orderingDesc).mkString("\n")
res18: String =
(Travel_Rarely,1043)
(Travel_Frequently,277)
(Non-Travel,150)

// Department
rdd1x.map( x => x(4)).distinct.count
res21: Long = 3

rdd1x.map( x => (x(4),1)).reduceByKey( _+_ ).top(10)(orderingDesc).mkString("\n")
res22: String =
(Research & Development,961)
(Sales,446)
(Human Resources,63)

// Education
rdd1x.map( x => x(6)).distinct.count
res23: Long = 5

scala> rdd1x.map( x => (x(6),1)).reduceByKey( _+_ ).top(10)(orderingDesc).mkString("\n")
res24: String =
(3,572)
(4,398)
(2,282)
(1,170)
(5,48)

// EducationField
rdd1x.map( x => x(7)).distinct.count
res26: Long = 6

scala> rdd1x.map( x => (x(7),1)).reduceByKey( _+_ ).top(10)(orderingDesc).mkString("\n")
res27: String =
(Life Sciences,606)
(Medical,464)
(Marketing,159)
(Technical Degree,132)
(Other,82)
(Human Resources,27)

// Gender
rdd1x.map( x => x(11)).distinct.count
res29: Long = 2

scala> rdd1x.map( x => (x(11),1)).reduceByKey( _+_ ).top(10)(orderingDesc).mkString("\n")
res30: String =
(Male,882)
(Female,588)

// JobLevel
rdd1x.map( x => x(14)).distinct.count
res31: Long = 5

scala> rdd1x.map( x => (x(14),1)).reduceByKey( _+_ ).top(10)(orderingDesc).mkString("\n")
res32: String =
(1,543)
(2,534)
(3,218)
(4,106)
(5,69)

// JobRole
rdd1x.map( x => x(15)).distinct.count
res33: Long = 9

scala> rdd1x.map( x => (x(15),1)).reduceByKey( _+_ ).top(10)(orderingDesc).mkString("\n")
res34: String =
(Sales Executive,326)
(Research Scientist,292)
(Laboratory Technician,259)
(Manufacturing Director,145)
(Healthcare Representative,131)
(Manager,102)
(Sales Representative,83)
(Research Director,80)
(Human Resources,52)

// MaritalStatus
rdd1x.map( x => x(17)).distinct.count
res35: Long = 3

scala> rdd1x.map( x => (x(17),1)).reduceByKey( _+_ ).top(10)(orderingDesc).mkString("\n")
res36: String =
(Married,673)
(Single,470)
(Divorced,327)


rdd1x.map( x => Array(x(2),x(4),x(7),x(11),x(15),x(17),x(22),x(6),x(0),x(5),x(10),x(13),x(16),x(18),x(20),x(24),x(25),x(27),x(28),x(29),x(30),x(31),x(32),x(33),x(34),x(1))).take(2)
res11: Array[Array[String]] = Array(Array(Travel_Rarely, Sales, Life Sciences, Female, Sales Executive, Single, Yes, 2, 41, 1, 2, 3, 4, 5993, 8, 3, 1, 0, 8, 0, 1, 6, 4, 0, 5, Yes), Array(Travel_Frequently, Research & Development, Life Sciences, Male, Research Scientist, Married, No, 1, 49, 8, 3, 2, 2, 5130, 1, 4, 4, 1, 10, 3, 3, 10, 7, 1, 7, No))

val rdd = rdd1x.map( x => Array(x(2),x(4),x(7),x(11),x(15),x(17),x(22),x(0),x(5),x(10),x(13),x(16),x(18),x(20),x(24),x(25),x(27),x(28),x(29),x(30),x(31),x(32),x(33),x(34),x(1)))

rdd.take(2)
res43: Array[Array[String]] = Array(Array(Travel_Rarely, Sales, Life Sciences, Female, Sales Executive, Single, Yes, 41, 1, 2, 3, 4, 5993, 8, 3, 1, 0, 8, 0, 1, 6, 4, 0, 5, Yes), Array(Travel_Frequently, Research & Development, Life Sciences, Male, Research Scientist, Married, No, 49, 8, 3, 2, 2, 5130, 1, 4, 4, 1, 10, 3, 3, 10, 7, 1, 7, No))


---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  print(hdr(idx) + " : ")
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  println(categories)
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
	val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
	if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
    categoryFeatures
	})
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val rdd1 = rdd.map( x => x.toArray )

val concat = mergeArray(rdd1,0,1,2,3,4,5,6)
Age : Map(Travel_Rarely -> 2, Non-Travel -> 0, Travel_Frequently -> 1)
Attrition : Map(Research & Development -> 2, Sales -> 1, Human Resources -> 0)
BusinessTravel : Map(Marketing -> 2, Medical -> 4, Other -> 1, Life Sciences -> 5, Technical Degree -> 3, Human Resources -> 0)
DailyRate : Map(Male -> 1, Female -> 0)
Department : Map(Sales Representative -> 7, Healthcare Representative -> 2, Laboratory Technician -> 6, Sales Executive -> 1, Research Director -> 8, Manager -> 5, Manufacturing Director -> 3, Research Scientist -> 4, Human Resources -> 0)
DistanceFromHome : Map(Divorced -> 2, Single -> 0, Married -> 1)
Education : Map(No -> 0, Yes -> 1)

concat.take(2)
res5: Array[Array[Double]] = Array(Array(0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0), Array(0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0))

val categories = rdd1.map( x => x(x.size-1)).distinct.zipWithIndex.collectAsMap
categories: scala.collection.Map[String,Long] = Map(No -> 0, Yes -> 1)

val rdd2 = rdd1.map( x => x.slice(7,x.size)).map( y => y.map( x => {
                     try { x.toDouble } catch { case _ : Throwable => categories(x).toString.toDouble } }))

rdd2.take(2)
res44: Array[Array[Double]] = Array(Array(41.0, 1.0, 2.0, 3.0, 4.0, 5993.0, 8.0, 3.0, 1.0, 0.0, 8.0, 0.0, 1.0, 6.0, 4.0, 0.0, 5.0, 1.0), Array(49.0, 8.0, 3.0, 2.0, 2.0, 5130.0, 1.0, 4.0, 4.0, 1.0, 10.0, 3.0, 3.0, 10.0, 7.0, 1.0, 7.0, 0.0))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(2)
res45: Array[Array[Double]] = Array(Array(0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 41.0, 1.0, 2.0, 3.0, 4.0, 5993.0, 8.0, 3.0, 1.0, 0.0, 8.0, 0.0, 1.0, 6.0, 4.0, 0.0, 5.0, 1.0), Array(0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 49.0, 8.0, 3.0, 2.0, 2.0, 5130.0, 1.0, 4.0, 4.0, 1.0, 10.0, 3.0, 3.0, 10.0, 7.0, 1.0, 7.0, 0.0))

val categ_travel = rdd.map( x => x(0)).distinct.zipWithIndex.collectAsMap
categ_travel: scala.collection.Map[String,Long] = Map(Travel_Rarely -> 2, Non-Travel -> 0, Travel_Frequently -> 1)

val categ_dept = rdd.map( x => x(1)).distinct.zipWithIndex.collectAsMap
categ_dept: scala.collection.Map[String,Long] = Map(Research & Development -> 2, Sales -> 1, Human Resources -> 0)

val categ_educ_field = rdd.map( x => x(2)).distinct.zipWithIndex.collectAsMap
categ_educ_field: scala.collection.Map[String,Long] = Map(Marketing -> 4, Medical -> 3, Other -> 2, Life Sciences -> 5, Technical Degree -> 1, Human Resources -> 0)

val categ_gender = rdd.map( x => x(3)).distinct.zipWithIndex.collectAsMap
categ_gender: scala.collection.Map[String,Long] = Map(Male -> 0, Female -> 1)

val categ_jobrole = rdd.map( x => x(4)).distinct.zipWithIndex.collectAsMap
categ_jobrole: scala.collection.Map[String,Long] = Map(Sales Representative -> 7, Healthcare Representative -> 3, Laboratory Technician -> 5, Sales Executive -> 1, Research Director -> 8, Manager -> 4, Manufacturing Director -> 6, Research Scientist -> 2, Human Resources -> 0)

val categ_marital = rdd.map( x => x(5)).distinct.zipWithIndex.collectAsMap
categ_marital: scala.collection.Map[String,Long] = Map(Divorced -> 2, Single -> 0, Married -> 1)

val categ_overtime = rdd.map( x => x(6)).distinct.zipWithIndex.collectAsMap
categ_overtime: scala.collection.Map[String,Long] = Map(No -> 0, Yes -> 1)

val rdd2_dt = rdd.map( x => Array(categ_travel(x(0)),categ_dept(x(1)),categ_educ_field(x(2)),categ_gender(x(3)),categ_jobrole(x(4)),categ_marital(x(5)),categ_overtime(x(6)))).
                  map( x => x.map( y => y.toDouble )).
                  zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

rdd2_dt.take(2)
res46: Array[Array[Double]] = Array(Array(2.0, 1.0, 5.0, 0.0, 1.0, 0.0, 1.0, 41.0, 1.0, 2.0, 3.0, 4.0, 5993.0, 8.0, 3.0, 1.0, 0.0, 8.0, 0.0, 1.0, 6.0, 4.0, 0.0, 5.0, 1.0), Array(1.0, 2.0, 5.0, 1.0, 4.0, 1.0, 0.0, 49.0, 8.0, 3.0, 2.0, 2.0, 5130.0, 1.0, 4.0, 4.0, 1.0, 10.0, 3.0, 3.0, 10.0, 7.0, 1.0, 7.0, 0.0))

val data = vect.zip(rdd2_dt)

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val train_rdd = sets(0)
val test_rdd = sets(1)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint  

val trainSet = train_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache
testSet.cache

---- MLlib logistic regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(100).setRegParam(0.01)
val lr = model.run(trainSet)

val validPredicts = testSet.map(x => (lr.predict(x.features),x.label))
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res54: Double = 0.17073170731707318

metrics.areaUnderROC
res55: Double = 0.5

----- Estimation is not so good. But analyze the individual statistics and standardize 

import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics

scala> matrixSummary.max.toArray.foreach( x => print(f"$x%.2f\t"))
1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00     1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    60.00   29.00   4.00     4.00    4.00    19999.00        9.00    4.00    4.00    3.00    40.00   6.00    4.00    37.00   18.00   15.00    17.00

scala> matrixSummary.min.toArray.foreach( x => print(f"$x%.2f\t"))
0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00     0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    18.00   1.00    1.00     1.00    1.00    1051.00 0.00    3.00    1.00    0.00    0.00    0.00    1.00    0.00    0.00    0.00    0.00

scala> matrixSummary.mean.toArray.foreach( x => print(f"$x%.2f\t"))
0.10    0.19    0.72    0.04    0.31    0.65    0.02    0.05    0.11    0.09    0.32    0.41    0.58    0.03    0.22     0.08    0.09    0.20    0.06    0.20    0.06    0.05    0.32    0.45    0.22    0.27    36.66   8.99    2.71     2.74    2.75    6324.16 2.61    3.15    2.74    0.79    10.94   2.77    2.74    6.88    4.20    2.23    4.02

scala> matrixSummary.variance.toArray.foreach( x => print(f"$x%.2f\t"))
0.09    0.15    0.20    0.04    0.21    0.23    0.02    0.05    0.10    0.08    0.22    0.24    0.24    0.03    0.17     0.08    0.08    0.16    0.06    0.16    0.06    0.05    0.22    0.25    0.17    0.20    81.29   63.88   1.18     0.52    1.23    21569661.63     5.96    0.13    1.15    0.72    57.57   1.71    0.50    36.33   12.81   10.33    12.25

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache
testScaled.cache

---- MLlib logistic regression STD --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(100).setRegParam(0.01)
val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res71: Double = 0.29960511033681764

metrics.areaUnderROC
res72: Double = 0.7394957983193277

---------------------------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], regSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step, reg  -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes; reg <- regSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step).setRegParam(reg)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, reg, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(10, 20, 40, 100),Array(1, 0.1, 0.01, 0.001),Array(0.1, 0.01), trainScaled, testScaled)
iter, step, reg  -> pred / total, AuPR, AuROC
10, 1.000, 0.100 -> 293 / 451, 0.3025, 0.7372
10, 1.000, 0.010 -> 292 / 451, 0.2986, 0.7307
10, 0.100, 0.100 -> 286 / 451, 0.2857, 0.7124
10, 0.100, 0.010 -> 286 / 451, 0.2857, 0.7124
10, 0.010, 0.100 -> 278 / 451, 0.2760, 0.7017
10, 0.010, 0.010 -> 278 / 451, 0.2760, 0.7017
10, 0.001, 0.100 -> 278 / 451, 0.2760, 0.7017
10, 0.001, 0.010 -> 278 / 451, 0.2760, 0.7017
20, 1.000, 0.100 -> 291 / 451, 0.2998, 0.7345
20, 1.000, 0.010 -> 291 / 451, 0.3023, 0.7397
20, 0.100, 0.100 -> 285 / 451, 0.2844, 0.7110
20, 0.100, 0.010 -> 285 / 451, 0.2844, 0.7110
20, 0.010, 0.100 -> 278 / 451, 0.2760, 0.7017
20, 0.010, 0.010 -> 278 / 451, 0.2760, 0.7017
20, 0.001, 0.100 -> 278 / 451, 0.2760, 0.7017
20, 0.001, 0.010 -> 278 / 451, 0.2760, 0.7017
40, 1.000, 0.100 -> 290 / 451, 0.2985, 0.7332
40, 1.000, 0.010 -> 292 / 451, 0.3087, 0.7513
40, 0.100, 0.100 -> 287 / 451, 0.2870, 0.7137
40, 0.100, 0.010 -> 286 / 451, 0.2857, 0.7124
40, 0.010, 0.100 -> 278 / 451, 0.2760, 0.7017
40, 0.010, 0.010 -> 278 / 451, 0.2760, 0.7017
40, 0.001, 0.100 -> 278 / 451, 0.2760, 0.7017
40, 0.001, 0.010 -> 278 / 451, 0.2760, 0.7017
100, 1.000, 0.100 -> 290 / 451, 0.2985, 0.7332
100, 1.000, 0.010 -> 287 / 451, 0.2996, 0.7395
100, 0.100, 0.100 -> 289 / 451, 0.2870, 0.7112
100, 0.100, 0.010 -> 289 / 451, 0.2870, 0.7112
100, 0.010, 0.100 -> 278 / 451, 0.2760, 0.7017
100, 0.010, 0.010 -> 278 / 451, 0.2760, 0.7017
100, 0.001, 0.100 -> 278 / 451, 0.2760, 0.7017
100, 0.001, 0.010 -> 278 / 451, 0.2760, 0.7017


----- with MLlib SVM regression STD ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(10, 20, 40, 100), trainScaled, testScaled)
iter -> pred / total, AuPR, AuROC
10 -> 294 / 451, 0.3038, 0.7385
20 -> 297 / 451, 0.3106, 0.7477
40 -> 299 / 451, 0.3134, 0.7504
100 -> 298 / 451, 0.3043, 0.7336


----- MLlib DecisionTree regression --------------

val trainSet = train_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache
testSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int](0->3, 1->3, 2->6, 3->2, 4->9, 5->3, 6->2)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, AuPR, AuROC") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AuPR, AuROC
gini, 10, 32 -> 355 / 451, 0.2976, 0.6035
gini, 10, 48 -> 360 / 451, 0.3164, 0.6102
gini, 10, 64 -> 363 / 451, 0.3393, 0.6297
gini, 20, 32 -> 355 / 451, 0.3089, 0.6190
gini, 20, 48 -> 353 / 451, 0.3023, 0.6163
gini, 20, 64 -> 362 / 451, 0.3479, 0.6490  *
gini, 30, 32 -> 355 / 451, 0.3089, 0.6190
gini, 30, 48 -> 353 / 451, 0.3023, 0.6163
gini, 30, 64 -> 362 / 451, 0.3479, 0.6490
entropy, 10, 32 -> 354 / 451, 0.2981, 0.6073
entropy, 10, 48 -> 353 / 451, 0.2909, 0.6008
entropy, 10, 64 -> 355 / 451, 0.3160, 0.6293
entropy, 20, 32 -> 352 / 451, 0.2915, 0.6047
entropy, 20, 48 -> 351 / 451, 0.2845, 0.5982
entropy, 20, 64 -> 353 / 451, 0.3059, 0.6215
entropy, 30, 32 -> 352 / 451, 0.2915, 0.6047
entropy, 30, 48 -> 351 / 451, 0.2845, 0.5982
entropy, 30, 64 -> 353 / 451, 0.3059, 0.6215


val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "gini", 20, 64)

model.toDebugString
res27: String =
"DecisionTreeModel classifier of depth 15 with 253 nodes
  If (feature 6 in {0.0})
   If (feature 7 <= 21.5)
    If (feature 12 <= 1688.5)
     If (feature 3 in {0.0})
      Predict: 0.0
     Else (feature 3 not in {0.0})
      If (feature 0 in {0.0})
       Predict: 0.0
      Else (feature 0 not in {0.0})
       Predict: 1.0
    Else (feature 12 > 1688.5)
     If (feature 21 <= 0.5)
      Predict: 1.0
     Else (feature 21 > 0.5)
      If (feature 3 in {1.0})
       Predict: 0.0
      Else (feature 3 not in {1.0})
       If (feature 10 <= 2.5)
        Predict: 1.0
       Else (feature 10 > 2.5)
        Predict: 0.0
   Else (feature 7 > 21.5)
    If (feature 10 <= 1.5)
     If (feature 13 <= 6.5)
      If (feature 4 in {0.0,5.0,1.0,6.0,2.0,3.0,8.0,4.0})
       If (featur...


val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res28: Array[(Double, Double)] = Array((0.0,1.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (1.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 362
validPredicts.count                            // 451

metrics.areaUnderPR   // 0.34790690695697113
metrics.areaUnderROC  // 0.6489686783804431