val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("attrition/HR-Employee-Attrition.csv")

scala> df.printSchema
root
 |-- Age: integer (nullable = true)
 |-- Attrition: string (nullable = true)
 |-- BusinessTravel: string (nullable = true)
 |-- DailyRate: integer (nullable = true)
 |-- Department: string (nullable = true)
 |-- DistanceFromHome: integer (nullable = true)
 |-- Education: integer (nullable = true)
 |-- EducationField: string (nullable = true)
 |-- EmployeeCount: integer (nullable = true)
 |-- EmployeeNumber: integer (nullable = true)
 |-- EnvironmentSatisfaction: integer (nullable = true)
 |-- Gender: string (nullable = true)
 |-- HourlyRate: integer (nullable = true)
 |-- JobInvolvement: integer (nullable = true)
 |-- JobLevel: integer (nullable = true)
 |-- JobRole: string (nullable = true)
 |-- JobSatisfaction: integer (nullable = true)
 |-- MaritalStatus: string (nullable = true)
 |-- MonthlyIncome: integer (nullable = true)
 |-- MonthlyRate: integer (nullable = true)
 |-- NumCompaniesWorked: integer (nullable = true)
 |-- Over18: string (nullable = true)
 |-- OverTime: string (nullable = true)
 |-- PercentSalaryHike: integer (nullable = true)
 |-- PerformanceRating: integer (nullable = true)
 |-- RelationshipSatisfaction: integer (nullable = true)
 |-- StandardHours: integer (nullable = true)
 |-- StockOptionLevel: integer (nullable = true)
 |-- TotalWorkingYears: integer (nullable = true)
 |-- TrainingTimesLastYear: integer (nullable = true)
 |-- WorkLifeBalance: integer (nullable = true)
 |-- YearsAtCompany: integer (nullable = true)
 |-- YearsInCurrentRole: integer (nullable = true)
 |-- YearsSinceLastPromotion: integer (nullable = true)
 |-- YearsWithCurrManager: integer (nullable = true)


val df1 = df.select("BusinessTravel","Department","EducationField","Gender","JobRole","MaritalStatus","OverTime","Education","Age","DistanceFromHome","EnvironmentSatisfaction","JobInvolvement","JobSatisfaction","MonthlyIncome","NumCompaniesWorked","PerformanceRating","RelationshipSatisfaction","StockOptionLevel","TotalWorkingYears","TrainingTimesLastYear","WorkLifeBalance","YearsAtCompany","YearsInCurrentRole","YearsSinceLastPromotion","YearsWithCurrManager","Attrition")

scala> df1.printSchema
root
 |-- BusinessTravel: string (nullable = true)
 |-- Department: string (nullable = true)
 |-- EducationField: string (nullable = true)
 |-- Gender: string (nullable = true)
 |-- JobRole: string (nullable = true)
 |-- MaritalStatus: string (nullable = true)
 |-- OverTime: string (nullable = true)
 |-- Education: integer (nullable = true)
 |-- Age: integer (nullable = true)
 |-- DistanceFromHome: integer (nullable = true)
 |-- EnvironmentSatisfaction: integer (nullable = true)
 |-- JobInvolvement: integer (nullable = true)
 |-- JobSatisfaction: integer (nullable = true)
 |-- MonthlyIncome: integer (nullable = true)
 |-- NumCompaniesWorked: integer (nullable = true)
 |-- PerformanceRating: integer (nullable = true)
 |-- RelationshipSatisfaction: integer (nullable = true)
 |-- StockOptionLevel: integer (nullable = true)
 |-- TotalWorkingYears: integer (nullable = true)
 |-- TrainingTimesLastYear: integer (nullable = true)
 |-- WorkLifeBalance: integer (nullable = true)
 |-- YearsAtCompany: integer (nullable = true)
 |-- YearsInCurrentRole: integer (nullable = true)
 |-- YearsSinceLastPromotion: integer (nullable = true)
 |-- YearsWithCurrManager: integer (nullable = true)
 |-- Attrition: string (nullable = true)
 
import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}

val dfInd1 = new StringIndexer().setInputCol("BusinessTravel").setOutputCol("BusinessTravelIdx")
val dfInd2 = new StringIndexer().setInputCol("Department").setOutputCol("DepartmentIdx")
val dfInd3 = new StringIndexer().setInputCol("EducationField").setOutputCol("EducationFieldIdx")
val dfInd4 = new StringIndexer().setInputCol("Gender").setOutputCol("GenderIdx")
val dfInd5 = new StringIndexer().setInputCol("JobRole").setOutputCol("JobRoleIdx")
val dfInd6 = new StringIndexer().setInputCol("MaritalStatus").setOutputCol("MaritalStatusIdx")
val dfInd7 = new StringIndexer().setInputCol("OverTime").setOutputCol("OverTimeIdx")
val dfInd8 = new StringIndexer().setInputCol("Attrition").setOutputCol("label")

val dfOne1 = new OneHotEncoder().setInputCol("BusinessTravelIdx").setOutputCol("BusinessTravelVect")
val dfOne2 = new OneHotEncoder().setInputCol("DepartmentIdx").setOutputCol("DepartmentVect")
val dfOne3 = new OneHotEncoder().setInputCol("EducationFieldIdx").setOutputCol("EducationFieldVect")
val dfOne4 = new OneHotEncoder().setInputCol("GenderIdx").setOutputCol("GenderVect")
val dfOne5 = new OneHotEncoder().setInputCol("JobRoleIdx").setOutputCol("JobRoleVect")
val dfOne6 = new OneHotEncoder().setInputCol("MaritalStatusIdx").setOutputCol("MaritalStatusVect")
val dfOne7 = new OneHotEncoder().setInputCol("OverTimeIdx").setOutputCol("OverTimeVect")

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("BusinessTravelVect","DepartmentVect","EducationFieldVect","GenderVect","JobRoleVect","MaritalStatusVect","OverTimeVect","Education","Age","DistanceFromHome","EnvironmentSatisfaction","JobInvolvement","JobSatisfaction","MonthlyIncome","NumCompaniesWorked","PerformanceRating","RelationshipSatisfaction","StockOptionLevel","TotalWorkingYears","TrainingTimesLastYear","WorkLifeBalance","YearsAtCompany","YearsInCurrentRole","YearsSinceLastPromotion","YearsWithCurrManager","label"))

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(500).setFitIntercept(true).setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfInd4,dfInd5,dfInd6,dfInd7,dfInd8,dfOne1,dfOne2,dfOne3,dfOne4,dfOne5,dfOne6,dfOne7,va,stdScaler,lr))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

val model = pipeline.fit(trainingData)

val pred = model.transform(testData)

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bceval = new BinaryClassificationEvaluator()

bceval.evaluate(pred)
res17: Double = 1.0

import org.apache.spark.ml.classification.LogisticRegressionModel

val lrmodel = model.stages(17).asInstanceOf[LogisticRegressionModel]

println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
0.0308820809267715
0.09998141532716262
-0.049382164936638606
0.017885284465135053
-0.05099538433561672
-0.01509887547193055
0.027725439087503575
0.0697659026043085
-0.026483362193678486
0.015232874226433832
-0.004119635219656607
-0.03137762504184419
0.09692762985330346
-0.0528127128953206
-0.05107235119524067
-0.03888496680303168
0.052193377709759674
-0.06767394973277646
0.006456880101653702
0.1169916298658585
-0.23712513807888172
-0.002146144108361229
-0.0672328289975517
0.0931178656867991
-0.12171080789982411
-0.11942804053746098
-0.10076679256937635
-0.05911253272862715
0.09577585699940802
-0.022709000475331535
-0.02238316543002006
-0.07967140885477061
-0.07145322594855193
-0.06567428955762938
-0.0879848427647325
0.06540099438722864
-0.07652253126539224
0.10640747730908538
-0.10498116428892218
2.520822911131947

-------------------

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(0.1, 0.01, 0.001)).
addGrid(lr.fitIntercept).
addGrid(lr.maxIter, Array(100,300,500)).build()

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new BinaryClassificationEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.classification.LogisticRegressionModel
val lrmodel = bestmodel.stages(17).asInstanceOf[LogisticRegressionModel]

scala> lrmodel.getRegParam
res3: Double = 0.1

scala> lrmodel.getMaxIter
res4: Int = 100

scala> lrmodel.getThreshold
res5: Double = 0.5

scala> lrmodel.getFitIntercept
res6: Boolean = true

scala> lrmodel.getStandardization
res7: Boolean = true

scala> println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
0.007218037857699571
0.06520401848799404
-0.040644025936269056
0.02078498120868721
-0.035228665942990865
-0.01047455810555779
0.026967946434023074
0.043957216188943
-0.02201261749151514
0.015165691020066699
0.00358077604850664
-0.021182522089179062
0.07936671810474917
-0.048529677638407466
-0.047966336194893024
-0.028273712770712427
0.04355576735259142
-0.04314684476633159
-0.019985100630890713
0.0919342683181454
-0.17960117816935736
-0.00497961374056746
-0.058266229325347606
0.06669751485311398
-0.08844231332735218
-0.09111819493575597
-0.07852315918857321
-0.053912094417945255
0.06237937213845692
-0.013894297475712039
-0.018364835248208502
-0.06929290230888219
-0.06067069772592699
-0.051138426134070714
-0.07054524471567254
0.005020014172696
-0.054642344115084854
0.05907328002895634
-0.070821133734378
1.236441942680971

val pred = bestmodel.transform(testData)
pred: org.apache.spark.sql.DataFrame = [BusinessTravel: string, Department: string ... 44 more fields]

val bceval = new BinaryClassificationEvaluator()
bceval: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_f590b3feb906

bceval.evaluate(pred)
res9: Double = 1.0
