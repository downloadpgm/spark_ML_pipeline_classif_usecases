---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("churn/Churn_Modelling.csv").filter( x => ! x.contains("RowNumber"))

val rdd1 = rdd.map( x => x.split(",")).map( x => x.slice(3,x.size))

rdd1.take(5)
res2: Array[Array[String]] = Array(Array(619, France, Female, 42, 2, 0, 1, 1, 1, 101348.88, 1), Array(608, Spain, Female, 41, 1, 83807.86, 1, 0, 1, 112542.58, 0), Array(502, France, Female, 42, 8, 159660.8, 3, 1, 0, 113931.57, 1), Array(699, France, Female, 39, 1, 0, 2, 0, 0, 93826.63, 0), Array(850, Spain, Female, 43, 2, 125510.82, 1, 1, 1, 79084.1, 0))

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
	val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
	if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
    categoryFeatures
	})
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd1,1,2)

concat.take(2)
res2: Array[Array[Double]] = Array(Array(1.0, 0.0, 0.0, 0.0), Array(0.0, 1.0, 0.0, 0.0))

val rdd2 = rdd1.map( x => {
  val y = Array(x(0),x(3),x(4),x(5),x(6),x(7),x(8),x(9),x(10))
  y.map( z => z.toDouble)
})

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

val categ_country = rdd1.map(x => x(1)).distinct.zipWithIndex.collectAsMap
categ_country: scala.collection.immutable.Map[String,Long] = Map(France -> 0, Spain -> 1, Germany -> 2)

val categ_gender = rdd1.map(x => x(2)).distinct.zipWithIndex.collectAsMap
categ_gender: scala.collection.immutable.Map[String,Long] = Map(Male -> 0, Female -> 1)

val rdd2_dt = rdd1.map( x => {
  val y = Array(x(0),categ_country(x(1)),categ_gender(x(2)),x(3),x(4),x(5),x(6),x(7),x(8),x(9),x(10))
  y.map( z => z.toString.toDouble)
})

val data = vect.zip(rdd2_dt)

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val train_rdd = sets(0)
val test_rdd = sets(1)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint  

val trainSet = train_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

---- MLlib logistic regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainSet, testSet)
iter, step -> pred / total, AuPR, AuROC
100, 0.100 -> 2318 / 2922, 0.2067, 0.5000
100, 0.010 -> 2318 / 2922, 0.2067, 0.5000
100, 0.001 -> 2318 / 2922, 0.2067, 0.5000
300, 0.100 -> 1914 / 2922, 0.2301, 0.5292
300, 0.010 -> 2318 / 2922, 0.2067, 0.5000
300, 0.001 -> 2318 / 2922, 0.2067, 0.5000
500, 0.100 -> 2318 / 2922, 0.2067, 0.5000
500, 0.010 -> 2318 / 2922, 0.2067, 0.5000
500, 0.001 -> 1888 / 2922, 0.2304, 0.5309
 
---- MLlib SVM regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(100,300,500), trainSet, testSet)
iter -> pred / total, AuPR, AuROC
100 -> 2318 / 2922, 0.2067, 0.5000
300 -> 605 / 2922, 0.2068, 0.5002
500 -> 2318 / 2922, 0.2067, 0.5000


---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res26: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,850.0,92.0,10.0,250898.09,4.0,1.0,1.0,199992.48]

matrixSummary.min
res3: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,350.0,18.0,0.0,0.0,1.0,0.0,0.0,90.07]

matrixSummary.mean
res28: org.apache.spark.mllib.linalg.Vector = [0.5018853695324283,0.24522373051784815,0.25289089994972347,0.5444947209653092,0.4555052790346908,650.3612368024109,38.970588235294116,5.031422825540482,76800.48831070877,1.5228758169934626,0.7044997486173957,0.5187280040221217,100277.98718451483]

matrixSummary.variance
res29: org.apache.spark.mllib.linalg.Vector = [0.25002787171049823,0.18511231951724427,0.18896084340680785,0.24805139770940043,0.24805139770940043,9404.219962970483,111.77713609642477,8.419377032509724,3.8894285466663036E9,0.3337318166319536,0.20820602250150813,0.2496806445506856,3.305986531756714E9]

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache

----- with MLlib logistic regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainScaled, testScaled)
iter, step -> pred / total, AuPR, AuROC
100, 0.100 -> 1902 / 2922, 0.3289, 0.6937
100, 0.010 -> 1903 / 2922, 0.3259, 0.6878
100, 0.001 -> 1903 / 2922, 0.3256, 0.6872
300, 0.100 -> 1902 / 2922, 0.3289, 0.6937
300, 0.010 -> 1903 / 2922, 0.3259, 0.6878
300, 0.001 -> 1903 / 2922, 0.3256, 0.6872
500, 0.100 -> 1902 / 2922, 0.3289, 0.6937
500, 0.010 -> 1903 / 2922, 0.3259, 0.6878
500, 0.001 -> 1903 / 2922, 0.3256, 0.6872


----- with MLlib SVM regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

scala> iterateSVM(Array(100,300,500), trainScaled, testScaled)
iter -> pred / total, AuPR, AuROC
100 -> 1935 / 2922, 0.3397, 0.7069
300 -> 1935 / 2922, 0.3397, 0.7069
500 -> 1935 / 2922, 0.3397, 0.7069

----- with MLlib Decision tree regression ----------------------

val trainSet = train_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(x1.size - 1)
   val f1 = x1.slice(0,x1.size - 1)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 1->3 , 2->2 )

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, AuPR, AuROC") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AuPR, AuROC
gini, 10, 32 -> 2416 / 2922, 0.4989, 0.6938
gini, 10, 48 -> 2406 / 2922, 0.4898, 0.6940
gini, 10, 64 -> 2424 / 2922, 0.5065, 0.7028 *
gini, 20, 32 -> 2268 / 2922, 0.4019, 0.6820
gini, 20, 48 -> 2277 / 2922, 0.4049, 0.6803
gini, 20, 64 -> 2298 / 2922, 0.4168, 0.6854
gini, 30, 32 -> 2268 / 2922, 0.4019, 0.6820
gini, 30, 48 -> 2277 / 2922, 0.4049, 0.6803
gini, 30, 64 -> 2297 / 2922, 0.4162, 0.6852
entropy, 10, 32 -> 2443 / 2922, 0.5280, 0.6916
entropy, 10, 48 -> 2432 / 2922, 0.5154, 0.6917
entropy, 10, 64 -> 2444 / 2922, 0.5271, 0.7010
entropy, 20, 32 -> 2332 / 2922, 0.4396, 0.6989
entropy, 20, 48 -> 2327 / 2922, 0.4323, 0.6868
entropy, 20, 64 -> 2337 / 2922, 0.4432, 0.7012
entropy, 30, 32 -> 2327 / 2922, 0.4358, 0.6960
entropy, 30, 48 -> 2326 / 2922, 0.4319, 0.6872
entropy, 30, 64 -> 2334 / 2922, 0.4413, 0.7006


val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "gini", 10, 64)

model.toDebugString
res9: String =
"DecisionTreeModel classifier of depth 10 with 581 nodes
  If (feature 3 <= 42.5)
   If (feature 6 <= 2.5)
    If (feature 6 <= 1.5)
     If (feature 8 <= 0.5)
      If (feature 3 <= 38.5)
       If (feature 1 in {0.0,1.0})
        If (feature 5 <= 57348.325)
         If (feature 0 <= 686.5)
          If (feature 5 <= 6229.595)
           Predict: 0.0
          Else (feature 5 > 6229.595)
           If (feature 0 <= 629.5)
            Predict: 0.0
           Else (feature 0 > 629.5)
            Predict: 1.0
         Else (feature 0 > 686.5)
          If (feature 9 <= 121340.76000000001)
           If (feature 9 <= 38261.36)
            Predict: 1.0
           Else (feature 9 > 38261.36)
            Predict: 0.0
          Else (feature 9 > 121340.76000000001)
           If...

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res8: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 2424
validPredicts.count                            // 2922
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.5064752783626146
metrics.areaUnderROC  // 0.702825283271146