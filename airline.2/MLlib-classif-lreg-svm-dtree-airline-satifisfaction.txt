---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("spark/data/airline/airline_satisfaction_train.csv").map( x => x.split(","))

rdd.take(1)
res9: Array[Array[String]] = Array(Array("", id, Gender, Customer Type, Age, Type of Travel, Class, Flight Distance, Inflight wifi service, Departure/Arrival time convenient, Ease of Online booking, Gate location, Food and drink, Online boarding, Seat comfort, Inflight entertainment, On-board service, Leg room service, Baggage handling, Checkin service, Inflight service, Cleanliness, Departure Delay in Minutes, Arrival Delay in Minutes, satisfaction))

val rdd1 = rdd.filter( x => x(3) != "Customer Type").map( x => x.slice(2,x.size))

rdd1.take(5)
res6: Array[Array[String]] = Array(Array(Male, Loyal Customer, 13, Personal Travel, Eco Plus, 460, 3, 4, 3, 1, 5, 3, 5, 5, 4, 3, 4, 4, 5, 5, 25, 18.0, neutral or dissatisfied), Array(Male, disloyal Customer, 25, Business travel, Business, 235, 3, 2, 3, 3, 1, 3, 1, 1, 1, 5, 3, 1, 4, 1, 1, 6.0, neutral or dissatisfied), Array(Female, Loyal Customer, 26, Business travel, Business, 1142, 2, 2, 2, 2, 5, 5, 5, 5, 4, 3, 4, 4, 4, 5, 0, 0.0, satisfied), Array(Female, Loyal Customer, 25, Business travel, Business, 562, 2, 5, 5, 5, 2, 2, 2, 2, 2, 5, 3, 1, 4, 2, 11, 9.0, neutral or dissatisfied), Array(Male, Loyal Customer, 61, Business travel, Business, 214, 3, 3, 3, 3, 4, 5, 5, 3, 3, 4, 4, 3, 3, 3, 0, 0.0, satisfied))

rdd1.take(10).map( x => x.mkString(", ")).foreach(println)
Male, Loyal Customer, 13, Personal Travel, Eco Plus, 460, 3, 4, 3, 1, 5, 3, 5, 5, 4, 3, 4, 4, 5, 5, 25, 18.0, neutral or dissatisfied
Male, disloyal Customer, 25, Business travel, Business, 235, 3, 2, 3, 3, 1, 3, 1, 1, 1, 5, 3, 1, 4, 1, 1, 6.0, neutral or dissatisfied
Female, Loyal Customer, 26, Business travel, Business, 1142, 2, 2, 2, 2, 5, 5, 5, 5, 4, 3, 4, 4, 4, 5, 0, 0.0, satisfied
Female, Loyal Customer, 25, Business travel, Business, 562, 2, 5, 5, 5, 2, 2, 2, 2, 2, 5, 3, 1, 4, 2, 11, 9.0, neutral or dissatisfied
Male, Loyal Customer, 61, Business travel, Business, 214, 3, 3, 3, 3, 4, 5, 5, 3, 3, 4, 4, 3, 3, 3, 0, 0.0, satisfied
Female, Loyal Customer, 26, Personal Travel, Eco, 1180, 3, 4, 2, 1, 1, 2, 1, 1, 3, 4, 4, 4, 4, 1, 0, 0.0, neutral or dissatisfied
Male, Loyal Customer, 47, Personal Travel, Eco, 1276, 2, 4, 2, 3, 2, 2, 2, 2, 3, 3, 4, 3, 5, 2, 9, 23.0, neutral or dissatisfied
Female, Loyal Customer, 52, Business travel, Business, 2035, 4, 3, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 0.0, satisfied
Female, Loyal Customer, 41, Business travel, Business, 853, 1, 2, 2, 2, 4, 3, 3, 1, 1, 2, 1, 4, 1, 2, 0, 0.0, neutral or dissatisfied
Male, disloyal Customer, 20, Business travel, Eco, 1061, 3, 3, 3, 4, 2, 3, 3, 2, 2, 3, 4, 4, 3, 2, 0, 0.0, neutral or dissatisfied


// Gender
rdd1.map( x => (x(0),1)).reduceByKey( _+_ ).take(10)
res11: Array[(String, Int)] = Array((Female,52727), (Male,51177))

// Customer Type
rdd1.map( x => (x(1),1)).reduceByKey( _+_ ).take(10)
res10: Array[(String, Int)] = Array((Loyal Customer,84923), (disloyal Customer,18981))

// Type of Travel
rdd1.map( x => (x(3),1)).reduceByKey( _+_ ).take(10)
res12: Array[(String, Int)] = Array((Personal Travel,32249), (Business travel,71655))

// Class
rdd1.map( x => (x(4),1)).reduceByKey( _+_ ).take(10)
res13: Array[(String, Int)] = Array((Business,49665), (Eco Plus,7494), (Eco,46745))

// satisfaction
rdd1.map( x => (x(22),1)).reduceByKey( _+_ ).take(10)
res14: Array[(String, Int)] = Array((neutral or dissatisfied,58879), (satisfied,45025))


val hdr = rdd.filter( x => x(3) == "Customer Type").map( x => x.slice(2,x.size)).take(1)(0)
hdr: Array[String] = Array(Gender, Customer Type, Age, Type of Travel, Class, Flight Distance, Inflight wifi service, Departure/Arrival time convenient, Ease of Online booking, Gate location, Food and drink, Online boarding, Seat comfort, Inflight entertainment, On-board service, Leg room service, Baggage handling, Checkin service, Inflight service, Cleanliness, Departure Delay in Minutes, Arrival Delay in Minutes, satisfaction)



rdd1.filter( x => x(x.size-2) == "").count
res20: Long = 310

val categ_label = rdd1.map( x => x(22)).distinct.zipWithIndex.collectAsMap
categ_label: scala.collection.Map[String,Long] = Map(neutral or dissatisfied -> 0, satisfied -> 1)

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  print(hdr(idx) + " : ")
  println(categories)
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
    val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories - 1) else Array.ofDim[Double](1)
    if (numCategories > 2) { 
      if (categoryIdx > 0) categoryFeatures(categoryIdx - 1) = 1.0
    }
    else categoryFeatures(0) = categoryIdx
    categoryFeatures
    })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd1,0,1,3,4)
Gender : Map(Male -> 1, Female -> 0)
Customer Type : Map(disloyal Customer -> 1, Loyal Customer -> 0)
Type of Travel : Map(Business travel -> 1, Personal Travel -> 0)
Class : Map(Eco -> 2, Eco Plus -> 1, Business -> 0)
concat: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[123] at map at <console>:30

concat.take(5)
res18: Array[Array[Double]] = Array(Array(1.0, 0.0, 0.0, 1.0, 0.0), Array(1.0, 1.0, 1.0, 0.0, 0.0), Array(0.0, 0.0, 1.0, 0.0, 0.0), Array(0.0, 0.0, 1.0, 0.0, 0.0), Array(1.0, 0.0, 1.0, 0.0, 0.0))

val rdd2 = rdd1.map( x => {
  val y = x.slice(5,x.size-1) ++ Array(x(2),categ_label(x(22)))
  y.map( z => { 
                val t = z.toString
                if (t == "") 0.0 else t.toDouble
       })
})

rdd2.take(5)
res19: Array[Array[Double]] = Array(Array(460.0, 3.0, 4.0, 3.0, 1.0, 5.0, 3.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 25.0, 18.0, 13.0, 0.0), Array(235.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 1.0, 1.0, 1.0, 5.0, 3.0, 1.0, 4.0, 1.0, 1.0, 6.0, 25.0, 0.0), Array(1142.0, 2.0, 2.0, 2.0, 2.0, 5.0, 5.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 4.0, 5.0, 0.0, 0.0, 26.0, 1.0), Array(562.0, 2.0, 5.0, 5.0, 5.0, 2.0, 2.0, 2.0, 2.0, 2.0, 5.0, 3.0, 1.0, 4.0, 2.0, 11.0, 9.0, 25.0, 0.0), Array(214.0, 3.0, 3.0, 3.0, 3.0, 4.0, 5.0, 5.0, 3.0, 3.0, 4.0, 4.0, 3.0, 3.0, 3.0, 0.0, 0.0, 61.0, 1.0))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(10).map( x => x.mkString(", ")).foreach(println)
1.0, 0.0, 0.0, 1.0, 0.0, 460.0, 3.0, 4.0, 3.0, 1.0, 5.0, 3.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 25.0, 18.0, 13.0, 0.0
1.0, 1.0, 1.0, 0.0, 0.0, 235.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 1.0, 1.0, 1.0, 5.0, 3.0, 1.0, 4.0, 1.0, 1.0, 6.0, 25.0, 0.0
0.0, 0.0, 1.0, 0.0, 0.0, 1142.0, 2.0, 2.0, 2.0, 2.0, 5.0, 5.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 4.0, 5.0, 0.0, 0.0, 26.0, 1.0
0.0, 0.0, 1.0, 0.0, 0.0, 562.0, 2.0, 5.0, 5.0, 5.0, 2.0, 2.0, 2.0, 2.0, 2.0, 5.0, 3.0, 1.0, 4.0, 2.0, 11.0, 9.0, 25.0, 0.0
1.0, 0.0, 1.0, 0.0, 0.0, 214.0, 3.0, 3.0, 3.0, 3.0, 4.0, 5.0, 5.0, 3.0, 3.0, 4.0, 4.0, 3.0, 3.0, 3.0, 0.0, 0.0, 61.0, 1.0
0.0, 0.0, 0.0, 0.0, 1.0, 1180.0, 3.0, 4.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 3.0, 4.0, 4.0, 4.0, 4.0, 1.0, 0.0, 0.0, 26.0, 0.0
1.0, 0.0, 0.0, 0.0, 1.0, 1276.0, 2.0, 4.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 4.0, 3.0, 5.0, 2.0, 9.0, 23.0, 47.0, 0.0
0.0, 0.0, 1.0, 0.0, 0.0, 2035.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, 5.0, 4.0, 4.0, 0.0, 52.0, 1.0
0.0, 0.0, 1.0, 0.0, 0.0, 853.0, 1.0, 2.0, 2.0, 2.0, 4.0, 3.0, 3.0, 1.0, 1.0, 2.0, 1.0, 4.0, 1.0, 2.0, 0.0, 0.0, 41.0, 0.0
1.0, 1.0, 1.0, 0.0, 1.0, 1061.0, 3.0, 3.0, 3.0, 4.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 4.0, 4.0, 3.0, 2.0, 0.0, 0.0, 20.0, 0.0



---- calculate cosine column similarities to check multicolinearity

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = vect.map( x => Vectors.dense(x.slice(0, x.size-1)))
val matrix = new RowMatrix(vectors)

val colsims = matrix.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0
var j = 0.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    i = x._1._1
    j = i + 1
    while (j < x._1._2) { 
      print("0.00 ")
      j = j + 1
    }
    print(f"$sim%.2f ")
  } else {
      j = j + 1
      while (j < x._1._2) { 
        print("0.00 ")
        j = j + 1
      }
      print(f"$sim%.2f ")
    }
})
// Gender,Customer Type,Age,Type of Travel,Class,Flight Distance,Inflight wifi service,Departure/Arrival time convenient,Ease of Online booking,Gate location,Food and drink,Online boarding,Seat comfort,Inflight entertainment,On-board service,Leg room service,Baggage handling,Checkin service,Inflight service,Cleanliness,Departure Delay in Minutes,Arrival Delay in Minutes
0.7383 0.8423 0.8417 0.8366 0.8577 0.8654 0.8909 0.8923 0.8778 0.8798 0.8743 0.8826 0.8764 0.8830 0.8741 0.3340 0.3363 0.6628 0.3078 0.7850 0.6814 0.5910
0.6911 0.6801 0.7025 0.7054 0.7218 0.7607 0.7517 0.7428 0.7412 0.7448 0.7413 0.7324 0.7406 0.7339 0.2782 0.2779 0.5433 0.1967 0.7321 0.7465 0.3213
0.8719 0.9436 0.8844 0.8531 0.9072 0.8588 0.8696 0.8593 0.8627 0.8715 0.8465 0.8706 0.8567 0.3178 0.3201 0.6378 0.3814 0.7724 0.6321 0.5911
0.8863 0.9007 0.8274 0.8386 0.8375 0.8311 0.8473 0.8351 0.8611 0.8507 0.8618 0.8336 0.3238 0.3259 0.6347 0.2991 0.6787 0.5876 0.6276
0.9013 0.8291 0.8937 0.8375 0.8367 0.8396 0.8479 0.8535 0.8346 0.8535 0.8309 0.3195 0.3218 0.6330 0.3732 0.7742 0.6517 0.5652
0.8485 0.8490 0.8585 0.8546 0.8548 0.8545 0.8742 0.8532 0.8747 0.8529 0.3341 0.3369 0.6545 0.3949 0.7699 0.6366 0.6153
0.8874 0.9412 0.9465 0.8711 0.8642 0.8824 0.8744 0.8829 0.9512 0.3230 0.3251 0.6564 0.3741 0.7805 0.6627 0.5971
0.9200 0.8987 0.8843 0.8771 0.8882 0.8905 0.8877 0.9049 0.3269 0.3289 0.6693 0.3291 0.8150 0.7301 0.5361
0.9485 0.8894 0.8829 0.8962 0.8965 0.8961 0.9572 0.3281 0.3305 0.6718 0.3474 0.8002 0.7059 0.5715
0.9238 0.9056 0.9271 0.8839 0.9304 0.9578 0.3264 0.3284 0.6605 0.3606 0.8023 0.6957 0.5752
0.9161 0.9458 0.9037 0.9495 0.8841 0.3272 0.3291 0.6637 0.3812 0.7873 0.7028 0.5783
0.9270 0.8893 0.9272 0.8775 0.3412 0.3434 0.6549 0.3820 0.8013 0.6999 0.5749
0.9139 0.9647 0.8942 0.3420 0.3443 0.6693 0.4134 0.7951 0.6957 0.6060
0.9147 0.8911 0.3313 0.3337 0.6626 0.3887 0.7722 0.6876 0.5918
0.8939 0.3282 0.3301 0.6694 0.4130 0.7941 0.6948 0.6073
0.3307 0.3332 0.6599 0.3689 0.7876 0.6794 0.5891
0.9655 0.2555 0.1578 0.3029 0.2428 0.2480
0.2595 0.1599 0.3058 0.2422 0.2524
0.3246 0.5943 0.4871 0.4806
0.5102 0.2396 0.3663
0.7964 0.3500


---- Splitting dataset as train/test sets  --------------

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)
   val f = x.slice(0, arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache
testSet.cache


----- Analyze the individual statistics and possibly standardize 

import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics

matrixSummary.max.toArray.foreach( x => print(f"$x%.2f\t"))
1.00    1.00    1.00    1.00    1.00    4983.00 5.00    5.00    5.00    5.00    5.00    5.00    5.00    5.00    5.00    5.00    5.00    5.00    5.00 5.00     1592.00 1584.00 85.00

matrixSummary.min.toArray.foreach( x => print(f"$x%.2f\t"))
0.00    0.00    0.00    0.00    0.00    31.00   0.00    0.00    0.00    1.00    0.00    0.00    1.00    0.00    0.00    0.00    1.00    1.00    0.00 0.00     0.00    0.00    7.00

matrixSummary.mean.toArray.foreach( x => print(f"$x%.2f\t"))
0.49    0.18    0.69    0.07    0.45    1187.71 2.73    3.06    2.75    2.98    3.20    3.25    3.44    3.36    3.38    3.35    3.63    3.30    3.63 3.29     14.92   15.25   39.40

matrixSummary.variance.toArray.foreach( x => print(f"$x%.2f\t"))
0.25    0.15    0.21    0.07    0.25    989750.65       1.77    2.33    1.96    1.64    1.77    1.82    1.74    1.78    1.66    1.73    1.40    1.61 1.39     1.72    1515.92 1552.72 227.91


----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache
testScaled.cache


---- MLlib logistic regression STD --------------

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(100).setRegParam(0.01)
val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res42: Double = 0.7853361651746327

metrics.areaUnderROC
res43: Double = 0.8558054768662499

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res44: Double = 0.854328801627539

metrics1.confusionMatrix
res45: org.apache.spark.mllib.linalg.Matrix =
14813.0  2724.0
1787.0   11643.0


---- Hyperparameter tunning with logistic regression STD --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], regSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step, reg  -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes; reg <- regSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step).setRegParam(reg)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, reg, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(10, 20, 40, 100),Array(1, 0.1, 0.01, 0.001),Array(0.1, 0.01), trainScaled, testScaled)
iter, step, reg  -> pred / total, AuPR, AuROC
10, 1.000, 0.100 -> 26147 / 30967, 0.7710, 0.8464
10, 1.000, 0.010 -> 26165 / 30967, 0.7718, 0.8470
10, 0.100, 0.100 -> 25539 / 30967, 0.7444, 0.8278
10, 0.100, 0.010 -> 25540 / 30967, 0.7444, 0.8278
10, 0.010, 0.100 -> 25399 / 30967, 0.7384, 0.8235
10, 0.010, 0.010 -> 25399 / 30967, 0.7384, 0.8235
10, 0.001, 0.100 -> 25390 / 30967, 0.7381, 0.8232
10, 0.001, 0.010 -> 25390 / 30967, 0.7381, 0.8232
20, 1.000, 0.100 -> 26216 / 30967, 0.7741, 0.8486
20, 1.000, 0.010 -> 26271 / 30967, 0.7764, 0.8503
20, 0.100, 0.100 -> 25640 / 30967, 0.7485, 0.8309
20, 0.100, 0.010 -> 25643 / 30967, 0.7486, 0.8310
20, 0.010, 0.100 -> 25408 / 30967, 0.7388, 0.8238
20, 0.010, 0.010 -> 25408 / 30967, 0.7388, 0.8238
20, 0.001, 0.100 -> 25390 / 30967, 0.7381, 0.8232
20, 0.001, 0.010 -> 25390 / 30967, 0.7381, 0.8232
40, 1.000, 0.100 -> 26294 / 30967, 0.7775, 0.8510
40, 1.000, 0.010 -> 26359 / 30967, 0.7805, 0.8530
40, 0.100, 0.100 -> 25754 / 30967, 0.7534, 0.8345
40, 0.100, 0.010 -> 25760 / 30967, 0.7537, 0.8346
40, 0.010, 0.100 -> 25413 / 30967, 0.7390, 0.8239
40, 0.010, 0.010 -> 25413 / 30967, 0.7390, 0.8239
40, 0.001, 0.100 -> 25390 / 30967, 0.7381, 0.8232
40, 0.001, 0.010 -> 25390 / 30967, 0.7381, 0.8232
100, 1.000, 0.100 -> 26316 / 30967, 0.7783, 0.8517
100, 1.000, 0.010 -> 26456 / 30967, 0.7853, 0.8558 *
100, 0.100, 0.100 -> 25881 / 30967, 0.7590, 0.8383
100, 0.100, 0.010 -> 25883 / 30967, 0.7592, 0.8384
100, 0.010, 0.100 -> 25413 / 30967, 0.7390, 0.8239
100, 0.010, 0.010 -> 25413 / 30967, 0.7390, 0.8239
100, 0.001, 0.100 -> 25390 / 30967, 0.7381, 0.8232
100, 0.001, 0.010 -> 25390 / 30967, 0.7381, 0.8232


val model = new LogisticRegressionWithSGD
model.optimizer.setNumIterations(100).setStepSize(1.0).setRegParam(0.01)

val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))

validPredicts.take(20)
res16: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res17: Double = 0.7853361651746327

metrics.areaUnderROC
res18: Double = 0.8558054768662499

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res19: Double = 0.854328801627539

metrics1.confusionMatrix
res20: org.apache.spark.mllib.linalg.Matrix =
14813.0  2724.0
1787.0   11643.0


---- MLlib SVM regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(10, 20, 40, 100), trainScaled, testScaled)
iter -> pred / total, AuPR, AuROC
10 -> 26387 / 30967, 0.7809, 0.8541
20 -> 26482 / 30967, 0.7853, 0.8570
40 -> 26555 / 30967, 0.7886, 0.8593
100 -> 26609 / 30967, 0.7919, 0.8607 *

val model = SVMWithSGD.train(trainScaled, 100)

val validPredicts = testScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res22: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res23: Double = 0.7918672068913146

metrics.areaUnderROC
res24: Double = 0.8606820995974431

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)

metrics1.accuracy
res25: Double = 0.8592695449995156

metrics1.confusionMatrix
res26: org.apache.spark.mllib.linalg.Matrix =
14907.0  2630.0
1728.0   11702.0


----- with MLlib Decision tree regression ----------------------

val categ_gender = rdd1.map(x => x(0)).distinct.zipWithIndex.collectAsMap
categ_gender: scala.collection.Map[String,Long] = Map(Male -> 1, Female -> 0)

val categ_ctype = rdd1.map(x => x(1)).distinct.zipWithIndex.collectAsMap
categ_ctype: scala.collection.Map[String,Long] = Map(disloyal Customer -> 1, Loyal Customer -> 0)

val categ_trtype = rdd1.map(x => x(3)).distinct.zipWithIndex.collectAsMap
categ_trtype: scala.collection.Map[String,Long] = Map(Business travel -> 1, Personal Travel -> 0)

val categ_class = rdd1.map(x => x(4)).distinct.zipWithIndex.collectAsMap
categ_class: scala.collection.Map[String,Long] = Map(Eco -> 2, Eco Plus -> 1, Business -> 0)

val rdd2_dt = rdd1.map( x => {
  val y = Array(categ_gender(x(0)),categ_ctype(x(1)),x(2),categ_trtype(x(3)),categ_class(x(4))) ++ x.slice(5,x.size-1) ++ Array(categ_label(x(22)))
  y.map( z => { 
                val t = z.toString
                if (t == "") 0.0 else t.toDouble
       })
})

rdd2_dt.take(10).map( x => x.mkString(", ")).foreach(println)
1.0, 0.0, 13.0, 0.0, 1.0, 460.0, 3.0, 4.0, 3.0, 1.0, 5.0, 3.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 25.0, 18.0, 0.0
1.0, 1.0, 25.0, 1.0, 0.0, 235.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 1.0, 1.0, 1.0, 5.0, 3.0, 1.0, 4.0, 1.0, 1.0, 6.0, 0.0
0.0, 0.0, 26.0, 1.0, 0.0, 1142.0, 2.0, 2.0, 2.0, 2.0, 5.0, 5.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 4.0, 5.0, 0.0, 0.0, 1.0
0.0, 0.0, 25.0, 1.0, 0.0, 562.0, 2.0, 5.0, 5.0, 5.0, 2.0, 2.0, 2.0, 2.0, 2.0, 5.0, 3.0, 1.0, 4.0, 2.0, 11.0, 9.0, 0.0
1.0, 0.0, 61.0, 1.0, 0.0, 214.0, 3.0, 3.0, 3.0, 3.0, 4.0, 5.0, 5.0, 3.0, 3.0, 4.0, 4.0, 3.0, 3.0, 3.0, 0.0, 0.0, 1.0
0.0, 0.0, 26.0, 0.0, 2.0, 1180.0, 3.0, 4.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 3.0, 4.0, 4.0, 4.0, 4.0, 1.0, 0.0, 0.0, 0.0
1.0, 0.0, 47.0, 0.0, 2.0, 1276.0, 2.0, 4.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 4.0, 3.0, 5.0, 2.0, 9.0, 23.0, 0.0
0.0, 0.0, 52.0, 1.0, 0.0, 2035.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, 5.0, 4.0, 4.0, 0.0, 1.0
0.0, 0.0, 41.0, 1.0, 0.0, 853.0, 1.0, 2.0, 2.0, 2.0, 4.0, 3.0, 3.0, 1.0, 1.0, 2.0, 1.0, 4.0, 1.0, 2.0, 0.0, 0.0, 0.0
1.0, 1.0, 20.0, 1.0, 2.0, 1061.0, 3.0, 3.0, 3.0, 4.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 4.0, 4.0, 3.0, 2.0, 0.0, 0.0, 0.0


val data = rdd2_dt.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)
   val f = x.slice(0, arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })
 
val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache
testSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 0->2, 1->2, 3->2, 4->3)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, AuPR, AuROC") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AuPR, AuROC
gini, 10, 32 -> 29299 / 30967, 0.9292, 0.9438
gini, 10, 48 -> 29298 / 30967, 0.9290, 0.9438
gini, 10, 64 -> 29291 / 30967, 0.9287, 0.9436
gini, 20, 32 -> 29376 / 30967, 0.9282, 0.9472
gini, 20, 48 -> 29399 / 30967, 0.9300, 0.9478
gini, 20, 64 -> 29371 / 30967, 0.9284, 0.9470
gini, 30, 32 -> 29253 / 30967, 0.9179, 0.9441
gini, 30, 48 -> 29309 / 30967, 0.9218, 0.9457
gini, 30, 64 -> 29254 / 30967, 0.9181, 0.9441
entropy, 10, 32 -> 29321 / 30967, 0.9287, 0.9448
entropy, 10, 48 -> 29328 / 30967, 0.9298, 0.9449
entropy, 10, 64 -> 29320 / 30967, 0.9290, 0.9447
entropy, 20, 32 -> 29392 / 30967, 0.9272, 0.9481
entropy, 20, 48 -> 29410 / 30967, 0.9276, 0.9488  *
entropy, 20, 64 -> 29396 / 30967, 0.9289, 0.9479
entropy, 30, 32 -> 29352 / 30967, 0.9222, 0.9474
entropy, 30, 48 -> 29336 / 30967, 0.9216, 0.9468
entropy, 30, 64 -> 29346 / 30967, 0.9234, 0.9469


val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "entropy", 20, 48)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res32: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)

metrics.areaUnderPR
res33: Double = 0.9275918363641307

metrics.areaUnderROC
res34: Double = 0.9487637795566451

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics1 = new MulticlassMetrics(validPredicts)
metrics1: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@5f986fcb

metrics1.accuracy
res35: Double = 0.9497206703910615

metrics1.confusionMatrix
res36: org.apache.spark.mllib.linalg.Matrix =
16765.0  772.0
785.0    12645.0