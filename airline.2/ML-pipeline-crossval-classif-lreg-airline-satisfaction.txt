---- Exploratory Data Analysis --------------

spark.conf.set("spark.sql.shuffle.partitions",10)

import org.apache.spark.sql.types._

val schemaAnswer = new StructType().
add("row", DoubleType).
add("id", DoubleType).
add("gender", StringType).
add("cust_type", StringType).
add("age", DoubleType).
add("travel_type", StringType).
add("class", StringType).
add("flight_distance", DoubleType).
add("inflight_wifi", DoubleType).
add("deprt_arrv_time_conv", DoubleType).
add("ease_online_booking", DoubleType).
add("gate_location", DoubleType).
add("food_and_drink", DoubleType).
add("online_boarding", DoubleType).
add("seat_comfort", DoubleType).
add("inflight_entertain", DoubleType).
add("onboard_service", DoubleType).
add("legroom_service", DoubleType).
add("baggage_handling", DoubleType).
add("checkin_service", DoubleType).
add("inflight_service", DoubleType).
add("cleanliness", DoubleType).
add("deprt_delay_minutes", DoubleType).
add("arrv_delay_minutes", DoubleType).
add("satisfaction", StringType)

val df = spark.read.format("csv").option("header","true").schema(schemaAnswer).load("hdfs://hdpmst:9000/data/airline_satisfaction_train.csv")


df.printSchema
root
 |-- row: double (nullable = true)
 |-- id: double (nullable = true)
 |-- gender: string (nullable = true)
 |-- cust_type: string (nullable = true)
 |-- age: double (nullable = true)
 |-- travel_type: string (nullable = true)
 |-- class: string (nullable = true)
 |-- flight_distance: double (nullable = true)
 |-- inflight_wifi: double (nullable = true)
 |-- deprt_arrv_time_conv: double (nullable = true)
 |-- ease_online_booking: double (nullable = true)
 |-- gate_location: double (nullable = true)
 |-- food_and_drink: double (nullable = true)
 |-- online_boarding: double (nullable = true)
 |-- seat_comfort: double (nullable = true)
 |-- inflight_entertain: double (nullable = true)
 |-- onboard_service: double (nullable = true)
 |-- legroom_service: double (nullable = true)
 |-- baggage_handling: double (nullable = true)
 |-- checkin_service: double (nullable = true)
 |-- inflight_service: double (nullable = true)
 |-- cleanliness: double (nullable = true)
 |-- deprt_delay_minutes: double (nullable = true)
 |-- arrv_delay_minutes: double (nullable = true)
 |-- satisfaction: string (nullable = true)
 

val df1 = df.drop("row","id").na.fill(0, Seq("arrv_delay_minutes"))


// ----- logistic regression model hyperparameter tunning

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

import org.apache.spark.ml.feature.RFormula
val rf = new RFormula().setFormula("satisfaction ~ .")

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression().setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(1, 0.1, 0.01, 0.001)).
addGrid(lr.fitIntercept).
addGrid(lr.maxIter, Array(10, 20, 40, 100)).build()

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bce = new BinaryClassificationEvaluator

val cv = new CrossValidator().
setEstimator(lr).
setEvaluator(bce.setMetricName("areaUnderROC")).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(rf,stdScaler,cv))
val pipelinemodel = pipeline.fit(trainingData)

import org.apache.spark.ml.tuning.CrossValidatorModel
val cvmodel = pipelinemodel.stages.last.asInstanceOf[CrossValidatorModel]

-- CV hyperparameter evaluation

cvmodel.getEstimatorParamMaps.zip(cvmodel.avgMetrics)
res27: Array[(org.apache.spark.ml.param.ParamMap, Double)] =
Array(({
        logreg_a9d732af22f4-fitIntercept: true,
        logreg_a9d732af22f4-maxIter: 10,
        logreg_a9d732af22f4-regParam: 1.0
},0.9100156502798381), ({
        logreg_a9d732af22f4-fitIntercept: true,
        logreg_a9d732af22f4-maxIter: 20,
        logreg_a9d732af22f4-regParam: 1.0
},0.9078227752487081), ({
        logreg_a9d732af22f4-fitIntercept: true,
        logreg_a9d732af22f4-maxIter: 40,
        logreg_a9d732af22f4-regParam: 1.0
},0.9078229564123802), ({
        logreg_a9d732af22f4-fitIntercept: true,
        logreg_a9d732af22f4-maxIter: 100,
        logreg_a9d732af22f4-regParam: 1.0
},0.9078229564123799), ({
        logreg_a9d732af22f4-fitIntercept: false,
        logreg_a9d732af22f4-maxIter: 10,
        logreg_a9d732af22f4-regParam: 1.0
},0.846377900312803), ({
        logreg_a9d732af22f4-fitIntercept: false,
        log...

-- extract best LR model 

import org.apache.spark.ml.classification.LogisticRegressionModel
val lrmodel = cvmodel.bestModel.asInstanceOf[LogisticRegressionModel]

lrmodel.getRegParam
res27: Double = 0.001

lrmodel.getMaxIter
res28: Int = 100

lrmodel.getThreshold
res29: Double = 0.5

lrmodel.getFitIntercept
res31: Boolean = true

-- collecting feature importance

val featureImp = df1.columns.diff(Array("satisfaction")).zip(lrmodel.coefficients.toArray)

val columns = Array("feature", "Importance")
val featureImpDF = spark.createDataFrame(featureImp).toDF(columns: _*)

featureImpDF.orderBy($"Importance".desc).show(40,false)
+--------------------+---------------------+
|feature             |Importance           |
+--------------------+---------------------+
|travel_type         |2.647379573235071    |
|cust_type           |2.0011764473380764   |
|class               |0.8252103045480773   |
|seat_comfort        |0.5951515654940599   |
|deprt_arrv_time_conv|0.3962765191347493   |
|inflight_service    |0.32188493208598207  |
|legroom_service     |0.30098167128965797  |
|baggage_handling    |0.24589393760137077  |
|deprt_delay_minutes |0.21470943673696455  |
|checkin_service     |0.12872010826488225  |
|cleanliness         |0.11260126528893526  |
|flight_distance     |0.08466993568496721  |
|onboard_service     |0.0747879662754752   |
|inflight_entertain  |0.06343630247913279  |
|food_and_drink      |0.018278451350663014 |
|arrv_delay_minutes  |0.0031691042625777515|
|inflight_wifi       |-8.159552842680517E-6|
|age                 |-0.00801352035253202 |
|online_boarding     |-0.021023785979270498|
|gender              |-0.02134560105521755 |
|ease_online_booking |-0.1282320239076263  |
|gate_location       |-0.1384714587677391  |
+--------------------+---------------------+


import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary

val trainingSummary = lrmodel.summary

trainingSummary.objectiveHistory
res32: Array[Double] = Array(0.684599692030566, 0.6691971595575897, 0.6289025513262314, 0.4836472315903916, 0.4638116307226653, 0.4441397379416355, 0.43303848342077733, 0.423864347454159, 0.41562013462964564, 0.40768189045624914, 0.39627942391820986, 0.3839970942877035, 0.38282192198625287, 0.37564696749798987, 0.3712945642904742, 0.36244571918243057, 0.3460539138440497, 0.3432110292579338, 0.3393136492071921, 0.33858467175735646, 0.33816920099607667, 0.3375058023537454, 0.33694576642452373, 0.3366521677810529, 0.3366512854162152, 0.3366208630908065, 0.3366043716990632, 0.3365553677425935, 0.3365293449680874, 0.33648040381839783, 0.33647287858190633, 0.3364585981931625, 0.33644305290637927, 0.33641729902607115, 0.33641350386942187, 0.33641244857450037, 0.3364082096256923, 0.336406935641...

val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]

binarySummary.areaUnderROC
res34: Double = 0.9267683647028008

binarySummary.accuracy
res35: Double = 0.8744018974759731

-- collecting metric performance

val pred = pipelinemodel.transform(testData)

val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res36: Double = 0.9267589225858791

bceval.setMetricName("areaUnderPR").evaluate(pred)
res37: Double = 0.9307019505696879

val predRDD = pred.select("prediction","label").rdd.map( row => (row.getDouble(0),row.getDouble(1))).cache

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

metrics.accuracy
res40: Double = 0.8771516227999354

metrics.confusionMatrix
res38: org.apache.spark.mllib.linalg.Matrix =
16003.0  1645.0
2159.0   11158.0


// ----- building reduced logistic regression model

import org.apache.spark.ml.feature.RFormula
val rf = new RFormula().setFormula("satisfaction ~ travel_type + cust_type + class + seat_confort")

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression
lr.setRegParam(0.001).setMaxIter(100).setFitIntercept(true).setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(rf,stdScaler,lr))

val model = pipeline.fit(trainingData)

// -----  metrics on test data

val pred = model.transform(testData)

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bceval = new BinaryClassificationEvaluator()

bceval.setMetricName("areaUnderROC").evaluate(pred)
res12: Double = 0.871030928061399

bceval.setMetricName("areaUnderPR").evaluate(pred)
res13: Double = 0.8702017347911732

val predRDD = pred.select("label","prediction").rdd.map( row => (row.getDouble(0),row.getDouble(1)) )

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

metrics.accuracy
res15: Double = 0.8042628774422735

metrics.confusionMatrix
res46: org.apache.spark.mllib.linalg.Matrix =
14535.0  2948.0
3113.0   10369.0

