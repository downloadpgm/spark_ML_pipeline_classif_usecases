---- Feature extraction & Data Munging --------------

val colname = sc.textFile("airline/airline_satisfaction_train.csv").filter( x => x.contains("Customer Type")).take(1)
colname: Array[String] = Array(,id,Gender,Customer Type,Age,Type of Travel,Class,Flight Distance,Inflight wifi service,Departure/Arrival time convenient,Ease of Online booking,Gate location,Food and drink,Online boarding,Seat comfort,Inflight entertainment,On-board service,Leg room service,Baggage handling,Checkin service,Inflight service,Cleanliness,Departure Delay in Minutes,Arrival Delay in Minutes,satisfaction)

val rdd = sc.textFile("airline/airline_satisfaction_train.csv").filter( x => ! x.contains("Customer Type"))

val rdd1 = rdd.map( x => x.split(",")).map( x => x.slice(2,x.size))

rdd1.take(5)
res0: Array[Array[String]] = Array(Array(Male, Loyal Customer, 13, Personal Travel, Eco Plus, 460, 3, 4, 3, 1, 5, 3, 5, 5, 4, 3, 4, 4, 5, 5, 25, 18.0, neutral or dissatisfied), Array(Male, disloyal Customer, 25, Business travel, Business, 235, 3, 2, 3, 3, 1, 3, 1, 1, 1, 5, 3, 1, 4, 1, 1, 6.0, neutral or dissatisfied), Array(Female, Loyal Customer, 26, Business travel, Business, 1142, 2, 2, 2, 2, 5, 5, 5, 5, 4, 3, 4, 4, 4, 5, 0, 0.0, satisfied), Array(Female, Loyal Customer, 25, Business travel, Business, 562, 2, 5, 5, 5, 2, 2, 2, 2, 2, 5, 3, 1, 4, 2, 11, 9.0, neutral or dissatisfied), Array(Male, Loyal Customer, 61, Business travel, Business, 214, 3, 3, 3, 3, 4, 5, 5, 3, 3, 4, 4, 3, 3, 3, 0, 0.0, satisfied))

rdd1.filter( x => x(x.size-2) == "").count
res20: Long = 310

val categories = rdd1.map( x => x(22)).distinct.zipWithIndex.collectAsMap
categories: scala.collection.Map[String,Long] = Map(neutral or dissatisfied -> 0, satisfied -> 1)

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  print(f"$idx%.0f -> "); println(categories)
  
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
    val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories - 1) else Array.ofDim[Double](1)
    if (numCategories > 2) { 
      if (categoryIdx > 0) categoryFeatures(categoryIdx - 1) = 1.0
    }
    else categoryFeatures(0) = categoryIdx
    categoryFeatures
    })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd1,0,1,3,4)
0 -> Map(Male -> 0, Female -> 1)
1 -> Map(disloyal Customer -> 1, Loyal Customer -> 0)
3 -> Map(Business travel -> 1, Personal Travel -> 0)
4 -> Map(Eco -> 2, Eco Plus -> 0, Business -> 1)

concat.take(2)
res2: Array[Array[Double]] = Array(Array(0.0, 0.0, 0.0, 0.0, 0.0), Array(0.0, 1.0, 1.0, 1.0, 0.0))

val rdd2 = rdd1.map( x => {
  val y = Array(categories(x(22)),x(2)) ++ x.slice(5,x.size-1)
  y.map( z => { 
                val t = z.toString
                if (t == "") 0.0 else t.toDouble
       })
})

rdd2.take(2)
res22: Array[Array[Double]] = Array(Array(0.0, 13.0, 460.0, 3.0, 4.0, 3.0, 1.0, 5.0, 3.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 25.0, 18.0), Array(0.0, 25.0, 235.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 1.0, 1.0, 1.0, 5.0, 3.0, 1.0, 4.0, 1.0, 1.0, 6.0))

val vect = rdd2.zip(concat).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(2)
res23: Array[Array[Double]] = Array(Array(0.0, 13.0, 460.0, 3.0, 4.0, 3.0, 1.0, 5.0, 3.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 25.0, 18.0, 0.0, 0.0, 0.0, 0.0, 0.0), Array(0.0, 25.0, 235.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 1.0, 1.0, 1.0, 5.0, 3.0, 1.0, 4.0, 1.0, 1.0, 6.0, 0.0, 1.0, 1.0, 1.0, 0.0))

val categ_gender = rdd1.map(x => x(0)).distinct.zipWithIndex.collectAsMap
categ_gender: scala.collection.Map[String,Long] = Map(Male -> 1, Female -> 0)

val categ_ctype = rdd1.map(x => x(1)).distinct.zipWithIndex.collectAsMap
categ_ctype: scala.collection.Map[String,Long] = Map(disloyal Customer -> 1, Loyal Customer -> 0)

val categ_trtype = rdd1.map(x => x(3)).distinct.zipWithIndex.collectAsMap
categ_trtype: scala.collection.Map[String,Long] = Map(Business travel -> 1, Personal Travel -> 0)

val categ_class = rdd1.map(x => x(4)).distinct.zipWithIndex.collectAsMap
categ_class: scala.collection.Map[String,Long] = Map(Eco -> 2, Eco Plus -> 1, Business -> 0)

val rdd2_dt = rdd1.map( x => {
  val y = Array(categories(x(22)),categ_gender(x(0)),categ_ctype(x(1)),x(2),categ_trtype(x(3)),categ_class(x(4))) ++ x.slice(5,x.size-1)
  y.map( z => { 
                val t = z.toString
                if (t == "") 0.0 else t.toDouble
       })
})

rdd2_dt.take(2)
res44: Array[Array[Double]] = Array(Array(0.0, 0.0, 0.0, 13.0, 0.0, 0.0, 460.0, 3.0, 4.0, 3.0, 1.0, 5.0, 3.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 25.0, 18.0), Array(0.0, 0.0, 1.0, 25.0, 1.0, 1.0, 235.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 1.0, 1.0, 1.0, 5.0, 3.0, 1.0, 4.0, 1.0, 1.0, 6.0))

val data = vect.zip(rdd2_dt)

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val train_rdd = sets(0)
val test_rdd = sets(1)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val trainSet = train_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(0)
   val f1 = x1.slice(1,x1.size)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._1
   val l1 = x1(0)
   val f1 = x1.slice(1,x1.size)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
trainSet.cache
testSet.cache

---- calculate cosine column similarities to check multicolinearity

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = vect.map( x => Vectors.dense(x.slice(1, x.size)))
val matrix = new RowMatrix(vectors)

val colsims = matrix.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0
var j = 0.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    i = x._1._1
    j = i + 1
    while (j < x._1._2) { 
      print("0.0000 ")
      j = j + 1
    }
    print(f"$sim%.4f ")
  } else {
      j = j + 1
      while (j < x._1._2) { 
        print("0.0000 ")
        j = j + 1
      }
      print(f"$sim%.4f ")
    }
})
// Gender,Customer Type,Age,Type of Travel,Class,Flight Distance,Inflight wifi service,Departure/Arrival time convenient,Ease of Online booking,Gate location,Food and drink,Online boarding,Seat comfort,Inflight entertainment,On-board service,Leg room service,Baggage handling,Checkin service,Inflight service,Cleanliness,Departure Delay in Minutes,Arrival Delay in Minutes
0.7383 0.8423 0.8417 0.8366 0.8577 0.8654 0.8909 0.8923 0.8778 0.8798 0.8743 0.8826 0.8764 0.8830 0.8741 0.3340 0.3363 0.6628 0.3078 0.7850 0.6814 0.5910
0.6911 0.6801 0.7025 0.7054 0.7218 0.7607 0.7517 0.7428 0.7412 0.7448 0.7413 0.7324 0.7406 0.7339 0.2782 0.2779 0.5433 0.1967 0.7321 0.7465 0.3213
0.8719 0.9436 0.8844 0.8531 0.9072 0.8588 0.8696 0.8593 0.8627 0.8715 0.8465 0.8706 0.8567 0.3178 0.3201 0.6378 0.3814 0.7724 0.6321 0.5911
0.8863 0.9007 0.8274 0.8386 0.8375 0.8311 0.8473 0.8351 0.8611 0.8507 0.8618 0.8336 0.3238 0.3259 0.6347 0.2991 0.6787 0.5876 0.6276
0.9013 0.8291 0.8937 0.8375 0.8367 0.8396 0.8479 0.8535 0.8346 0.8535 0.8309 0.3195 0.3218 0.6330 0.3732 0.7742 0.6517 0.5652
0.8485 0.8490 0.8585 0.8546 0.8548 0.8545 0.8742 0.8532 0.8747 0.8529 0.3341 0.3369 0.6545 0.3949 0.7699 0.6366 0.6153
0.8874 0.9412 0.9465 0.8711 0.8642 0.8824 0.8744 0.8829 0.9512 0.3230 0.3251 0.6564 0.3741 0.7805 0.6627 0.5971
0.9200 0.8987 0.8843 0.8771 0.8882 0.8905 0.8877 0.9049 0.3269 0.3289 0.6693 0.3291 0.8150 0.7301 0.5361
0.9485 0.8894 0.8829 0.8962 0.8965 0.8961 0.9572 0.3281 0.3305 0.6718 0.3474 0.8002 0.7059 0.5715
0.9238 0.9056 0.9271 0.8839 0.9304 0.9578 0.3264 0.3284 0.6605 0.3606 0.8023 0.6957 0.5752
0.9161 0.9458 0.9037 0.9495 0.8841 0.3272 0.3291 0.6637 0.3812 0.7873 0.7028 0.5783
0.9270 0.8893 0.9272 0.8775 0.3412 0.3434 0.6549 0.3820 0.8013 0.6999 0.5749
0.9139 0.9647 0.8942 0.3420 0.3443 0.6693 0.4134 0.7951 0.6957 0.6060
0.9147 0.8911 0.3313 0.3337 0.6626 0.3887 0.7722 0.6876 0.5918
0.8939 0.3282 0.3301 0.6694 0.4130 0.7941 0.6948 0.6073
0.3307 0.3332 0.6599 0.3689 0.7876 0.6794 0.5891
0.9655 0.2555 0.1578 0.3029 0.2428 0.2480
0.2595 0.1599 0.3058 0.2422 0.2524
0.3246 0.5943 0.4871 0.4806
0.5102 0.2396 0.3663
0.7964 0.3500

---- MLlib logistic regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
    model.optimizer.setNumIterations(numIter).setStepSize(step)
    val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainSet, testSet)
iter, step -> pred / total, AuPR, AuROC
100, 0.100 -> 17619 / 31072, 0.4330, 0.5000
100, 0.010 -> 13522 / 31072, 0.4337, 0.5016
100, 0.001 -> 16405 / 31072, 0.4678, 0.5566
300, 0.100 -> 13861 / 31072, 0.4369, 0.5079
300, 0.010 -> 17619 / 31072, 0.4330, 0.5000
300, 0.001 -> 17619 / 31072, 0.4330, 0.5000
500, 0.100 -> 14395 / 31072, 0.4421, 0.5175
500, 0.010 -> 14423 / 31072, 0.4425, 0.5182
500, 0.001 -> 17619 / 31072, 0.4330, 0.5000

---- MLlib SVM regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
    val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(100,300,500), trainSet, testSet)
iter -> pred / total, AuPR, AuROC
100 -> 17619 / 31072, 0.4330, 0.5000
300 -> 14079 / 31072, 0.4389, 0.5116
500 -> 13866 / 31072, 0.4369, 0.5078


---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res54: org.apache.spark.mllib.linalg.Vector = [85.0,4983.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,1305.0,1280.0,1.0,1.0,1.0,1.0,1.0]

matrixSummary.min
res55: org.apache.spark.mllib.linalg.Vector = [7.0,31.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]

matrixSummary.mean
res56: org.apache.spark.mllib.linalg.Vector = [39.438474846221396,1189.790284490338,2.728704415641466,3.0593832381370643,2.75509391476274,2.9773451230228627,3.1989235500878634,3.2530069200351646,3.440699143233774,3.357768563268875,3.380066454305782,3.3507386862917294,3.6310413005272846,3.303492970123011,3.637206173110739,3.2852043057996534,14.740224077328552,15.089177833919345,0.5089932996485061,0.18254338752196836,0.6897105667838312,0.4774549648506151,0.4497885544815466]

matrixSummary.variance
res57: org.apache.spark.mllib.linalg.Vector = [229.05161683604243,989419.9275693777,1.7595593984520257,2.3190971906888063,1.9488488440035612,1.6317861824358815,1.773063607616624,1.8180835287791113,1.7442830607794193,1.7844135676690194,1.663555086135873,1.7350219035668675,1.394616877809841,1.6083479752907652,1.3870041426330275,1.7259647150242348,1460.1475064126932,1494.8208838265239,0.24992255205517194,0.14922334806463497,0.21401283929573675,0.24949514701548395,0.2474822087262157]

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache
testScaled.cache

----- with MLlib logistic regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(0.1, 0.01, 0.001), trainScaled, testScaled)
iter, step -> pred / total, AuPR, AuROC
100, 0.100 -> 25765 / 31072, 0.7528, 0.8309
100, 0.010 -> 25514 / 31072, 0.7423, 0.8231
100, 0.001 -> 25486 / 31072, 0.7410, 0.8222
300, 0.100 -> 25829 / 31072, 0.7555, 0.8329
300, 0.010 -> 25514 / 31072, 0.7423, 0.8231
300, 0.001 -> 25486 / 31072, 0.7410, 0.8222
500, 0.100 -> 25829 / 31072, 0.7555, 0.8329
500, 0.010 -> 25514 / 31072, 0.7423, 0.8231
500, 0.001 -> 25486 / 31072, 0.7410, 0.8222


----- with MLlib SVM regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(100,300,500), trainScaled, testScaled)
iter -> pred / total, AuPR, AuROC
100 -> 26559 / 31072, 0.7867, 0.8558
300 -> 26559 / 31072, 0.7868, 0.8558
500 -> 26559 / 31072, 0.7868, 0.8558

----- with MLlib Decision tree regression ----------------------

val trainSet = train_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(0)
   val f1 = x1.slice(1,x1.size)
   LabeledPoint(l1,Vectors.dense(f1))
 })
 
val testSet = test_rdd.map( x => {
   val x1 = x._2
   val l1 = x1(0)
   val f1 = x1.slice(1,x1.size)
   LabeledPoint(l1,Vectors.dense(f1))
 })

trainSet.cache
testSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 0->2, 1->2, 3->2, 4->3)

def iterateDTC(depthNums:Array[Int], binNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("impurity, depth, bin -> pred / total, AuPR, AuROC") 
  for(impurity <- Array("gini", "entropy"); numDepth <- depthNums; numBin <- binNums ) {
    val model = DecisionTree.trainClassifier(train, 2, categoricalFeaturesInfo, impurity, numDepth, numBin)
    val validPredicts = test.map(x => (model.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%s, %d, %d -> %d / %d, %.4f, %.4f".format(impurity, numDepth, numBin, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateDTC(Array(10,20,30), Array(32,48,64), trainSet, testSet)
impurity, depth, bin -> pred / total, AuPR, AuROC
gini, 10, 32 -> 29323 / 31072, 0.9298, 0.9405
gini, 10, 48 -> 29313 / 31072, 0.9285, 0.9404
gini, 10, 64 -> 29323 / 31072, 0.9303, 0.9405
gini, 20, 32 -> 29432 / 31072, 0.9237, 0.9462
gini, 20, 48 -> 29458 / 31072, 0.9245, 0.9472
gini, 20, 64 -> 29465 / 31072, 0.9259, 0.9472
gini, 30, 32 -> 29344 / 31072, 0.9168, 0.9439
gini, 30, 48 -> 29360 / 31072, 0.9171, 0.9446
gini, 30, 64 -> 29361 / 31072, 0.9183, 0.9443
entropy, 10, 32 -> 29386 / 31072, 0.9358, 0.9420
entropy, 10, 48 -> 29389 / 31072, 0.9356, 0.9422
entropy, 10, 64 -> 29405 / 31072, 0.9377, 0.9425
entropy, 20, 32 -> 29469 / 31072, 0.9263, 0.9473
entropy, 20, 48 -> 29479 / 31072, 0.9271, 0.9475
entropy, 20, 64 -> 29502 / 31072, 0.9286, 0.9482 *
entropy, 30, 32 -> 29407 / 31072, 0.9203, 0.9459
entropy, 30, 48 -> 29401 / 31072, 0.9202, 0.9456
entropy, 30, 64 -> 29425 / 31072, 0.9216, 0.9464


val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "entropy", 20, 64)

model.toDebugString
res66: String =
"DecisionTreeModel classifier of depth 20 with 3807 nodes
  If (feature 11 <= 3.5)
   If (feature 6 <= 3.5)
    If (feature 6 <= 0.5)
     If (feature 13 <= 0.5)
      Predict: 0.0
     Else (feature 13 > 0.5)
      Predict: 1.0
    Else (feature 6 > 0.5)
     If (feature 4 in {0.0,2.0})
      If (feature 3 in {0.0})
       Predict: 0.0
      Else (feature 3 not in {0.0})
       If (feature 1 in {1.0})
        If (feature 5 <= 912.5)
         If (feature 4 in {2.0})
          Predict: 0.0
         Else (feature 4 not in {2.0})
          If (feature 18 <= 4.5)
           Predict: 0.0
          Else (feature 18 > 4.5)
           If (feature 15 <= 4.5)
            Predict: 0.0
           Else (feature 15 > 4.5)
            If (feature 17 <= 3.5)
             Predict: 0.0
  ...

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res67: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 29502
validPredicts.count                            // 31072
model.getClass.getSimpleName
metrics.areaUnderPR    // 0.9285668245676998
metrics.areaUnderROC   // 0.9482045331385112
