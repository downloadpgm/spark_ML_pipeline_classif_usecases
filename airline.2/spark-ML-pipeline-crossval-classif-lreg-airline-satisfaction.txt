import org.apache.spark.sql.types._

val schemaAnswer = new StructType().
add("row", DoubleType).
add("id", DoubleType).
add("gender", StringType).
add("cust_type", StringType).
add("age", DoubleType).
add("travel_type", StringType).
add("class", StringType).
add("flight_distance", DoubleType).
add("inflight_wifi", DoubleType).
add("deprt_arrv_time_conv", DoubleType).
add("ease_online_booking", DoubleType).
add("gate_location", DoubleType).
add("food_and_drink", DoubleType).
add("online_boarding", DoubleType).
add("seat_comfort", DoubleType).
add("inflight_entertain", DoubleType).
add("onboard_service", DoubleType).
add("legroom_service", DoubleType).
add("baggage_handling", DoubleType).
add("checkin_service", DoubleType).
add("inflight_service", DoubleType).
add("cleanliness", DoubleType).
add("deprt_delay_minutes", DoubleType).
add("arrv_delay_minutes", DoubleType).
add("satisfaction", StringType)

val df = spark.read.format("csv").schema(schemaAnswer).load("airline/airline_satisfaction_train.csv").drop("row","id")

val df1 = df.na.fill(0, Seq("arrv_delay_minutes"))

import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}

val dfrawIndexer1 = new StringIndexer().setInputCol("gender").setOutputCol("gender_cat")
val dfrawIndexer2 = new StringIndexer().setInputCol("cust_type").setOutputCol("cust_type_cat")
val dfrawIndexer3 = new StringIndexer().setInputCol("travel_type").setOutputCol("travel_type_cat")
val dfrawIndexer4 = new StringIndexer().setInputCol("class").setOutputCol("class_cat")
val dfrawIndexer5 = new StringIndexer().setInputCol("satisfaction").setOutputCol("label")

val dfrawIndexer11 = new OneHotEncoder().setInputCol("gender_cat").setOutputCol("gender_vect")
val dfrawIndexer21 = new OneHotEncoder().setInputCol("cust_type_cat").setOutputCol("cust_type_vect")
val dfrawIndexer31 = new OneHotEncoder().setInputCol("travel_type_cat").setOutputCol("travel_type_vect")
val dfrawIndexer41 = new OneHotEncoder().setInputCol("class_cat").setOutputCol("class_vect")

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("gender_vect","cust_type_vect","age","travel_type_vect","class_vect","class_vect","flight_distance","inflight_wifi","deprt_arrv_time_conv","ease_online_booking","gate_location","food_and_drink","online_boarding","seat_comfort","inflight_entertain","onboard_service","legroom_service","baggage_handling","checkin_service","inflight_service","cleanliness","deprt_delay_minutes","arrv_delay_minutes"))

import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(500).setFitIntercept(true)

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfrawIndexer1,dfrawIndexer2,dfrawIndexer3,dfrawIndexer4,dfrawIndexer5,dfrawIndexer11,dfrawIndexer21,dfrawIndexer31,dfrawIndexer41,va,lr))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

val model = pipeline.fit(trainingData)

val pred = model.transform(testData)

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bceval = new BinaryClassificationEvaluator()

bceval.evaluate(pred)
res50: Double = 0.9266840605045447

import org.apache.spark.ml.classification.LogisticRegressionModel

val lrmodel = model.stages(10).asInstanceOf[LogisticRegressionModel]

println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
-0.03705169790982573
1.6252137302474792
-0.00376802491523374
2.179553911579835
0.388924480032073
-0.04610387682679138
0.388924480032073
-0.04610387682679138
3.474717463509254E-5
0.335352867017258
-0.12445390954220653
-0.07541642964444342
0.004811770904544997
-0.013505296327838541
0.5385053618051067
0.07202161208215657
0.10774910903773828
0.2568200996195964
0.23190012554535205
0.11052844313452195
0.27421592003904266
0.09960709738274422
0.17754464489196914
-3.167879792079513E-5
-0.004068172499914349


-------------------

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(0.1, 0.01, 0.001)).
addGrid(lr.fitIntercept).
addGrid(lr.maxIter, Array(100,300,500)).build()

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new BinaryClassificationEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.classification.LogisticRegressionModel
val lrmodel = bestmodel.stages(10).asInstanceOf[LogisticRegressionModel]

lrmodel.getRegParam
res1: Double = 0.001

lrmodel.getMaxIter
res2: Int = 300

lrmodel.getThreshold
res3: Double = 0.5

lrmodel.getFitIntercept
res4: Boolean = true

lrmodel.getStandardization
res5: Boolean = true

println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
-0.050329707861038
1.9967533029222329
-0.007792272916656942
2.6627568063835154
0.43635437686172973
0.05443510236777008
0.43635437686172973
0.05443510236777008
-1.681240165444391E-5
0.40067400849260265
-0.12478020467298455
-0.13163199152915497
0.02176443449214916
-0.013960251547311283
0.6087313118282898
0.0663373956617141
0.06012612036059292
0.29755686543555215
0.24582714401959885
0.14101398461166784
0.3176794652605801
0.1262364046652996
0.2169571995974317
0.002152587400246859
-0.006604826877716541


val pred = bestmodel.transform(testData)

val bceval = new BinaryClassificationEvaluator()

bceval.evaluate(pred)
res8: Double = 0.9241564099504398
