
val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("mobile_pricing_train.csv")

df.printSchema
root
 |-- battery_power: integer (nullable = true)
 |-- blue: integer (nullable = true)
 |-- clock_speed: double (nullable = true)
 |-- dual_sim: integer (nullable = true)
 |-- fc: integer (nullable = true)
 |-- four_g: integer (nullable = true)
 |-- int_memory: integer (nullable = true)
 |-- m_dep: double (nullable = true)
 |-- mobile_wt: integer (nullable = true)
 |-- n_cores: integer (nullable = true)
 |-- pc: integer (nullable = true)
 |-- px_height: integer (nullable = true)
 |-- px_width: integer (nullable = true)
 |-- ram: integer (nullable = true)
 |-- sc_h: integer (nullable = true)
 |-- sc_w: integer (nullable = true)
 |-- talk_time: integer (nullable = true)
 |-- three_g: integer (nullable = true)
 |-- touch_screen: integer (nullable = true)
 |-- wifi: integer (nullable = true)
 |-- price_range: integer (nullable = true)
 
import org.apache.spark.sql.types._

val df1 = df.withColumn("battery_power", 'battery_power.cast(DoubleType)).
withColumn("blue", 'blue.cast(DoubleType)).
withColumn("dual_sim", 'dual_sim.cast(DoubleType)).
withColumn("fc", 'fc.cast(DoubleType)).
withColumn("four_g", 'four_g.cast(DoubleType)).
withColumn("int_memory", 'int_memory.cast(DoubleType)).
withColumn("mobile_wt", 'mobile_wt.cast(DoubleType)).
withColumn("n_cores", 'n_cores.cast(DoubleType)).
withColumn("pc", 'pc.cast(DoubleType)).
withColumn("px_height", 'px_height.cast(DoubleType)).
withColumn("px_width", 'px_width.cast(DoubleType)).
withColumn("ram", 'ram.cast(DoubleType)).
withColumn("sc_h", 'sc_h.cast(DoubleType)).
withColumn("sc_w", 'sc_w.cast(DoubleType)).
withColumn("talk_time", 'talk_time.cast(DoubleType)).
withColumn("three_g", 'three_g.cast(DoubleType)).
withColumn("touch_screen", 'touch_screen.cast(DoubleType)).
withColumn("wifi", 'wifi.cast(DoubleType)).
withColumn("price_range", 'price_range.cast(DoubleType)).
withColumnRenamed("price_range", "label")


df1.printSchema
root
 |-- battery_power: double (nullable = true)
 |-- blue: double (nullable = true)
 |-- clock_speed: double (nullable = true)
 |-- dual_sim: double (nullable = true)
 |-- fc: double (nullable = true)
 |-- four_g: double (nullable = true)
 |-- int_memory: double (nullable = true)
 |-- m_dep: double (nullable = true)
 |-- mobile_wt: double (nullable = true)
 |-- n_cores: double (nullable = true)
 |-- pc: double (nullable = true)
 |-- px_height: double (nullable = true)
 |-- px_width: double (nullable = true)
 |-- ram: double (nullable = true)
 |-- sc_h: double (nullable = true)
 |-- sc_w: double (nullable = true)
 |-- talk_time: double (nullable = true)
 |-- three_g: double (nullable = true)
 |-- touch_screen: double (nullable = true)
 |-- wifi: double (nullable = true)
 |-- label: double (nullable = true)
 
import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("battery_power","blue","clock_speed","dual_sim","fc","four_g","int_memory","m_dep","mobile_wt","n_cores","pc","px_height","px_width","ram","sc_h","sc_w","talk_time","three_g","touch_screen","wifi"))

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.classification.{LogisticRegression,OneVsRest}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(500).setFitIntercept(true).setFamily("binomial").setFeaturesCol("scaledFeatures")

val ovr = new OneVsRest().setClassifier(lr)

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(va,stdScaler,ovr))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

val model = pipeline.fit(trainingData)

val pred = model.transform(testData)

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

val accuracy = evaluator.evaluate(pred)
accuracy: Double = 0.7892918825561313

------------------------------

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().addGrid(lr.regParam, Array(0.1, 0.01, 0.001)).addGrid(lr.fitIntercept, Array(true)).addGrid(lr.maxIter, Array(100,300,500)).build()

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new MulticlassClassificationEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.classification.OneVsRestModel
val lrmodel = bestmodel.stages(2).asInstanceOf[OneVsRestModel]

val pred = bestmodel.transform(testData)

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)
res12: Double = 0.7892918825561313


