---- Feature extraction & Data Munging --------------

val rdd1 = sc.textFile("insurance/train.csv").map( x => x.split(","))

rdd1.first
res0: Array[String] = Array(id, Gender, Age, Driving_License, Region_Code, Previously_Insured, Vehicle_Age, Vehicle_Damage, Annual_Premium, Policy_Sales_Channel, Vintage, Response)

val rdd = rdd1.filter( x => x(3) != "Driving_License")

rdd.take(2)
res2: Array[Array[String]] = Array(Array(1, Male, 44, 1, 28.0, 0, > 2 Years, Yes, 40454.0, 26.0, 217, 1), Array(2, Male, 76, 1, 3.0, 0, 1-2 Year, No, 33536.0, 26.0, 183, 0))

val rdd = rdd1.filter( x => x(3) != "Driving_License").map( x => Array(x(1),x(6),x(7),x(5),x(2),x(10),x(11)))

rdd.take(2)
res3: Array[Array[String]] = Array(Array(Male, > 2 Years, Yes, 0, 44, 217, 1), Array(Male, 1-2 Year, No, 0, 76, 183, 0))

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
	  if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,0,1,2,3)

concat.take(5)
res3: Array[Array[Double]] = Array(Array(1.0, 0.0, 1.0, 0.0, 1.0, 0.0), Array(1.0, 0.0, 0.0, 1.0, 0.0, 0.0), Array(1.0, 0.0, 1.0, 0.0, 1.0, 0.0), Array(1.0, 1.0, 0.0, 0.0, 0.0, 1.0), Array(0.0, 1.0, 0.0, 0.0, 0.0, 1.0))

val rdd2 = rdd.map( x => x.slice(4,x.size)).map( x => x.map( y => y.toDouble))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(5)
res4: Array[Array[Double]] = Array(Array(1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 44.0, 217.0, 1.0), Array(1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 76.0, 183.0, 0.0), Array(1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 47.0, 27.0, 1.0), Array(1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 21.0, 203.0, 0.0), Array(0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 29.0, 39.0, 0.0))

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)
   val f = x.slice(0, arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

---- MLlib logistic regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(1, 0.1, 0.01), trainSet, testSet)
iter, step -> pred / total, AuPR, AuROC
100, 1.000 -> 100457 / 114438, 0.1222, 0.5000
100, 0.100 -> 100457 / 114438, 0.1222, 0.5000
100, 0.010 -> 100457 / 114438, 0.1222, 0.5000
300, 1.000 -> 100457 / 114438, 0.1222, 0.5000
300, 0.100 -> 100457 / 114438, 0.1222, 0.5000
300, 0.010 -> 100457 / 114438, 0.1222, 0.5000
500, 1.000 -> 100457 / 114438, 0.1222, 0.5000
500, 1.000 -> 100457 / 114438, 0.1222, 0.5000
500, 1.000 -> 100457 / 114438, 0.1222, 0.5000


---- MLlib SVM regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
	val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateSVM(Array(100,300,500), trainSet, testSet)
iter -> pred / total, AuPR, AuROC
100 -> 100457 / 114438, 0.1222, 0.5000
300 -> 14115 / 114438, 0.1223, 0.5007
500 -> 100457 / 114438, 0.1222, 0.5000


---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res12: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,1.0,85.0,299.0]

matrixSummary.min
res13: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,20.0,10.0]

matrixSummary.mean
res14: org.apache.spark.mllib.linalg.Vector = [0.5406887138084007,0.432619219937676,0.04197306793764601,0.525407712124678,0.504134307817498,0.4591537887509328,38.80024074608746,154.23134874058292]

matrixSummary.variance
res15: org.apache.spark.mllib.linalg.Vector = [0.24834535984858422,0.24546075094105124,0.04021148029614853,0.24935538323208176,0.2499838449230555,0.24833251825840755,240.22449778581597,7005.710973003284]

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache

----- with MLlib logistic regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LogisticRegressionWithSGD
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d, %5.3f -> %d / %d, %.4f, %.4f".format(numIter, step, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

iterateLRwSGD(Array(100,300,500),Array(1, 0.1, 0.01), trainScaled, testScaled)
iter, step -> pred / total, AuPR, AuROC
100, 1.000 -> 68819 / 114438, 0.2324, 0.7679
100, 0.100 -> 67466 / 114438, 0.2276, 0.7618
100, 0.010 -> 67466 / 114438, 0.2276, 0.7618
300, 1.000 -> 68819 / 114438, 0.2324, 0.7679
300, 0.100 -> 67466 / 114438, 0.2276, 0.7618
300, 0.010 -> 67466 / 114438, 0.2276, 0.7618
500, 1.000 -> 68819 / 114438, 0.2324, 0.7679
500, 0.100 -> 67466 / 114438, 0.2276, 0.7618
500, 0.010 -> 67466 / 114438, 0.2276, 0.7618


----- with MLlib SVM regression ----------------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

def iterateSVM(iterNums:Array[Int], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter -> pred / total, AuPR, AuROC") 
  for(numIter <- iterNums) {
    val lr = SVMWithSGD.train(train, numIter)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new BinaryClassificationMetrics(validPredicts)
    println("%d -> %d / %d, %.4f, %.4f".format(numIter, validPredicts.filter(x => x._1 == x._2).count, test.count, metrics.areaUnderPR, metrics.areaUnderROC))
  }
}

scala> iterateSVM(Array(100,300,500), trainScaled, testScaled)
iter -> pred / total, AuPR, AuROC
100 -> 69862 / 114438, 0.2350, 0.7692
300 -> 69862 / 114438, 0.2350, 0.7692
500 -> 69862 / 114438, 0.2350, 0.7692